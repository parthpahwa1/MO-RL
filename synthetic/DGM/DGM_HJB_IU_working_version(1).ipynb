{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BErSeefeQwQi"
      },
      "source": [
        "### Setup packages "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8DGCgVxR2AB"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xIx5C6UQn4u",
        "outputId": "ce5e5d6e-5a55-42d1-fd13-90e285b0fca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting progressbar\n",
            "  Downloading progressbar-2.5.tar.gz (10 kB)\n",
            "Building wheels for collected packages: progressbar\n",
            "  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12082 sha256=0b9db790e4bacf23cb5ac0dc4017027a8d0e7bbb89a6474ccc198e039d6fa69f\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/fd/1f/3e35ed57e94cd8ced38dd46771f1f0f94f65fec548659ed855\n",
            "Successfully built progressbar\n",
            "Installing collected packages: progressbar\n",
            "Successfully installed progressbar-2.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: plotnine in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.10.2)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.21.6)\n",
            "Requirement already satisfied: mizani>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.6.0)\n",
            "Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.3.5)\n",
            "Requirement already satisfied: descartes>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.1.0)\n",
            "Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.7/dist-packages (from plotnine) (3.2.2)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.5.2)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (1.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.1.1->plotnine) (4.1.1)\n",
            "Requirement already satisfied: palettable in /usr/local/lib/python3.7/dist-packages (from mizani>=0.6.0->plotnine) (3.3.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->plotnine) (2022.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.4.1->plotnine) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ipython-autotime\n",
            "  Downloading ipython_autotime-0.3.1-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (5.5.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (1.0.18)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (57.4.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipython-autotime) (0.7.0)\n",
            "Installing collected packages: ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.1\n",
            "time: 7.46 ms (started: 2022-07-10 17:59:07 +00:00)\n"
          ]
        }
      ],
      "source": [
        "%pip install progressbar\n",
        "%pip install plotnine\n",
        "%pip install torch\n",
        "%pip install ipython-autotime\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfIU_eNp3Zio",
        "outputId": "57f29799-77c0-48bb-9dfc-0c0a9a7720c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.22 s (started: 2022-07-10 17:59:07 +00:00)\n"
          ]
        }
      ],
      "source": [
        "from plotnine import *\n",
        "from plotnine.themes import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmUjYbArAuQT",
        "outputId": "26273ad7-f0c4-4749-ae4a-0ae36b7308d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.91 s (started: 2022-07-10 17:59:09 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from scipy.io import loadmat\n",
        "import random\n",
        "import math\n",
        "import tensorflow_probability as tfp\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PieVKPfHHYQ6"
      },
      "source": [
        "_paper_name_ establishes the reusable name of the paper, it represents the directory under data_papers on the google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI4p7ZKb0Qz2",
        "outputId": "c66e2a87-e5cd-4905-fe46-56e1ca52e535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 876 Âµs (started: 2022-07-10 17:59:12 +00:00)\n"
          ]
        }
      ],
      "source": [
        "paper_name = \"dgm_hjb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "433z6V3T2rB2",
        "outputId": "87de93b9-2662-445c-9da1-6380e93a4417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 13.2 ms (started: 2022-07-10 17:59:12 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import os, sys\n",
        "import errno\n",
        "\n",
        "# make a directory if it does not exist\n",
        "def make_dir_if_not_exist(used_path):\n",
        "    if not os.path.isdir(used_path):\n",
        "        try:\n",
        "            os.mkdir(used_path)\n",
        "        except OSError as exc:\n",
        "            if exc.errno != errno.EEXIST:\n",
        "                raise exc\n",
        "            else:\n",
        "                raise ValueError(f'{used_path} directoy cannot be created because its parent directory does not exist.')\n",
        "\n",
        "# make directories if they do not exist\n",
        "\n",
        "#make_dir_if_not_exist(\"/content/drive/MyDrive/data_papers/\")\n",
        "#make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}\")\n",
        "#make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features/\")\n",
        "#make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/\")\n",
        "#make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_history/\")\n",
        "#make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/\")\n",
        "#make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_predictions/\")\n",
        "#make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_ccs/\")\n",
        "#make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/\")\n",
        "#make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/temp/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uat0pG8aR3Rh",
        "outputId": "a0a0068e-d3fe-42a1-c033-8dbd49a314b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 15.3 ms (started: 2022-07-10 17:59:12 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Set up the imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import site\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import h5py as h5\n",
        "import matplotlib.pyplot as plt\n",
        "import errno\n",
        "import numpy as np\n",
        "import itertools\n",
        "import multiprocessing\n",
        "import json\n",
        "import datetime\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "pd.set_option('display.width', 400)\n",
        "pd.set_option('display.max_columns', 40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpFjo3MkLus9",
        "outputId": "eaac85f6-86bf-41a2-d942-62b0cdf3169c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.2 s (started: 2022-07-10 17:59:12 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import torch \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from scipy.stats import norm\n",
        "from matplotlib import cm\n",
        "import pdb\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbfN42gpGZhC",
        "outputId": "eae2512c-032f-4685-c2dd-62bdc86568c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.84 s (started: 2022-07-10 17:59:15 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from pprint import pprint as pp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvy0WvxDGCxk"
      },
      "source": [
        "### Shared functions across models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpVaz5dwXZNq",
        "outputId": "01eb8600-838e-4daa-e08a-1cfd169ed75a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 83.5 ms (started: 2022-07-10 17:59:19 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def plot_report(train_instance):\n",
        "        \n",
        "    history_tl_cpu = [ x for x in train_instance.history_tl ]\n",
        "    history_internal_cpu = [ x.cpu().detach().numpy() for x in train_instance.history_internal_cpu ]\n",
        "    history_terminal_cpu = [ x.cpu().detach().numpy() for x in train_instance.history_terminal ]\n",
        "    history_initial_cpu = [ x.cpu().detach().numpy() for x in train_instance.history_initial ]\n",
        "    history_nonzero_cpu = [ x.cpu().detach().numpy() for x in train_instance.history_nonzero ]\n",
        "\n",
        "    obs_data = pd.DataFrame({\"Epochs\" : [ (x+1)*train_instance.hook_interval for x in range(len(history_initial_cpu))], \n",
        "                             \"AvgLogLoss\": np.log(history_tl_cpu), \n",
        "                             \"TerminalLogLoss\" :  np.log(history_terminal_cpu),\n",
        "                             \"InternalLogLoss\" :  np.log(history_internal_cpu),\n",
        "                             \"InitialLogLoss\" : np.log(history_initial_cpu),\n",
        "                             \"NonZeroLogLoss\" : np.log(history_nonzero_cpu),\n",
        "                             })\n",
        "\n",
        "    return (ggplot(obs_data, aes(\"Epochs\",\"AvgLogLoss\")) + geom_line() + geom_point(),\n",
        "            ggplot(obs_data, aes(\"Epochs\",\"TerminalLogLoss\")) + geom_line() + geom_point(),\n",
        "            ggplot(obs_data, aes(\"Epochs\",\"InternalLogLoss\")) + geom_line() + geom_point(),\n",
        "            ggplot(obs_data, aes(\"Epochs\",\"InitialLogLoss\")) + geom_line() + geom_point(),\n",
        "            ggplot(obs_data, aes(\"Epochs\",\"NonZeroLogLoss\")) + geom_line() + geom_point(),\n",
        "            )\n",
        "\n",
        "def plot_activation_mean(train_instance):\n",
        "    \n",
        "    # pdb.set_trace()\n",
        "\n",
        "    if train_instance.debug == False:\n",
        "        print( 'error: debug is off , turn it on and train again ' )\n",
        "    else:\n",
        "        history = np.array(train_instance.history_mean_hooks)\n",
        "        jet= plt.get_cmap('jet')\n",
        "        colors = iter(jet(np.linspace(0,1,10)))\n",
        "        fig, ax = plt.subplots()\n",
        "        for i in range(history.shape[1]):\n",
        "            ax.plot(history[:,i], '--r', label= i , color=next(colors) )\n",
        "        fig.suptitle('Layers activation mean value', fontsize=10)\n",
        "        leg = ax.legend();\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMAuMqdgU9kL",
        "outputId": "4eff2c00-2e39-41b8-9770-58b5e1c82b89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.37 ms (started: 2022-07-10 17:59:19 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# plot_report(train)\n",
        "# plot_activation_mean(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCV-yFDXUV4J",
        "outputId": "2621b90d-bcef-438e-feca-8d682d6c9e3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 13.2 ms (started: 2022-07-10 17:59:19 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# print( 'Value at 0' , net( torch.tensor( [ 0. , 1. , 1. , 1. ] ).cuda() ) )\n",
        "# #%% save\n",
        "# torch.save(net.state_dict(), './model3Assets')\n",
        "# #%%\n",
        "# net = TheModelClass(*args, **kwargs)\n",
        "# net.load_state_dict(torch.load('./modelmodel3Assets'))\n",
        "# net.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONB5NopRa3fD",
        "outputId": "f7f11d10-a9d3-4c9d-f371-6b63aa2c5379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 134 ms (started: 2022-07-10 17:59:19 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# a set up that just maximizes the loss s.t. loss < eps (maximizeloss_weights_st) using the weights on the losses\n",
        "from scipy.optimize import LinearConstraint, NonlinearConstraint\n",
        "from scipy.optimize import Bounds\n",
        "from functools import partial\n",
        "from scipy.optimize import minimize\n",
        "from functools import wraps\n",
        "\n",
        "def negative(f):\n",
        "    @wraps(f)\n",
        "    def g(*args,**kwargs):\n",
        "        return - f(*args,**kwargs)\n",
        "    # g.__name__ = f'negative({f.__name__})'\n",
        "    return g\n",
        "# kl_loss = nn.KLDivLoss(size_average=None, reduction=\"batchmean\")\n",
        "\n",
        "# we can add more minimization functions here later (e.g. SS diff)\n",
        "def KLDiffHere( varX, loss_terms, log_target = False, reduction = \"mean\"):  \n",
        "  target = torch.tensor([1./len(loss_terms)]*len(loss_terms))*torch.tensor(loss_terms)\n",
        "  input = torch.tensor(varX*loss_terms)\n",
        "  loss_pointwise = target * (torch.log(target) - torch.log(input))\n",
        "  if reduction == \"mean\":  # default\n",
        "      loss = loss_pointwise.mean()\n",
        "  elif reduction == \"batchmean\":  # mathematically correct\n",
        "      loss = loss_pointwise.sum() / input.size(0)\n",
        "  elif reduction == \"sum\":\n",
        "      loss = loss_pointwise.sum()\n",
        "  else:  # reduction == \"none\"\n",
        "      loss = loss_pointwise  \n",
        "  return loss\n",
        "\n",
        "  # return torch.nn.KLDivLoss(varX*loss_terms,np.array([1./len(loss_terms)]*len(loss_terms))*loss_terms)\n",
        "\n",
        "def minimize_weights_st(loss_terms, loss_func):\n",
        "  bounds = Bounds([0]*len(loss_terms), [1.0]*len(loss_terms))\n",
        "  linear_constraint = LinearConstraint([[1]*len(loss_terms)], [1.0], [1.0])\n",
        "  x0 = [0.25]*len(loss_terms)\n",
        "  res = minimize( partial(loss_func, loss_terms=loss_terms), \n",
        "                  x0, \n",
        "                  method='trust-constr', \n",
        "                  constraints=[linear_constraint],\n",
        "                  options={'verbose': 0}, \n",
        "                  bounds=bounds )\n",
        "  return res\n",
        "\n",
        "def maximizeloss_weights_st(loss_terms, loss_func, eps):\n",
        "  bounds = Bounds([0]*len(loss_terms), [1.0]*len(loss_terms))\n",
        "  linear_constraint = LinearConstraint([[1]*len(loss_terms)], [1.0], [1.0])\n",
        "  nonlinear_constraint  = NonlinearConstraint(negative(partial(loss_func, loss_terms=loss_terms)),1E-9,eps)\n",
        "  # even though zero is the KL minimum it helps to put a negative number here to explore\n",
        "\n",
        "  x0 = [1.0/len(loss_terms)]*len(loss_terms)\n",
        "  res = minimize( negative(partial(loss_func, loss_terms=loss_terms)), \n",
        "                  x0, \n",
        "                  method='trust-constr', \n",
        "                  constraints=[linear_constraint, nonlinear_constraint],\n",
        "                  options={'verbose': 0}, \n",
        "                  bounds=bounds )\n",
        "  return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RM0IVdZ_TXW3",
        "outputId": "b61cb6d5-6a28-4f96-caf5-c4a691b14bc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.33334757 0.33333761 0.33331482]\n",
            "time: 136 ms (started: 2022-07-10 17:59:20 +00:00)\n"
          ]
        }
      ],
      "source": [
        "r1 = maximizeloss_weights_st( [ 34.25, 100.12, 23.45] , KLDiffHere, 1E9)\n",
        "print(r1.x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewko67bDIcz9",
        "outputId": "415a56e9-9536-4853-8cb9-37fa12fed8dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 19.8 ms (started: 2022-07-10 17:59:20 +00:00)\n"
          ]
        }
      ],
      "source": [
        "### There is an issue getting this to work because of nonlinear_constraint  = NonlinearConstraint(negative(partial(loss_func, loss_terms=loss_terms)),1E-9,eps)\n",
        "\n",
        "    # def calculateLossAdaptWeights(self , size = 2**8 , train = True, min_max = True):\n",
        "    #     '''\n",
        "    #     Helper function that Sample and Calculate loss,\n",
        "    #     This is adapted in that it changes the weights on the losses to maximize the loss provided\n",
        "    #     the KL distance of the new weighting is within self.eps of the previous distribution (starting at equally weighted)\n",
        "    #     '''        \n",
        "    #     x , x_terminal , x_boundary = self.sample(size)\n",
        "    #     x = Variable( x , requires_grad=True)\n",
        "    #     Ls = self.criterion( x , x_terminal , x_boundary )\n",
        "    #     DO , TC , BC = Ls\n",
        "    #     DOm = torch.mean(DO).detach().cpu().float().item()\n",
        "    #     TCm = torch.mean(TC).detach().cpu().float().item()\n",
        "    #     BCm = torch.mean(BC).detach().cpu().float().item()\n",
        "\n",
        "    #     losses_for_reweighting = [ torch.mean(lv).detach().cpu().float().item() for lv in Ls if list(lv.size())] \n",
        "    #     mask_for_available_losses = [ True if list(lv.size()) else False for lv in Ls ]\n",
        "\n",
        "    #     # print([ DOm, TCm, BCm])\n",
        "    #     # if is.nan(DOm):\n",
        "    #     #   print(DO)\n",
        "\n",
        "    #     if self.weights is None:\n",
        "    #       self.weights = torch.ones(1,len(Ls))/len(Ls)\n",
        "\n",
        "    #     # pdb.set_trace()\n",
        "\n",
        "    #     if min_max:\n",
        "    #         r1 = maximizeloss_weights_st( losses_for_reweighting , KLDiffHere, self.eps)\n",
        "    #         candidate_weigths = torch.zeros_like(self.weights).to(torch.device(\"cuda:0\"))\n",
        "    #         candidate_weigths[0][mask_for_available_losses] = torch.tensor(r1.x).to(torch.device(\"cuda:0\")).float()\n",
        "    #         self.weights = candidate_weigths.to(torch.device(\"cuda:0\"))\n",
        "    #         self.weights_tbl.append(self.weights.detach().cpu().numpy())\n",
        "\n",
        "    #     numActive = np.sum([1 if list(lv.size()) else 0 for lv in Ls ])\n",
        "    #     if train == True:\n",
        "    #         return  (self.weights[0,0]*torch.mean(DO) + \n",
        "    #                  self.weights[0,1]*torch.mean(TC) + \n",
        "    #                  self.weights[0,2]*torch.mean(BC)) , \\\n",
        "    #                  self.weights[0,0]*torch.mean(DO) , \\\n",
        "    #                  self.weights[0,1]*torch.mean(TC) , \\\n",
        "    #                  self.weights[0,2]*torch.mean(BC) , \\\n",
        "    #                  (1./numActive*torch.mean(DO) + \n",
        "    #                  1./numActive*torch.mean(TC) + \n",
        "    #                  1./numActive*torch.mean(BC))             \n",
        "    #     else:\n",
        "    #         return  DO , TC , BC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyacROFeXgNp",
        "outputId": "d0da1a6d-2565-425a-a66a-c456e21796fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 27.3 ms (started: 2022-07-10 17:59:20 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.distributions import Normal\n",
        "\n",
        "std_norm_cdf = Normal(0, 1).cdf\n",
        "std_norm_pdf = lambda x: torch.exp(Normal(0, 1).log_prob(x))\n",
        "\n",
        "def bs_price(right, K, S, T, sigma, r):\n",
        "    d_1 = (1 / (sigma * torch.sqrt(T))) * (torch.log(S / K) + (r + (torch.square(sigma) / 2)) * T)\n",
        "    d_2 = d_1 - sigma * torch.sqrt(T)\n",
        "    \n",
        "    if right == \"C\":\n",
        "        C = std_norm_cdf(d_1) * S - std_norm_cdf(d_2) * K * torch.exp(-r * T)\n",
        "        return C\n",
        "        \n",
        "    elif right == \"P\":\n",
        "        P = std_norm_cdf(-d_2) * K * torch.exp(-r * T) - std_norm_cdf(-d_1) * S\n",
        "        return P"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLsA5AvqpMM7",
        "outputId": "8b851d6c-286d-423d-884c-bb7df91a93f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.77 ms (started: 2022-07-10 17:59:20 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def to_cpu_detach(x):\n",
        "  if isinstance(x, list):\n",
        "    return [ y.detach().cpu().item() for y in x ]\n",
        "  else:\n",
        "    return x.detach().cpu().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PC-E2SeX46A9",
        "outputId": "65a8a6d6-caad-4fa9-b138-f9e440f2ed9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 9.77 ms (started: 2022-07-10 17:59:20 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def huber_loss_zero_target(x, delta = 1.0):\n",
        "  loss_function = torch.nn.HuberLoss(delta=delta)\n",
        "  return loss_function(x, torch.zeros_like(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNYJyHWpeL66",
        "outputId": "6f229ba7-3d04-4652-e908-1494ee127e12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 45.7 ms (started: 2022-07-10 17:59:20 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# def save_model_train(lr, net,  eqLossFn, sample_method, trainObj, eqType, eqObject = None ):\n",
        "\n",
        "#   model_id_str =  f\"{eqType}_{datetime.datetime.now():%Y%m%d%H%M%S}_{eqLossFn}_{sample_method}_{trainObj.stop_epoch}_{str(lr).replace('.','p')}_{net.NL}_{net.NN}\"\n",
        "  \n",
        "#   if eqObject is not None:\n",
        "#     try:\n",
        "#         beta = getattr(eqObject,\"beta\")\n",
        "#         beta_str = str(beta).replace('.','p')\n",
        "#         model_id_str = model_id_str + f\"_beta{beta_str}\"\n",
        "#     except AttributeError:\n",
        "#         pass\n",
        "#     try:\n",
        "#         wgamma = getattr(eqObject,\"wgamma\")\n",
        "#         wgamma_str = str(wgamma).replace('.','p')\n",
        "#         model_id_str = model_id_str + f\"_wgamma{wgamma_str}\"\n",
        "#     except AttributeError:\n",
        "#         pass\n",
        "  \n",
        "#   torch.save(net.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{model_id_str}\")\n",
        "#   df_at_hookintervals = None\n",
        "#   train_losses = None\n",
        "#   validation_losses = None\n",
        "#   try:\n",
        "#       df_at_hookintervals = getattr(trainObj, \"history_surfaces_hooks\")\n",
        "#       if df_at_hookintervals is not None:\n",
        "#         df_at_hookintervals.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationHook_{trainObj.hook_interval}_{model_id_str}.csv\", index=False)\n",
        "#   except AttributeError:\n",
        "#       print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"history_surfaces_hooks\"))\n",
        "\n",
        "#   try:\n",
        "#       train_losses = getattr(trainObj,\"train_losses\")\n",
        "#       if train_losses is not None:\n",
        "#         train_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/trainlosses_{model_id_str}.csv\", sep = ',')    \n",
        "#   except AttributeError:\n",
        "#       print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n",
        "#       # raise NotImplementedError(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n",
        "\n",
        "#   try:\n",
        "#       validation_losses = getattr(trainObj,\"validation_losses\")\n",
        "#       if validation_losses is not None:\n",
        "#         validation_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationlosses_{model_id_str}.csv\", sep = ',')    \n",
        "#   except AttributeError:\n",
        "#       print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"validation_losses\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipogSsVTbv0k",
        "outputId": "273551c6-6661-4b1e-813b-3e6aa9223f33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 136 ms (started: 2022-07-10 17:59:20 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def save_model_train_stratified(lr, net,  eqLossFn, sample_method, trainObj, eqType, eqObject = None ):\n",
        "\n",
        "  model_id_str =  f\"{eqType}_{datetime.datetime.now():%Y%m%d%H%M%S}_{eqLossFn}_{sample_method}_{trainObj.stop_epoch}_{str(lr).replace('.','p')}_{net.NL}_{net.NN}\"\n",
        "  \n",
        "  if eqObject is not None:\n",
        "    try:\n",
        "        beta = getattr(eqObject,\"beta\")\n",
        "        beta_str = str(beta).replace('.','p')\n",
        "        model_id_str = model_id_str + f\"_beta{beta_str}\"\n",
        "    except AttributeError:\n",
        "        pass\n",
        "    try:\n",
        "        wgamma = getattr(eqObject,\"wgamma\")\n",
        "        wgamma_str = str(wgamma).replace('.','p')\n",
        "        model_id_str = model_id_str + f\"_gamma{wgamma_str}\"\n",
        "    except AttributeError:\n",
        "        pass\n",
        "    try:\n",
        "        xbreaks = getattr(eqObject,\"xbreaks\")\n",
        "        xbreaks_str = str(len(xbreaks))\n",
        "        model_id_str = model_id_str + f\"_StSaXbrks{xbreaks_str}\"\n",
        "    except AttributeError:\n",
        "        pass\n",
        "    try:\n",
        "        tbreaks = getattr(eqObject,\"tbreaks\")\n",
        "        tbreaks_str = str(len(tbreaks))\n",
        "        model_id_str = model_id_str + f\"_StSaTbrks{tbreaks_str}\"\n",
        "    except AttributeError:\n",
        "        pass\n",
        "  \n",
        "  torch.save(net.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{model_id_str}\")\n",
        "  df_at_hookintervals = None\n",
        "  train_losses = None\n",
        "  validation_losses = None\n",
        "  try:\n",
        "      df_at_hookintervals = getattr(trainObj, \"history_surfaces_hooks\")\n",
        "      if df_at_hookintervals is not None:\n",
        "        df_at_hookintervals.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationHook_{trainObj.hook_interval}_{model_id_str}.csv\", index=False)\n",
        "  except AttributeError:\n",
        "      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"history_surfaces_hooks\"))\n",
        "\n",
        "  try:\n",
        "      train_losses = getattr(trainObj,\"train_losses\")\n",
        "      if train_losses is not None:\n",
        "        train_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/trainlosses_{model_id_str}.csv\", sep = ',')    \n",
        "  except AttributeError:\n",
        "      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n",
        "      # raise NotImplementedError(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n",
        "\n",
        "  try:\n",
        "      validation_losses = getattr(trainObj,\"validation_losses\")\n",
        "      if validation_losses is not None:\n",
        "        validation_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationlosses_{model_id_str}.csv\", sep = ',')    \n",
        "  except AttributeError:\n",
        "      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"validation_losses\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz5tUJuYaXKu"
      },
      "source": [
        "### Merton Invest-Consumption Problem - Equation HJB optimization\n",
        "\n",
        "[Extensions of the Deep Galerkin Method](https://arxiv.org/pdf/1912.01455v3.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-GO35FcJPP6"
      },
      "source": [
        "##### Closed form terminal utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cWoRXs02PoF",
        "outputId": "f58becd8-406e-4009-ab2f-cf58ecce3040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 6.26 ms (started: 2022-07-10 17:59:20 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def expTerminalUtilityOfWealth(x, gamma_discount = 1):\n",
        "  return(-1*torch.exp(-gamma_discount*x))\n",
        "\n",
        "def expTerminalUtilityOfWealth_np(x, gamma_discount = 1):\n",
        "  return(-np.exp(-gamma_discount*x))\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "# should give a closed form solution for the control => PI(x,t) = [(mu-r)/(gamma*sigma^2)]*exp(-r*(T-t))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrivvbmubiiY"
      },
      "source": [
        "#### MertonUtilityNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRraqOG4aXKx",
        "outputId": "ede98dfa-5f4d-4b5d-9cca-654581808360"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 70.3 ms (started: 2022-07-10 17:59:20 +00:00)\n"
          ]
        }
      ],
      "source": [
        "class MertonUtilityNet(nn.Module):\n",
        "    def __init__(self , NL  , NN, activation = torch.tanh):\n",
        "        super(MertonUtilityNet, self).__init__()\n",
        "        self.NL = NL\n",
        "        self.NN = NN\n",
        "        self.Input = 5  # wealth, time, mu, r, sigma, pi\n",
        "        self.fc_input = nn.Linear(self.Input,self.NN)\n",
        "        torch.nn.init.xavier_uniform_(self.fc_input.weight)\n",
        "        self.linears = nn.ModuleList([nn.Linear(self.NN, self.NN) for i in range(self.NL)])\n",
        "        for i, l in enumerate(self.linears):    \n",
        "            torch.nn.init.xavier_uniform_(l.weight)\n",
        "        self.fc_output = nn.Linear(self.NN,1)\n",
        "        torch.nn.init.xavier_uniform_(self.fc_output.weight)\n",
        "        self.act = activation\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h = self.act(self.fc_input(x))\n",
        "        for i, l in enumerate(self.linears):\n",
        "            h = self.act( l(h) )\n",
        "        out = self.fc_output(h)\n",
        "        return out "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyFbPZr7I5RE"
      },
      "source": [
        "#### MertonPiNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PR7PHL4KI1S9",
        "outputId": "1f54b443-9b76-4bea-d199-b612c30f0409"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 44.8 ms (started: 2022-07-10 17:59:20 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class MertonPiNet(nn.Module):\n",
        "    def __init__(self , NL  , NN, activation = torch.relu  ):\n",
        "        super(MertonPiNet, self).__init__()\n",
        "        self.NL = NL\n",
        "        self.NN = NN\n",
        "        self.Input = 5   # wealth, time, mu, r, sigma\n",
        "        self.fc_input = nn.Linear(self.Input,self.NN)\n",
        "        torch.nn.init.xavier_uniform_(self.fc_input.weight)\n",
        "        self.linears = nn.ModuleList([nn.Linear(self.NN, self.NN) for i in range(self.NL)])\n",
        "        for i, l in enumerate(self.linears):    \n",
        "            torch.nn.init.xavier_uniform_(l.weight)            \n",
        "        # self.fc_output_d = nn.Linear(self.NN, 2)\n",
        "        # self.fc_output = torch.nn.Softmax(dim=1)\n",
        "        self.fc_output = nn.Linear(self.NN, 1)\n",
        "        torch.nn.init.xavier_uniform_(self.fc_output.weight)\n",
        "        self.act = activation\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h = self.act( self.fc_input(x)  )\n",
        "        for i, l in enumerate(self.linears):\n",
        "            h = self.act( l(h) )\n",
        "        # out = self.fc_output_d(h)\n",
        "        out = self.fc_output(h)\n",
        "        return out\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNhAbZ727RC_"
      },
      "source": [
        "#### MertonAlternativePiNet\n",
        "\n",
        "[implement from github](https://github.com/Plemeur/DGM/blob/master/first_net.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c-dZ5b37NwV",
        "outputId": "04870643-df2a-4e7d-fd53-3d64d42232ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 313 ms (started: 2022-07-10 17:59:20 +00:00)\n"
          ]
        }
      ],
      "source": [
        "class LinearWithXavier(nn.Module):\n",
        "    \"\"\" Copy of linear module from Pytorch, modified to have a Xavier init,\n",
        "        TODO : figure out what to do with the bias\"\"\"\n",
        "    def __init__(self, in_features, out_features, bias=True, batch_normalize=True):\n",
        "        super(LinearWithXavier, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.batch_normalize = batch_normalize\n",
        "        \n",
        "        if self.batch_normalize == True:\n",
        "          self.batch_norm = torch.nn.BatchNorm1d(out_features)\n",
        "        \n",
        "        if bias:\n",
        "            self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "    \n",
        "    def reset_parameters(self):\n",
        "        torch.nn.init.xavier_uniform_(self.weight)\n",
        "        if self.bias is not None:\n",
        "            torch.nn.init.uniform_(self.bias, -1, 1) #boundary matter?\n",
        "    \n",
        "    def forward(self, input):\n",
        "        if self.batch_normalize == True:\n",
        "          return self.batch_norm(torch.nn.functional.linear(input, self.weight, self.bias))\n",
        "        return torch.nn.functional.linear(input, self.weight, self.bias)\n",
        "    \n",
        "    def extra_repr(self):\n",
        "        return 'in_features={}, out_features={}, bias={}'.format(\n",
        "            self.in_features, self.out_features, self.bias is not None\n",
        "        )\n",
        "\n",
        "\n",
        "class DGM_layer(nn.Module):\n",
        "    \"\"\" See readme for paper source\"\"\"\n",
        "    def __init__(self, in_features, out_feature, residual=False, batch_normalize=False):\n",
        "        super(DGM_layer, self).__init__()\n",
        "        self.residual = residual\n",
        "\n",
        "        self.Z = LinearWithXavier(out_feature, out_feature, batch_normalize=batch_normalize)\n",
        "        self.UZ = LinearWithXavier(in_features, out_feature, bias=False, batch_normalize=batch_normalize)\n",
        "        self.G = LinearWithXavier(out_feature, out_feature, batch_normalize=batch_normalize)\n",
        "        self.UG = LinearWithXavier(in_features, out_feature, bias=False, batch_normalize=batch_normalize)\n",
        "        self.R = LinearWithXavier(out_feature, out_feature, batch_normalize=batch_normalize)\n",
        "        self.UR = LinearWithXavier(in_features, out_feature, bias=False, batch_normalize=batch_normalize)\n",
        "        self.H = LinearWithXavier(out_feature, out_feature, batch_normalize=batch_normalize)\n",
        "        self.UH = LinearWithXavier(in_features, out_feature, bias=False, batch_normalize=batch_normalize)\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        z = torch.tanh(self.UZ(x) + self.Z(s))\n",
        "        g = torch.tanh(self.UG(x) + self.G(s))\n",
        "        r = torch.tanh(self.UR(x) + self.R(s))\n",
        "        h = torch.tanh(self.UH(x) + self.H(s * r))\n",
        "        return (1 - g) * h + z * s\n",
        "\n",
        "\n",
        "class MertonAlternativePiNet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_size, out_size, neurons, depth):\n",
        "        super(MertonAlternativePiNet, self).__init__()\n",
        "        self.neurons=neurons\n",
        "        self.depth=depth\n",
        "\n",
        "        self.dim = in_size\n",
        "        self.input_layer = LinearWithXavier(in_size, neurons)\n",
        "        self.middle_layer = nn.ModuleList([DGM_layer(in_size, neurons) for i in range(depth)])\n",
        "        # self.middle_layer_2 = nn.ModuleList([DGM_layer(in_size, neurons, batch_normalize=False) for i in range(2)])\n",
        "        self.final_layer = LinearWithXavier(neurons, out_size, batch_normalize=False)\n",
        "\n",
        "    def forward(self, X):\n",
        "        s = torch.tanh(self.input_layer(X))\n",
        "        for i, layer in enumerate(self.middle_layer):\n",
        "            s = torch.tanh(layer(X, s))\n",
        "        \n",
        "        # for i, layer in enumerate(self.middle_layer_2):\n",
        "        #     s = torch.tanh(layer(X, s))\n",
        "        \n",
        "        # s = torch.nn.functional.gelu(self.input_layer(X))\n",
        "        # for i, layer in enumerate(self.middle_layer):\n",
        "        #     s = torch.nn.functional.elu(layer(X, s))\n",
        "        # for i, layer in enumerate(self.middle_layer):\n",
        "        #     s = torch.nn.functional.gelu(layer(X, s))\n",
        "        # for i, layer in enumerate(self.middle_layer_2):\n",
        "        #     s = torch.nn.functional.gelu(layer(X, s))\n",
        "\n",
        "        return self.final_layer(s)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNsqOm1ithSG"
      },
      "source": [
        "#### MertonMatchPiNet\n",
        "\n",
        "[Matching Paper by hand](https://arxiv.org/abs/1912.01455v3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REWml4aYthSH",
        "outputId": "84b7c484-7f39-4c4d-aeb6-6463b350c569"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 106 ms (started: 2022-07-10 17:59:21 +00:00)\n"
          ]
        }
      ],
      "source": [
        "class DGMLayerPaper(nn.Module):\n",
        "\n",
        "    def __init__(self, in_features, out_feature, activation=torch.relu, residual=False):\n",
        "        \n",
        "        super(DGMLayerPaper, self).__init__()\n",
        "        self.residual = residual\n",
        "        self.activation = activation\n",
        "\n",
        "        self.Z = LinearWithXavier(out_feature, out_feature) # w.S\n",
        "        self.UZ = LinearWithXavier(in_features, out_feature, bias=True) # u.x\n",
        "        self.G = LinearWithXavier(out_feature, out_feature)\n",
        "        self.UG = LinearWithXavier(in_features, out_feature, bias=True)\n",
        "        self.R = LinearWithXavier(out_feature, out_feature)\n",
        "        self.UR = LinearWithXavier(in_features, out_feature, bias=True)\n",
        "        self.H = LinearWithXavier(out_feature, out_feature) # w.(S(o)R)\n",
        "        self.UH = LinearWithXavier(in_features, out_feature, bias=True)\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        z = self.activation(self.UZ(x) + self.Z(s))\n",
        "        g = self.activation(self.UG(x) + self.G(s))\n",
        "        r = self.activation(self.UR(x) + self.R(s))\n",
        "        h = self.activation(self.UH(x) + self.H(s * r))\n",
        "        return (1 - g) * h + z * s\n",
        "\n",
        "\n",
        "class MertonMatchPiNet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_size, out_size, neurons, depth):\n",
        "        super(MertonMatchPiNet, self).__init__()\n",
        "        self.dim = in_size\n",
        "        self.input_layer = LinearWithXavier(in_size, neurons)\n",
        "        self.middle_layer = nn.ModuleList([DGMLayerPaper(in_size, neurons) for i in range(depth)])\n",
        "        self.final_layer = LinearWithXavier(neurons, out_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        s = torch.tanh(self.input_layer(X))\n",
        "        for i, layer in enumerate(self.middle_layer):\n",
        "            s = torch.tanh(layer(X, s))\n",
        "\n",
        "        return self.final_layer(s)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wClW1g9rbm8o"
      },
      "source": [
        "#### PiEquation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQTTkUhWuiJi",
        "outputId": "75c45c68-0729-4ce1-f772-cb418158e60a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 97.4 ms (started: 2022-07-10 17:59:21 +00:00)\n"
          ]
        }
      ],
      "source": [
        "class PiEquation():\n",
        "\n",
        "    def __init__(self, pi_net, du_dx, d2u_dx2, curr_epoch=0):\n",
        "        self.pi_net = pi_net\n",
        "        self.grads = du_dx, d2u_dx2\n",
        "        self.curr_epoch = curr_epoch\n",
        "\n",
        "        #self.wgamma = 0.0001\n",
        "        #seld.du_dx = Variable(du_dx, requires_grad=False)\n",
        "        #self.d2u_dx2 = Variable(d2u_dx2, requires_grad=False)\n",
        "\n",
        "    def criterion(self, x_internal, grads):\n",
        "      \n",
        "      #du_dx = Variable(grads[1], requires_grad=False)   # the derivatives with respect to u are held constant when finding pi \n",
        "      #d2u_dx2 = Variable(grads[2], requires_grad=False) # du_dt is grads[0], not relevant here\n",
        "      du_dx, d2u_dx2 = grads\n",
        "\n",
        "      #print('gradient shapes:', du_dx.shape, d2u_dx2.shape)\n",
        "\n",
        "      pi_net_preds = self.pi_net(x_internal)\n",
        "      pi_net_preds = pi_net_preds.reshape(-1,1)\n",
        "      #print('pi_net prediction shape:', pi_net_preds.shape)\n",
        "      \n",
        "      intC = None\n",
        "      #  time, wealth, mu, r, sigma\n",
        "      if len(x_internal) == 0:\n",
        "        intC_loss = torch.tensor(0).cuda().float()  \n",
        "      else:\n",
        "        \n",
        "        # pi * (mu-r)\n",
        "        loss_1 = pi_net_preds*(x_internal[:,2].reshape(-1,1) - x_internal[:,3].reshape(-1,1))\n",
        "        #print('loss1 shape:', loss_1.shape)\n",
        "        \n",
        "        # r * wealth\n",
        "        loss_2 = x_internal[:,3].reshape(-1,1)*x_internal[:,1].reshape(-1,1)\n",
        "        #print('loss2 shape:', loss_2.shape)\n",
        "        \n",
        "        # sigma^2 * pi^2\n",
        "        loss_3 = (x_internal[:,4].reshape(-1,1)**2)*(pi_net_preds**2)\n",
        "        #print('loss3 shape:', loss_3.shape)\n",
        "        \n",
        "        intC_loss = -(loss_1 +loss_2)*du_dx - 0.5*loss_3*d2u_dx2\n",
        "        #print('total internal loss shape:', intC_loss.shape)\n",
        "\n",
        "      return intC_loss\n",
        "\n",
        "    def calculatePiLoss(self, x_internal, grads, keep_batch = False):\n",
        "        '''\n",
        "        Helper function that Sample and Calculate loss,\n",
        "        '''        \n",
        "        #x_internal = Variable( x_internal , requires_grad=True)\n",
        "        Ls = self.criterion( x_internal, grads)\n",
        "        \n",
        "        if not keep_batch:          \n",
        "          return torch.mean(Ls)           \n",
        "        else:\n",
        "          return Ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RRoBgFQINMv"
      },
      "source": [
        "#### TrainInternalPiWithDGM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VREn3fanpP1b",
        "outputId": "b19b5dc7-207e-453a-e515-6d37838676b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 499 ms (started: 2022-07-10 17:59:21 +00:00)\n"
          ]
        }
      ],
      "source": [
        "class TrainInternalPiWithDGM():\n",
        "    \n",
        "    def __init__(self, u_equation, pi_equation, BATCH_SIZE, epoch, lr, debug = False, loss_multiply = 1.0):\n",
        "        self.BATCH_SIZE = BATCH_SIZE\n",
        "        self.u_model = u_equation        \n",
        "        self.pi_model = pi_equation\n",
        "        \n",
        "        self.debug = debug  \n",
        "        self.hook_interval = 100    \n",
        "        \n",
        "        if self.debug == True:\n",
        "            self.hooks = {}            \n",
        "            self.get_all_layers(self.pi_model.pi_net)\n",
        "\n",
        "        self.optimizer_used = optim.Adam\n",
        "\n",
        "        #self.use_early_stop = False\n",
        "        #self.early_stop_patience = 10\n",
        "        #self.early_stop_delta = 0.0        \n",
        "        #self.best_loss = np.Inf\n",
        "        #self.early_stop_counter = 0\n",
        "\n",
        "        self.stop_epoch = 0\n",
        "\n",
        "        #self.validation_sample = None\n",
        "        #self.validation_losses = None\n",
        "        #self.train_losses = None        \n",
        "\n",
        "        self.epoch = epoch\n",
        "        self.lr = lr\n",
        "\n",
        "        #self.loss_multiply = loss_multiply\n",
        "\n",
        "        \n",
        "    def train(self , eqLossFn = 'calculatePiLoss', sample_method_X = \"U\", sample_batch = None):\n",
        "        \n",
        "        #if self.validation_sample is not None:\n",
        "        #  self.validation_losses = np.ones((self.epoch, 3 ), dtype='float32') * np.nan\n",
        "        \n",
        "        #self.train_losses = np.ones((self.epoch, 1 ), dtype='float32') * np.nan\n",
        "\n",
        "        # set it to train mode\n",
        "        self.pi_model.pi_net.train()\n",
        "        \n",
        "        optimizer = self.optimizer_used(self.pi_model.pi_net.parameters(), self.lr)\n",
        "        if self.u_model.pi_optim_state is not None:\n",
        "            optimizer.load_state_dict(self.u_model.pi_optim_state)  ## check \n",
        "        \n",
        "        #if self.u_model.pi_optim_state is None:\n",
        "        #    optimizer = self.optimizer_used(self.pi_model.pi_net.parameters(), self.lr)\n",
        "        #else:\n",
        "        #    optimizer = self.u_model.pi_optim_state\n",
        "\n",
        "\n",
        "        loss_avg = 0.0\n",
        "        loss_calc_method = None\n",
        "        \n",
        "        try:\n",
        "            loss_calc_method = getattr(self.pi_model, eqLossFn)\n",
        "        except AttributeError:\n",
        "            raise NotImplementedError(\"Class `{}` does not implement `{}`\".format(self.pi_model.__class__.__name__, eqLossFn))\n",
        "        \n",
        "        if sample_batch is None:\n",
        "            sample_batch = self.u_model.sample(sample_method_X = sample_method_X, size=self.BATCH_SIZE)\n",
        "\n",
        "        x_internal, _ = sample_batch\n",
        "\n",
        "        for e in range(self.epoch):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "  \n",
        "\n",
        "            loss  = 1e4 * loss_calc_method(x_internal, self.pi_model.grads, keep_batch = False )            \n",
        "            # print(f\"Pi Net Epoch {e} Loss {round(loss.item(),5)}\")\n",
        "            \n",
        "            #self.train_losses[e,:] = [ to_cpu_detach(loss) ]\n",
        "\n",
        "            #if self.debug == True and (self.validation_sample is not None):\n",
        "            #  losses_L2_validation, losses_ABS_validation, losses_Huber_valiation = loss_calc_method( self.validation_sample, \n",
        "            #                                                                                         loss_transforms = [ torch.square, torch.abs, partial(huber_loss_zero_target, delta=0.5) ], \n",
        "            #                                                                                         keep_batch = False )\n",
        "            #  validation_loss_list = [*to_cpu_detach(losses_L2_validation),\n",
        "            #                          *to_cpu_detach(losses_ABS_validation),\n",
        "            #                          *to_cpu_detach(losses_Huber_valiation)]\n",
        "            #  self.validation_losses[e,:] = validation_loss_list\n",
        "            #\n",
        "            #if self.use_early_stop:\n",
        "            #  loss_to_check = loss\n",
        "            #  if loss_to_check < (self.best_loss-self.early_stop_delta):\n",
        "            #    self.best_loss = loss_to_check\n",
        "            #    self.early_stop_counter = 0\n",
        "            #  else:\n",
        "            #    self.early_stop_counter += 1\n",
        "            #  if self.early_stop_counter>=self.early_stop_patience:\n",
        "            #    # print(f\"Pi Early Stop at epoch {e}: {loss_to_check} with patience {self.early_stop_patience}\")\n",
        "            #    break\n",
        "            \n",
        "            \n",
        "            loss_avg = loss_avg + float(loss.item())\n",
        "\n",
        "            # print('self.epoch:', e)\n",
        "            loss.backward()\n",
        "            # for param in self.pi_model.pi_net.parameters():\n",
        "            #   print('pi grad', param.grad)\n",
        "            optimizer.step()\n",
        "            \n",
        "            #if (e % self.hook_interval == (self.hook_interval-1)) or e == 0:\n",
        "            if (self.pi_model.curr_epoch % self.hook_interval == (self.hook_interval-1)) and e==0:\n",
        "\n",
        "                loss_avg = loss_avg/self.hook_interval\n",
        "                #print(\"Unet Epoch {} - Pi Epoch {} - lr {} -  key loss: {}\".format(self.pi_model.curr_epoch, e , self.lr , loss_avg))\n",
        "                print(\"Pi Epoch {} - lr {} -  key loss: {}\".format(self.pi_model.curr_epoch, self.lr , loss_avg))\n",
        "                loss_avg = 0.0\n",
        "        \n",
        "        self.u_model.pi_optim_state = optimizer.state_dict()\n",
        "        self.stop_epoch = e\n",
        "\n",
        "    def hook_fn(self, m, i, o):\n",
        "              self.hooks[m] = o.detach()\n",
        "            \n",
        "    def get_all_layers(self, net):\n",
        "      for name, layer in net._modules.items():\n",
        "          if isinstance(layer, nn.ModuleList):\n",
        "              for n , l in layer.named_children():\n",
        "                l.register_forward_hook(self.hook_fn)\n",
        "          else:\n",
        "              # it's a non sequential. Register a hook\n",
        "              layer.register_forward_hook(self.hook_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp4BG1ewKF6o"
      },
      "source": [
        "#### MertonEquation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBMZYQSPaXKy",
        "outputId": "f4a2d969-c59e-4df2-f85c-608ff5bf6579"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.13 s (started: 2022-07-10 17:59:21 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "class MertonEquation():\n",
        "    \n",
        "    def __init__(self , u_net, pi_net, pi_net_epoch, pi_net_lr, term_utility_function = partial(expTerminalUtilityOfWealth, gamma_discount=1)):\n",
        "\n",
        "        self.u_net = u_net\n",
        "        self.pi_net = pi_net\n",
        "\n",
        "        self.wgamma = 0.0001\n",
        "        self.term_utility_func = term_utility_function\n",
        "        self.xbreaks = None\n",
        "        self.tbreaks = None\n",
        "\n",
        "        self.MAX_X = 1.0\n",
        "        self.T = 1.0\n",
        "        self.MAX_MU = 0.2\n",
        "        self.MAX_SIGMA = 0.5\n",
        "\n",
        "        self.pi_net_epoch = pi_net_epoch\n",
        "        self.pi_net_lr = pi_net_lr\n",
        "        #self.loss_multiply = 1.0\n",
        "\n",
        "        #self.FORCE_MU = None\n",
        "        #self.FORCE_R = None\n",
        "        #self.FORCE_SIGMA = None\n",
        "\n",
        "        self.epoch_of_u = None\n",
        "        #self.adapt_pi_epochs = False\n",
        "        #self.start_adapt_epochs = [ [500, 1000, 2000, 5000, 10000, 15000 ], [4, 8, 10 ,20, 40, 60] ]\n",
        "        #self.was_loss_beaten = False\n",
        "        self.pi_optim_state = None\n",
        "\n",
        "        #self.pi_net.train()\n",
        "\n",
        "    def g(self,x):\n",
        "        # Time, Wealth, Mu, R, Sigma\n",
        "        return self.term_utility_func(x[:,1].reshape(-1,1))\n",
        "\n",
        "    @staticmethod\n",
        "    def to_device(x, to_cpu):\n",
        "      if to_cpu:\n",
        "        return x.cpu()\n",
        "      else:\n",
        "        return x.cuda()\n",
        "\n",
        "    def mu_r_sample(self, size):\n",
        "      mu_candidate = self.MAX_MU * torch.rand([size, 1])\n",
        "      r_candidate = mu_candidate * torch.rand([size, 1])  # ensuring that r is lower than mu\n",
        "      return mu_candidate, r_candidate\n",
        "\n",
        "      # r_sample = torch.where(r_candidate < mu_candidate, r_candidate, mu_candidate)\n",
        "      # mu_sample = torch.where(r_candidate > mu_candidate, r_candidate, mu_candidate)\n",
        "      # return (mu_sample, r_sample)\n",
        "\n",
        "    def apply_forced_mu_r_sigma(self, mu_sample, r_sample, sigma_sample):\n",
        "      if self.FORCE_MU is not None:\n",
        "         mu_sample = self.FORCE_MU*torch.ones_like(mu_sample)            \n",
        "      if self.FORCE_R is not None:\n",
        "        r_sample = self.FORCE_R*torch.ones_like(r_sample)\n",
        "      if self.FORCE_SIGMA is not None:\n",
        "        sigma_sample = self.FORCE_SIGMA*torch.ones_like(sigma_sample)\n",
        "      return mu_sample, r_sample, sigma_sample\n",
        "\n",
        "\n",
        "    def sample(self , sample_method_X = \"U\", size = 2**8, to_cpu = False ):\n",
        "        '''\n",
        "        Sampling function\n",
        "        '''\n",
        "        if sample_method_X in [\"U\"]:\n",
        "            #range_multiplier = 1.0\n",
        "            \n",
        "            ### internal samples of Time, Wealth, Mu, R, Sigma\n",
        "            mu_sample_internal, r_sample_internal = self.mu_r_sample(size)\n",
        "            sigma_sample_internal =  0.2 + self.MAX_SIGMA * (1. - torch.rand([size, 1]))\n",
        "            #mu_sample_internal, r_sample_internal, sigma_sample_internal = self.apply_forced_mu_r_sigma(mu_sample_internal, r_sample_internal, sigma_sample_internal)\n",
        "            x_internal = self.to_device(torch.cat(( (1 - torch.rand([size,1])) * self.T, # time to maturity\n",
        "                                                    self.MAX_X * (1 - torch.rand([size, 1])), # Wealth\n",
        "                                                    mu_sample_internal, # mu\n",
        "                                                    r_sample_internal, # R\n",
        "                                                    sigma_sample_internal # Sigma\n",
        "                                                   ) , dim = 1 ),to_cpu)\n",
        "            ### Terminal time samples\n",
        "            mu_sample_terminal, r_sample_terminal = self.mu_r_sample(size)\n",
        "            sigma_sample_terminal =  0.2 + self.MAX_SIGMA * (1. - torch.rand([size, 1]))\n",
        "            #mu_sample_terminal, r_sample_terminal, sigma_sample_terminal = self.apply_forced_mu_r_sigma(mu_sample_terminal, r_sample_terminal, sigma_sample_terminal)\n",
        "            x_terminal = self.to_device(torch.cat(( torch.zeros(size, 1), # time to maturity\n",
        "                                                   self.MAX_X * (1 - torch.rand([size, 1])), # Wealth\n",
        "                                                    mu_sample_terminal, # mu\n",
        "                                                    r_sample_terminal, # R\n",
        "                                                    sigma_sample_terminal # Sigma\n",
        "                                                   ) , dim = 1 ),to_cpu)\n",
        "            \n",
        "            # x_initial = torch.cat( ( torch.zeros(size, 1), -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier) , dim = 1 ).cuda()\n",
        "            return x_internal , x_terminal\n",
        "\n",
        "        raise ValueError(f\"{sample_method_X} is not a supported sampling method\")\n",
        "        \n",
        "    def sample_stratified(self , sample_method_X = \"U\", size = 2**8, to_cpu = False ):\n",
        "\n",
        "      if self.xbreaks is None and self.tbreaks is None:\n",
        "        return self.sample(sample_method_X, size, to_cpu)\n",
        "\n",
        "      internal_strata_xts = []\n",
        "      terminal_strata_xts = []\n",
        "      \n",
        "      if sample_method_X in [\"U\"]:\n",
        "          range_multiplier = 1.0\n",
        "          xbreaks_used = self.xbreaks[:] if self.xbreaks is not None else [0,range_multiplier*self.MAX_X]\n",
        "          tbreaks_used = self.tbreaks[:] if self.tbreaks is not None else [0,self.T]\n",
        "          if xbreaks_used[-1] < range_multiplier*self.MAX_X:\n",
        "            xbreaks_used.append(range_multiplier*self.MAX_X)\n",
        "          while xbreaks_used[0] < 0.0:\n",
        "            xbreaks_used.pop(0)\n",
        "          if not xbreaks_used:\n",
        "            xbreaks_used = [0,range_multiplier*self.MAX_X]\n",
        "          if xbreaks_used[0] > 0.0:            \n",
        "            xbreaks_used.insert(0, 0.0)\n",
        "\n",
        "          if tbreaks_used[-1] < self.T:\n",
        "            tbreaks_used.append(self.T)\n",
        "          xbreaks_range = xbreaks_used[-1]-xbreaks_used[0]\n",
        "          tbreaks_range = tbreaks_used[-1]-tbreaks_used[0]\n",
        "\n",
        "          total_strat_processed = 0\n",
        "\n",
        "          # internal samples\n",
        "          for stratum_x_count in range(len(xbreaks_used)-1):\n",
        "              \n",
        "            num_samples_in_stratum = 0\n",
        "            if len(xbreaks_used) > 2:  # x division takes priority so assign it if there is no T division\n",
        "              range_ratio_x_stratum = (xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count])/xbreaks_range\n",
        "              num_samples_in_stratum = math.ceil(range_ratio_x_stratum*size)\n",
        "\n",
        "            for stratum_t_count in range(len(self.tbreaks)-1):\n",
        "\n",
        "              if num_samples_in_stratum == 0: # there is only a T division, so use it\n",
        "                range_ratio_t_stratum = (tbreaks_used[stratum_t_count+1]-tbreaks_used[stratum_t_count])/tbreaks_range\n",
        "                num_samples_in_stratum = math.ceil(range_ratio_t_stratum*size)\n",
        "              else:\n",
        "                # there is both an X and a T division, assign the number of samples uniformly, assuming same scale of X and T\n",
        "                stratum_coverage_on_unit_square = \\\n",
        "                  ((xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count])/xbreaks_range)*\\\n",
        "                  ((tbreaks_used[stratum_t_count+1]-tbreaks_used[stratum_t_count])/tbreaks_range)\n",
        "                num_samples_in_stratum = math.ceil(stratum_coverage_on_unit_square * size)\n",
        "\n",
        "              range_multiplier = 1.0\n",
        "\n",
        "              ### internal samples of Time, Wealth, Mu, R, Sigma\n",
        "              internal_stratum_t_sample = tbreaks_used[stratum_t_count] + torch.rand([num_samples_in_stratum,1])*(tbreaks_used[stratum_t_count+1]-tbreaks_used[stratum_t_count])\n",
        "              internal_stratum_x_sample = xbreaks_used[stratum_x_count] + torch.rand([num_samples_in_stratum,1])*(xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count])\n",
        "              stratum_mu_sample_internal, stratum_r_sample_internal = self.mu_r_sample(num_samples_in_stratum, range_multiplier)\n",
        "              stratum_sigma_sample_internal = -self.MAX_SIGMA*range_multiplier*torch.rand([num_samples_in_stratum, 1])+self.MAX_SIGMA*range_multiplier\n",
        "              stratum_mu_sample_internal, stratum_r_sample_internal, stratum_sigma_sample_internal = \\\n",
        "                self.apply_forced_mu_r_sigma(stratum_mu_sample_internal, stratum_r_sample_internal, stratum_sigma_sample_internal)\n",
        "              x_internal_stratum = self.to_device(torch.cat(( internal_stratum_t_sample , # Time\n",
        "                                                              internal_stratum_x_sample, # Wealth\n",
        "                                                              stratum_mu_sample_internal, # mu\n",
        "                                                              stratum_r_sample_internal, # R\n",
        "                                                               # Sigma\n",
        "                                                            ) , dim = 1 ),to_cpu)\n",
        "              if not internal_strata_xts: \n",
        "                internal_strata_xts = [ x_internal_stratum ] \n",
        "              else:\n",
        "                internal_strata_xts.append(x_internal_stratum) \n",
        "\n",
        "              ### Terminal time samples\n",
        "              terminal_stratum_x_sample = xbreaks_used[stratum_x_count] + torch.rand([num_samples_in_stratum,1])*(xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count])\n",
        "              stratum_mu_sample_terminal, stratum_r_sample_terminal = self.mu_r_sample(num_samples_in_stratum, range_multiplier)\n",
        "              stratum_sigma_sample_terminal = -self.MAX_SIGMA*range_multiplier*torch.rand([num_samples_in_stratum, 1])+self.MAX_SIGMA*range_multiplier\n",
        "              stratum_mu_sample_terminal, stratum_r_sample_terminal, stratum_sigma_sample_terminal = \\\n",
        "                self.apply_forced_mu_r_sigma(stratum_mu_sample_terminal, stratum_r_sample_terminal, stratum_sigma_sample_terminal)\n",
        "              x_terminal_stratum = self.to_device(torch.cat(( torch.zeros(num_samples_in_stratum, 1) + self.T , # Time\n",
        "                                                      terminal_stratum_x_sample, # Wealth\n",
        "                                                      stratum_mu_sample_terminal, # mu\n",
        "                                                      stratum_r_sample_terminal, # R\n",
        "                                                      stratum_sigma_sample_terminal # Sigma\n",
        "                                                    ) , dim = 1 ),to_cpu)\n",
        "              if not terminal_strata_xts:\n",
        "                terminal_strata_xts = [ x_terminal_stratum ] # terminal_stratum_xt[None,:,:]\n",
        "              else:\n",
        "                terminal_strata_xts.append(x_terminal_stratum) # torch.vstack((terminal_strata_xts,terminal_stratum_xt[None,:,: ]))\n",
        "\n",
        "              total_strat_processed += 1 \n",
        "              # print((len(internal_strata_xts),xbreaks_used[stratum_x_count],tbreaks_used[stratum_t_count]))\n",
        "\n",
        "          # pdb.set_trace()\n",
        "          # x_initial = torch.cat( ( torch.zeros(size, 1), -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier) , dim = 1 ).cuda()\n",
        "          return internal_strata_xts , terminal_strata_xts\n",
        "    \n",
        "      raise ValueError(f\"{sample_method_X} is not a supported sampling method\")\n",
        "\n",
        "    def get_num_pi_epochs(self):\n",
        "      #if self.adapt_pi_epochs and self.was_loss_beaten:\n",
        "      #  if any([ i < self.epoch_of_u for i in self.start_adapt_epochs[0] ]):\n",
        "      #    return self.pi_net_epoch * self.start_adapt_epochs[1][[i for i in range(len(self.start_adapt_epochs[0])) if self.start_adapt_epochs[0][i] < self.epoch_of_u][-1]] \n",
        "      \n",
        "      return self.pi_net_epoch\n",
        "\n",
        "    def get_utility_function_derivatives(self, u_net_val, x_internal, normalize=False):\n",
        "        du = torch.autograd.grad( u_net_val, \n",
        "                                  x_internal, \n",
        "                                  grad_outputs=torch.ones_like(u_net_val),\n",
        "                                  create_graph=True)\n",
        "        \n",
        "        du_dt = du[0][:,0].reshape(-1,1)\n",
        "        du_dx = du[0][:,1].reshape(-1,1)     \n",
        "\n",
        "        d2u_dx2 = torch.autograd.grad(du_dx, \n",
        "                                      x_internal , \n",
        "                                      grad_outputs=torch.ones_like(du_dx),\n",
        "                                      create_graph=True, allow_unused=True\n",
        "                                      )[0][:,1].reshape(-1,1)\n",
        "        \n",
        "        return du_dt, du_dx, d2u_dx2\n",
        "    \n",
        "    def Pi_star(self, x, gamma=1):\n",
        "        lamda = (x[:,2].reshape(-1,1) - x[:,3].reshape(-1,1))/x[:,4].reshape(-1,1)\n",
        "        return lamda / (gamma * x[:,4].reshape(-1,1)) * torch.exp(-1. * x[:,3].reshape(-1,1)*x[:,0].reshape(-1,1))\n",
        "\n",
        "        \n",
        "    def Htx(self, x, gamma=1):\n",
        "      # 3.6a in https://arxiv.org/abs/1912.01455v3\n",
        "      #  wealth * gamma * exp(r*tau)\n",
        "      part_a = x[:,1].reshape(-1,1)*gamma*torch.exp(x[:,3].reshape(-1,1) * x[:,0].reshape(-1,1))\n",
        "      #  0.5* tau *[(mu - r)/sigma]^2\n",
        "      part_b = 0.5 * x[:,0].reshape(-1,1) * ((x[:,2].reshape(-1,1) - x[:,3].reshape(-1,1))/(x[:,4].reshape(-1,1)))**2 \n",
        "      return -1.0*torch.exp(-part_a - part_b)\n",
        "\n",
        "    \n",
        "    def dHdx(self, x, gamma=1):\n",
        "        \n",
        "        return self.Htx(x) * (-1 * gamma*torch.exp(x[:,3].reshape(-1,1) * x[:,0].reshape(-1,1)))\n",
        "\n",
        "    \n",
        "    def d2Hdx2(self, x, gamma=1):\n",
        "        \n",
        "        return self.Htx(x) * gamma**2 * torch.exp(2* x[:,3].reshape(-1,1) * x[:,0].reshape(-1,1))\n",
        "\n",
        "\n",
        "    def criterion(self, x_internal, x_terminal, loss_transforms = [torch.square], util_network=None):\n",
        "        '''\n",
        "        Loss function that helps network find solution to equation\n",
        "        '''   \n",
        "        # Time / Wealth / Mu / r / Sigma (sample data order)\n",
        "        # pdb.set_trace()\n",
        "        \n",
        "        u_net_val = util_network(x_internal)\n",
        "        #print(torch.mean(u_net_val), torch.min(u_net_val), torch.max(u_net_val))\n",
        "\n",
        "        \n",
        "        # Trying to used a correct solution to see Pi convergence\n",
        "        #u_net_val = self.Htx(x_internal)\n",
        "        #du_dx, d2u_dx2 = self.dHdx(x_internal), self.d2Hdx2(x_internal)\n",
        "        #du_dt = torch.zeros_like(du_dx)\n",
        "\n",
        "\n",
        "        du_dt, du_dx, d2u_dx2 = self.get_utility_function_derivatives(u_net_val, x_internal)\n",
        "        #print('Unet derivatives shape:', du_dt.shape, du_dx.shape, d2u_dx2.shape)\n",
        "\n",
        "\n",
        "        pi_model = PiEquation(self.pi_net, du_dx.detach(), d2u_dx2.detach())\n",
        "        pi_model.curr_epoch = self.epoch_of_u              \n",
        "        \n",
        "        pi_trainer = TrainInternalPiWithDGM(self, pi_model, \n",
        "                                            x_internal.shape[0], \n",
        "                                            self.get_num_pi_epochs(), \n",
        "                                            self.pi_net_lr, \n",
        "                                            debug=True, \n",
        "                                            loss_multiply=1.0)\n",
        "        #pi_trainer.use_early_stop = True\n",
        "        #pi_trainer.early_stop_patience = min(20,math.ceil(self.pi_net_epoch/10.0))\n",
        "        \n",
        "        pi_trainer.train(sample_batch=(x_internal, x_terminal))\n",
        "        \n",
        "\n",
        "        if loss_transforms is None:\n",
        "          loss_transforms = [torch.square]\n",
        "\n",
        "        intC = None\n",
        "        terC = None\n",
        "\n",
        "        if len(x_internal) == 0:\n",
        "          intC = [ torch.tensor(0).cuda().float() for loss_transform in loss_transforms ] \n",
        "        else:\n",
        "          # Time, Wealth, Mu, R, Sigma\n",
        "          \n",
        "          pi_net_preds = self.pi_net(x_internal)\n",
        "          #pi_net_preds = (x_internal[:,2].reshape(-1,1) - x_internal[:,3].reshape(-1,1)) / (x_internal[:,4].reshape(-1,1) ** 2) * (du_dx / d2u_dx2)\n",
        "          \n",
        "          pi_net_preds = pi_net_preds.detach().reshape(-1,1)\n",
        "          #print('Pi net prediction inside Unet:', pi_net_preds.shape)\n",
        "\n",
        "          # pi*(mu-r)\n",
        "          intC_loss_1 = pi_net_preds*(x_internal[:,2].reshape(-1,1) - x_internal[:,3].reshape(-1,1)) \n",
        "          #print('interim loss1 in unet shape:', intC_loss_1.shape)\n",
        "          \n",
        "          # r * wealth\n",
        "          intC_loss_2 = x_internal[:,3].reshape(-1,1)*x_internal[:,1].reshape(-1,1)  \n",
        "          #print('interim loss2 in unet shape:', intC_loss_2.shape)\n",
        "          \n",
        "          # sigma^2 * pi^2\n",
        "          intC_loss_3 = (x_internal[:,4].reshape(-1,1)**2)*(pi_net_preds**2)\n",
        "          #print('interim loss3 in unet shape:', intC_loss_3.shape)\n",
        "          \n",
        "          # it is minus in front of time derivative as it is time to maturity instead of elapsed time\n",
        "          intC_loss = -du_dt + (intC_loss_1 + intC_loss_2)*du_dx + 0.5*intC_loss_3*d2u_dx2\n",
        "          \n",
        "          #print(torch.mean(intC_loss), torch.min(intC_loss), torch.max(intC_loss))\n",
        "          #print('interim total loss in unet shape:', intC_loss.shape)\n",
        "\n",
        "          intC = [ loss_transform(intC_loss) for loss_transform in loss_transforms ] \n",
        "        \n",
        "        \n",
        "        #print('Terminal condition shapes:')\n",
        "        #print(self.g(x_terminal).shape)\n",
        "        #print(util_network(x_terminal).shape)\n",
        "\n",
        "        terC = [ loss_transform(self.g(x_terminal) - util_network(x_terminal)) for loss_transform in loss_transforms ]\n",
        "        \n",
        "        # print('mean d2u_dx2:', d2u_dx2.mean(),'max d2u_dx2:', d2u_dx2.max())\n",
        "        # print('mean du_dx:', du_dx.mean(),'max du_dx:', d2u_dx2.max())\n",
        "        # print('mean du_dt:', du_dt.mean(),'max d2u_dx2:', d2u_dx2.max())\n",
        "        \n",
        "        return intC, terC\n",
        "\n",
        "    def calculateLoss(self, batch_x, train = True, loss_transforms = [ torch.square ], keep_batch = False, util_network=None):\n",
        "        '''\n",
        "        Helper function that Sample and Calculate loss,\n",
        "        '''        \n",
        "        # pdb.set_trace()\n",
        "        x_internal , x_terminal = batch_x\n",
        "        x_internal = Variable( x_internal , requires_grad=True)\n",
        "        \n",
        "        #x_terminal = Variable( x_terminal , requires_grad=True)\n",
        "        # print('MertonEquation calling self.criterion')\n",
        "        Ls = self.criterion(x_internal , x_terminal, loss_transforms = loss_transforms, util_network=util_network)\n",
        "        intC, terC = Ls\n",
        "\n",
        "        return_losses = []\n",
        "        # print('internal Loss', torch.mean(intC[0]))\n",
        "        # print('external Loss', torch.mean(terC[0]))\n",
        "\n",
        "        for lc in range(len(loss_transforms)):\n",
        "          if not keep_batch:\n",
        "            loss_equalWeightedByType = torch.mean(intC[lc]) + torch.mean(terC[lc])\n",
        "            return_losses.append( [ 1e4 * loss_equalWeightedByType , \n",
        "                                    1e4 * torch.mean(intC[lc]) , \n",
        "                                    1e4 * torch.mean(terC[lc])\n",
        "                                   ] )            \n",
        "          else:\n",
        "            return_losses.append( [intC.numpy(), terC.numpy()] )\n",
        "        return return_losses\n",
        "\n",
        "    def calculateLossUsingKLMinMax(self , batch_x , train = True, loss_transforms = [ torch.square ], keep_batch = False):\n",
        "        '''\n",
        "        Helper function that Samples and Calculate loss,\n",
        "        This is adapted in that it changes the weights on the losses\n",
        "        and the distribution of sampling to maximize the loss provided \n",
        "        the KL distance of the loss is within positive constraints\n",
        "        beta represents the constraints on the weights\n",
        "        gamma represents the constraints on the sampling distribution\n",
        "        (each representing an upper bound the KL distribution)\n",
        "        '''        \n",
        "        # x , x_terminal , x_initial = self.sample(sample_method_X, size)\n",
        "        x , x_terminal = batch_x\n",
        "        x = Variable( x, requires_grad=True)\n",
        "        Ls = self.criterion( x , x_terminal , loss_transforms = loss_transforms)\n",
        "        intC , terC = Ls\n",
        "\n",
        "        if self.weights is None:\n",
        "          self.weights = torch.ones(1,len(Ls)).to(intC[0].device)/len(Ls)\n",
        "        \n",
        "        return_losses = []\n",
        "        for lc in range(len(loss_transforms)):\n",
        "          if not keep_batch:\n",
        "            intCt = self.weights[0,0] * torch.pow((1.0/intC[lc].numel() if intC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.beta * intC[lc])), self.gamma/self.beta) \n",
        "            terCt = self.weights[0,1] * torch.pow((1.0/terC[lc].numel() if terC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.beta * terC[lc])), self.gamma/self.beta) \n",
        "            loss_equalWeightedByType = 100*torch.mean(intC[lc]) + 0.5*torch.mean(terC[lc]) \n",
        "            transformed_loss = 1.0/self.gamma * torch.log(intCt + terCt)\n",
        "            return_losses.append( [ transformed_loss , \n",
        "                                    100*torch.mean(intC[lc]) , 0.5*torch.mean(terC[lc]),\n",
        "                                    loss_equalWeightedByType ] )            \n",
        "          else:\n",
        "            return_losses.append( [intC[lc].numpy(), terC[lc].numpy()] )\n",
        "        return return_losses\n",
        "\n",
        "\n",
        "    def calculateLossKLMinMaxGamma(self , batch_x , train = True, loss_transforms = [ torch.square ], keep_batch = False):\n",
        "        '''\n",
        "        Helper function that Samples and Calculate loss,\n",
        "        This is adapted in that it changes the weights on the losses\n",
        "        and the distribution of sampling to maximize the loss provided \n",
        "        the KL distance of the loss is within positive constraints\n",
        "        beta represents the constraints on the weights\n",
        "        gamma represents the constraints on the sampling distribution\n",
        "        (each representing an upper bound the KL distribution)\n",
        "        '''        \n",
        "        # x , x_terminal , x_initial = self.sample(sample_method_X, size)\n",
        "        x , x_terminal  = batch_x\n",
        "        x = Variable( x, requires_grad=True)\n",
        "        Ls = self.criterion( x , x_terminal, loss_transforms = loss_transforms)\n",
        "        intC , terC  = Ls\n",
        "\n",
        "        if self.weights is None:\n",
        "          self.weights = torch.ones(1,len(Ls)).to(intC[0].device)/len(Ls)\n",
        "        \n",
        "        return_losses = []\n",
        "        for lc in range(len(loss_transforms)):\n",
        "          if not keep_batch:\n",
        "            intCt = self.weights[0,0] * (1.0/intC[lc].numel() if intC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.gamma * intC[lc])) \n",
        "            terCt = self.weights[0,1] * (1.0/terC[lc].numel() if terC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.gamma * terC[lc])) \n",
        "            loss_equalWeightedByType = 0.5*torch.mean(intC[lc]) + 0.5*torch.mean(terC[lc]) \n",
        "            transformed_loss = 1.0/self.gamma * torch.log(intCt + terCt )\n",
        "            return_losses.append( [ transformed_loss , \n",
        "                                    0.5*torch.mean(intC[lc]) , 0.5*torch.mean(terC[lc]),\n",
        "                                    loss_equalWeightedByType ] )            \n",
        "          else:\n",
        "            return_losses.append( [intC[lc].numpy(), terC[lc].numpy()] )\n",
        "        return return_losses\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkqBrz2JplP9"
      },
      "source": [
        "#### Save info from run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDHKw-yFpqMl",
        "outputId": "3d748bcd-3ac8-4978-c67e-2e159cbadff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 49.1 ms (started: 2022-07-10 17:59:25 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def save_model_train(training, eqObject, lr, eqLossFn, sample_method, loss, batch_size, epoch): \n",
        "\n",
        "  model_id_base_str =  f\"{eqObject.__class__.__name__}_{datetime.datetime.now():%Y%m%d%H%M%S}_{batch_size}_{str(torch.round(loss,decimals=2)).replace('.','p')}\"\n",
        "  model_id_base_str = model_id_base_str + f\"_{eqLossFn}_{sample_method}_{epoch}_{str(lr).replace('.','p')}\"  \n",
        "  model_id_base_str = model_id_base_str + f\"_U{eqObject.u_net.neurons}_U{eqObject.u_net.depth}_P{eqObject.pi_net.neurons}_P{eqObject.pi_net.depth}\"\n",
        "  \n",
        "  torch.save(eqObject.u_net.state_dict(),  f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/{model_id_base_str}_U\")\n",
        "  torch.save(eqObject.pi_net.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/{model_id_base_str}_P\")\n",
        "\n",
        "  df_at_hookintervals = None\n",
        "  train_losses = None\n",
        "  validation_losses = None\n",
        "  try:\n",
        "      df_at_hookintervals = getattr(training, \"history_surfaces_hooks\")\n",
        "      if df_at_hookintervals is not None:\n",
        "        df_at_hookintervals.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationHook_{training.hook_interval}_{model_id_base_str}.csv\", index=False)\n",
        "  except AttributeError:\n",
        "      print(\"Class `{}` does not have `{}`\".format(training.__class__.__name__, \"history_surfaces_hooks\"))\n",
        "\n",
        "  try:\n",
        "      train_losses = getattr(training,\"train_losses\")\n",
        "      if train_losses is not None:\n",
        "        train_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/trainlosses_{model_id_base_str}.csv\", sep = ',')    \n",
        "  except AttributeError:\n",
        "      print(\"Class `{}` does not have `{}`\".format(training.__class__.__name__, \"train_losses\"))\n",
        "      # raise NotImplementedError(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n",
        "\n",
        "  try:\n",
        "      validation_losses = getattr(training,\"validation_losses\")\n",
        "      if validation_losses is not None:\n",
        "        validation_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationlosses_{model_id_base_str}.csv\", sep = ',')    \n",
        "  except AttributeError:\n",
        "      print(\"Class `{}` does not have `{}`\".format(training.__class__.__name__, \"validation_losses\"))\n",
        "\n",
        "  import numpy as np\n",
        "\n",
        "  training_dict = dict({ \"BATCH_SIZE\" : training.BATCH_SIZE,\n",
        "                         \"use_early_stop\" : training.use_early_stop,\n",
        "                        \"early_stop_patience\" : training.early_stop_patience,\n",
        "                        \"early_stop_delta\" : training.early_stop_delta,\n",
        "                        \"monitored_loss_type\" : training.monitored_loss_type,\n",
        "                        \"use_early_stop\" : training.use_early_stop,\n",
        "                        \"stop_epoch\" : training.stop_epoch            })\n",
        "  equation_dict = dict({ \"wgamma\" : eqObject.wgamma,\n",
        "                        \"xbreaks\" : eqObject.xbreaks,\n",
        "                        \"tbreaks\" : eqObject.tbreaks,\n",
        "                        \"MAX_X\" : eqObject.MAX_X,\n",
        "                        \"T\" : eqObject.T,\n",
        "                        \"MAX_MU\" : eqObject.MAX_MU,\n",
        "                        \"MAX_SIGMA\" : eqObject.MAX_SIGMA,\n",
        "                        \"pi_net_epoch\" : eqObject.pi_net_epoch,\n",
        "                        \"wgamma\" : eqObject.wgamma,\n",
        "                        \"pi_net_lr\" : eqObject.pi_net_lr,\n",
        "                        \"loss_multiply\" : eqObject.loss_multiply,\n",
        "                        \"epoch_of_u\" : eqObject.epoch_of_u,\n",
        "                        \"adapt_pi_epochs\" : eqObject.adapt_pi_epochs,\n",
        "                        \"start_adapt_epochs\" : eqObject.start_adapt_epochs,\n",
        "                        \"was_loss_beaten\" : eqObject.was_loss_beaten  })\n",
        "\n",
        "  np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/trainingDict_{model_id_base_str}.npy\", training_dict)\n",
        "  np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/equationDict_{model_id_base_str}.npy\", equation_dict) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65nooklCbsdy"
      },
      "source": [
        "#### TrainHJBMertonWithDGM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtO8fV7oaXK2",
        "outputId": "5a489201-c911-4611-a7eb-bdac63bd0d6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.21 s (started: 2022-07-10 17:59:25 +00:00)\n"
          ]
        }
      ],
      "source": [
        "class TrainHJBMertonWithDGM():\n",
        "    \n",
        "    def __init__(self , net , equation , BATCH_SIZE , debug = False):\n",
        "        \n",
        "        #self.history_mean_hooks = [] \n",
        "        #self.history_surfaces_hooks = None       \n",
        "        #self.history_tl = []\n",
        "        #self.history_internal = []\n",
        "        #self.history_terminal = []\n",
        "        #self.history_initial = []              \n",
        "        #self.history_nonzero = []\n",
        "        self.BATCH_SIZE = BATCH_SIZE\n",
        "        self.net = net\n",
        "        self.model = equation        \n",
        "        self.debug = debug  \n",
        "        self.hook_interval = 20      \n",
        "        if self.debug == True:\n",
        "            self.hooks = {}            \n",
        "            self.get_all_layers(self.net)\n",
        "\n",
        "        self.optimizer_used = optim.Adam\n",
        "\n",
        "        #self.use_early_stop = False\n",
        "        #self.early_stop_patience = 10\n",
        "        #self.early_stop_delta = 0.0        \n",
        "        #self.best_loss = np.Inf\n",
        "        #self.monitored_loss_type = \"Train_L2\"\n",
        "        #self.early_stop_counter = 0\n",
        "        self.stop_epoch = 0\n",
        "        #self.validation_sample = None\n",
        "        #self.validation_losses = None\n",
        "        #self.train_losses = None        \n",
        "        \n",
        "    \n",
        "    def Htx(self, x, gamma=1):\n",
        "      # 3.6a in https://arxiv.org/abs/1912.01455v3\n",
        "      #  wealth * gamma * exp(r*tau)\n",
        "      part_a = x[:,1].reshape(-1,1)*gamma*torch.exp(x[:,3].reshape(-1,1) * x[:,0].reshape(-1,1))\n",
        "      #  0.5* tau *[(mu - r)/sigma]^2\n",
        "      part_b = 0.5 * x[:,0].reshape(-1,1) * ((x[:,2].reshape(-1,1) - x[:,3].reshape(-1,1))/(x[:,4].reshape(-1,1)))**2 \n",
        "      return -1.0*torch.exp(-part_a - part_b)\n",
        "\n",
        "    \n",
        "    def Pi_star(self, x, gamma=1):\n",
        "        lamda = (x[:,2].reshape(-1,1) - x[:,3].reshape(-1,1))/x[:,4].reshape(-1,1)\n",
        "        return lamda / (gamma * x[:,4].reshape(-1,1)) * torch.exp(-1. * x[:,3].reshape(-1,1)*x[:,0].reshape(-1,1))\n",
        "\n",
        "    def train(self, \n",
        "              epoch, \n",
        "              lr, \n",
        "              eqLossFn = 'calculateLoss', \n",
        "              sample_method_X = \"U\", \n",
        "              key_loss_func = torch.square, \n",
        "              huber_delta = 0.5):\n",
        "        \n",
        "        #if self.validation_sample is not None:\n",
        "        #  self.validation_losses = np.ones((epoch, 3*4 ), dtype='float32') * np.nan\n",
        "        #self.train_losses = np.ones((epoch, 3 ), dtype='float32') * np.nan\n",
        "        \n",
        "        self.net.train()\n",
        "        optimizer = self.optimizer_used(self.net.parameters(), lr)\n",
        "        \n",
        "        loss_avg = 0.0\n",
        "        loss_calc_method = None\n",
        "        try:\n",
        "            loss_calc_method = getattr(self.model, eqLossFn)\n",
        "        except AttributeError:\n",
        "            raise NotImplementedError(\"Class `{}` does not implement `{}`\".format(self.model.__class__.__name__, eqLossFn))\n",
        "        \n",
        "        #self.model.adapt_pi_epochs = True\n",
        "\n",
        "        # self.epoch_of_u = None\n",
        "        # self.adapt_pi_epochs = False\n",
        "        # self.start_adapt_epochs = [ [500, 1000, 2000, 5000 ], [4, 8, 10 ,20] ]\n",
        "        # self.was_loss_beaten = False\n",
        "\n",
        "\n",
        "        #full_dataset = self.model.sample(sample_method_X = sample_method_X, size=100000)\n",
        "        #full_batch_internal = DataLoader(full_dataset[0], batch_size=self.BATCH_SIZE)\n",
        "        #full_batch_terminal = DataLoader(full_dataset[1], batch_size=self.BATCH_SIZE)\n",
        "\n",
        "        for e in tqdm(range(epoch)):\n",
        "\n",
        "            loss_avg = 0.0\n",
        "            loss_int_avg = 0.0\n",
        "            i = 0\n",
        "            \n",
        "            full_dataset = self.model.sample(sample_method_X = sample_method_X, size=self.BATCH_SIZE)\n",
        "            full_batch_internal = DataLoader(full_dataset[0], batch_size=self.BATCH_SIZE)\n",
        "            full_batch_terminal = DataLoader(full_dataset[1], batch_size=self.BATCH_SIZE)\n",
        "\n",
        "            for sample_int, sample_term in zip(full_batch_internal, full_batch_terminal):\n",
        "                sample_batch = sample_int, sample_term \n",
        "\n",
        "            #batch_size_used = self.BATCH_SIZE\n",
        "            #if self.model.was_loss_beaten and self.model.adapt_pi_epochs:\n",
        "            #  batch_size_used = self.BATCH_SIZE*20\n",
        "            #sample_batch = self.model.sample(sample_method_X = sample_method_X, size=batch_size_used)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                self.model.epoch_of_u = e\n",
        "                losses_L2 = loss_calc_method(sample_batch, loss_transforms = [key_loss_func], keep_batch=False, util_network=self.net )\n",
        "                \n",
        "                # pdb.set_trace()\n",
        "                loss, internal, terminal = losses_L2[0]\n",
        "\n",
        "            #    self.train_losses[e,:] = [ to_cpu_detach(loss), to_cpu_detach(internal), to_cpu_detach(terminal)]\n",
        "            #                           to_cpu_detach(loss_abs) , to_cpu_detach(internal_abs) , to_cpu_detach(terminal_abs), \n",
        "            #                           to_cpu_detach(losses_equalWeightedByType_abs)]\n",
        "\n",
        "            #if self.debug == True and (self.validation_sample is not None):\n",
        "            #  losses_L2_validation, losses_ABS_validation, losses_Huber_valiation = loss_calc_method( self.validation_sample, \n",
        "            #                                                                                         loss_transforms = [ torch.square, torch.abs, partial(huber_loss_zero_target, delta=huber_delta) ], \n",
        "            #                                                                                         keep_batch = False,\n",
        "            #                                                                                         util_network =self.net)\n",
        "            #  validation_loss_list = [*to_cpu_detach(losses_L2_validation),\n",
        "            #                          *to_cpu_detach(losses_ABS_validation),\n",
        "            #                          *to_cpu_detach(losses_Huber_valiation)]\n",
        "              # validation_loss_list = validation_loss_list.pop(5) # the L2 loss is duplicated at index 1\n",
        "            #  self.validation_losses[e,:] = validation_loss_list\n",
        "              # pdb.set_trace()\n",
        "              # print(f\"Epoch {e} - Pi Pred (0.47) {self.model.pi_net(self.validation_sample[0]).item()}\")\n",
        "\n",
        "            \n",
        "            #if self.use_early_stop or self.model.adapt_pi_epochs:\n",
        "            #  loss_to_check = losses_equalWeightedByType\n",
        "            #  if self.monitored_loss_type == \"Train_L2\":\n",
        "            #    pass\n",
        "            #  elif self.monitored_loss_type == \"Train_L1\":             \n",
        "            #    loss_to_check = losses_equalWeightedByType_abs\n",
        "            #  elif self.monitored_loss_type == \"Train_MAXL2\":             \n",
        "            #    loss_to_check = max_loss_L2\n",
        "\n",
        "            #  if loss_to_check < (self.best_loss-self.early_stop_delta):\n",
        "            #    self.best_loss = loss_to_check\n",
        "            #    if self.model.was_loss_beaten and self.model.adapt_pi_epochs:\n",
        "            #      self.model.was_loss_beaten = False  # this was an increased batch for longer run, so we discount the loss here\n",
        "            #      if self.model.adapt_pi_epochs and e>(self.model.start_adapt_epochs[0][0]):\n",
        "            #        save_model_train(self, self.model, lr, eqLossFn, sample_method, self.best_loss, batch_size_used, e)  # but we still store it\n",
        "            #    else:\n",
        "            #      self.model.was_loss_beaten = True\n",
        "            #    self.early_stop_counter = 0\n",
        "            #    # print(f\"New Loss to beat for early Stop at epoch {e}, original loss: {losses_equalWeightedByType} with patience {self.early_stop_patience}\")\n",
        "            #  else:\n",
        "            #    self.early_stop_counter += 1\n",
        "            #    self.model.was_loss_beaten = False\n",
        "            #  if self.early_stop_counter>=self.early_stop_patience and self.use_early_stop:\n",
        "            #    print(f\"Early Stop at epoch {e}, {self.monitored_loss_type}: {loss_to_check} with patience {self.early_stop_patience}\")\n",
        "            #    break\n",
        "            \n",
        "            \n",
        "                loss_avg = loss_avg + float(loss.item())\n",
        "                loss_int_avg = loss_int_avg + float(internal.item())\n",
        "                loss.backward()\n",
        "\n",
        "                optimizer.step()\n",
        "                i += 1\n",
        "\n",
        "            if (e % self.hook_interval == (self.hook_interval-1)) or e == 0:\n",
        "\n",
        "                loss_avg = loss_avg / i\n",
        "                loss_int_avg = loss_int_avg / i\n",
        "                \n",
        "                #sample_batch = self.model.sample(sample_method_X = sample_method_X, size=self.BATCH_SIZE)\n",
        "                #x_internal , x_terminal = sample_batch\n",
        "                \n",
        "                # pdb.set_trace()\n",
        "                # print(f\"Epoch {e} - lr {lr} - Pi Weight {self.model.pi_net.fc_output.weight[0][0]} - key loss: {round(loss.item(),5)} - eqWeighted loss: {round(losses_equalWeightedByType.item(),5)} - L1 loss {round(loss_abs.item(),5)} - Max Loss {round(max_loss_L2.item(),5)}\")\n",
        "                # print(f\"Epoch {e} - lr {lr} - Pi Weight {self.model.pi_net.fc_output.weight[0][0]} - key loss: {round(loss.item(),5)} - eqWeighted loss: {round(losses_equalWeightedByType.item(),5)} - L1 loss {round(loss_abs.item(),5)} - Max Loss {round(max_loss_L2.item(),5)}\")\n",
        "                #print(f\"Epoch {e} - lr {lr} - key loss: {round(loss.item(),5)} - eqWeighted loss: {round(losses_equalWeightedByType.item(),5)} - L1 loss {round(loss_abs.item(),5)} - Max Loss {round(max_loss_L2.item(),5)}\")\n",
        "                #print('internal loss:', internal , 'terminal loss:', terminal)\n",
        "\n",
        "                print(\"Unet Epoch {} - lr {} - Total Loss: {} - Internal Loss: {} - Terminal Loss: {}\".format(e , lr , loss_avg, loss_int_avg, loss_avg - loss_int_avg))\n",
        "\n",
        "\n",
        "                # plot the fitted value function vs the closed form (ideally straight line...)\n",
        "                #u_internal_sample = x_internal.clone()\n",
        "                #mask = (u_internal_sample[:,0] > 0.1) & (u_internal_sample[:,4] > 0.1)\n",
        "                #u_internal_sample = u_internal_sample[mask.reshape(-1),:]\n",
        "                \n",
        "                #gamma = 1\n",
        "                #time = u_internal_sample[:,0].cpu().detach()\n",
        "                #wealth = u_internal_sample[:,1].cpu().detach()\n",
        "                #mu = u_internal_sample[:,2].cpu().detach()\n",
        "                #r = u_internal_sample[:,3].cpu().detach()\n",
        "                #sigma = u_internal_sample[:,4].cpu().detach()\n",
        "                \n",
        "                #u_internal_sample = Variable(u_internal_sample, requires_grad=True)\n",
        "                #unet_preds = self.net(u_internal_sample)\n",
        "                #du_dt, du_dx, d2u_dx2 = self.model.get_utility_function_derivatives(unet_preds, u_internal_sample)\n",
        "                \n",
        "                #pi_pred_example = self.model.pi_net(u_internal_sample.detach())\n",
        "                \n",
        "                #print('mean pi', pi_pred_example.mean(), 'max pi', pi_pred_example.max(), 'min pi', pi_pred_example.min())\n",
        "                #print('mean d2u_dx2:', d2u_dx2.mean(),'max abs d2u_dx2:', torch.abs(d2u_dx2).max(), 'min abs d2u_dx2:', torch.abs(d2u_dx2).min())\n",
        "                #print('mean du_dx:', du_dx.mean(),'max abs du_dx:', torch.abs(du_dx).max(), 'min abs du_dx:', torch.abs(du_dx).min())\n",
        "                #print('mean du_dt:', du_dt.mean(),'max abs d2u_dx2:', torch.abs(du_dt).max(), 'min abs d2u_dx2:', torch.abs(du_dt).min())\n",
        "\n",
        "                #u_net_results = unet_preds.detach().cpu().numpy().reshape(-1).tolist()\n",
        "                #htx_results = self.Htx(u_internal_sample, gamma).cpu().detach().numpy().reshape(-1).tolist()\n",
        "                #dataf2 = pd.DataFrame( { 'u_net': u_net_results, 'closed_form': htx_results } )\n",
        "                \n",
        "                #print(ggplot(dataf2, aes(x='u_net', y='closed_form')) + geom_point())\n",
        "                \n",
        "                #dataf = pd.DataFrame( { 'pi_net': self.model.pi_net(u_internal_sample).cpu().detach().numpy().reshape(-1).tolist(), \n",
        "                #       'closed_form': (((mu-r)/(gamma*(sigma**2)))*np.exp(-r*(1.0-time))).numpy().tolist() } )\n",
        "\n",
        "                #print(ggplot(dataf, aes(x='pi_net', y='closed_form')) + geom_point())\n",
        "                # loss_avg = 0\n",
        "                ## report detailed loss ## ## puting inside no grad??? for memory optimization!\n",
        "                # tl , dl , il , bl, _ = self.model.calculateLoss( 2**6 )  # note that this is the standard loss!!\n",
        "\n",
        "                #self.history_tl.append( loss_avg )\n",
        "                #self.history_internal.append( internal )\n",
        "                #self.history_terminal.append( terminal )\n",
        "                \n",
        "                #if self.debug == True and (self.validation_sample is not None):\n",
        "                #    mean = []\n",
        "                #    for l in self.hooks:\n",
        "                #        mean.append(torch.mean( self.hooks[l] ).item())\n",
        "                #    self.history_mean_hooks.append( mean )\n",
        "                #    xinternal, xterminal = self.validation_sample\n",
        "                #    xinternal_before, xinternal_expanded = attach_pi_used(xinternal, self.model.pi_net, requires_grad=False)\n",
        "                #    xterminal_before, xterminal_expanded = attach_pi_used(xterminal, self.model.pi_net, requires_grad=False)\n",
        "\n",
        "                #    xinternal_res = self.model.u_net(xinternal_expanded).detach()\n",
        "                #    xterminal_res = self.model.u_net(xterminal_expanded).detach()\n",
        "\n",
        "                    # pdb.set_trace()\n",
        "                #    df_internal = self.create_result_df(e, xinternal, xinternal_res, \"INTERNAL\")\n",
        "                #    df_terminal = self.create_result_df(e, xterminal, xterminal_res, \"TERMINAL\")\n",
        "                    \n",
        "                #    if self.history_surfaces_hooks is None:\n",
        "                #      self.history_surfaces_hooks = pd.concat([df_internal, df_terminal],axis=0)\n",
        "                #    else:\n",
        "                #      self.history_surfaces_hooks = pd.concat([self.history_surfaces_hooks,pd.concat([df_internal, df_terminal],axis=0) ], axis=0)\n",
        "\n",
        "        self.stop_epoch = e\n",
        "\n",
        "    def hook_fn(self, m, i, o):\n",
        "      self.hooks[m] = o.detach()\n",
        "            \n",
        "    def get_all_layers(self, net):\n",
        "      for name, layer in net._modules.items():\n",
        "          if isinstance(layer, nn.ModuleList):\n",
        "              for n , l in layer.named_children():\n",
        "                l.register_forward_hook(self.hook_fn)\n",
        "          else:\n",
        "              # it's a non sequential. Register a hook\n",
        "              layer.register_forward_hook(self.hook_fn)\n",
        "    \n",
        "    def create_result_df(self, e, xsample, xsample_res, sample_type):\n",
        "      df_xsample = pd.DataFrame(xsample.cpu().numpy(), columns = [\"Time\", \"S1\", \"Mu\", \"R\", \"Sigma\"])\n",
        "      df_xsample[\"Epoch\"] = e\n",
        "      df_xsample[\"Sample\"] = sample_type\n",
        "      df_xsample[\"Result\"] = xsample_res.cpu().numpy()\n",
        "      return df_xsample\n",
        "\n",
        "    def train_stratified(self , epoch , lr, \n",
        "                         eqLossFn = 'calculateLoss', \n",
        "                         sample_method_X = \"U\", \n",
        "                         key_loss_func = torch.square, \n",
        "                         huber_delta = 0.5\n",
        "                         ):\n",
        "        \n",
        "        self.validation_losses = np.ones((epoch, 3*3 ), dtype='float32') * np.nan\n",
        "        self.train_losses = np.ones((epoch, 3*2 + 1), dtype='float32') * np.nan\n",
        "        optimizer = self.optimizer_used(self.net.parameters(), lr)\n",
        "        # optimizer = optim.SGD(self.net.parameters(), lr)\n",
        "        loss_avg = 0.0\n",
        "        loss_calc_method = None\n",
        "        try:\n",
        "            loss_calc_method = getattr(self.model, eqLossFn)\n",
        "        except AttributeError:\n",
        "            raise NotImplementedError(\"Class `{}` does not implement `{}`\".format(self.model.__class__.__name__, eqLossFn))\n",
        "        \n",
        "        for e in range(epoch):\n",
        "            optimizer.zero_grad()\n",
        "            # pdb.set_trace()\n",
        "            internal_xts_bts, terminal_xts_bts = self.model.sample_stratified(sample_method_X = sample_method_X, size=self.BATCH_SIZE)\n",
        "            validation_stratum_losses = None #np.array([])#.reshape(1,self.validation_losses.shape[1])\n",
        "            training_stratum_losses = None # np.array([])#.reshape(1,self.train_losses.shape[1])  \n",
        "            training_value_to_optimize = torch.tensor(0.0,requires_grad=True)           \n",
        "            \n",
        "            # pdb.set_trace()\n",
        "            for stratum_count in range(len(internal_xts_bts)):              \n",
        "              sample_batch = (internal_xts_bts[stratum_count], \n",
        "                              terminal_xts_bts[stratum_count])  \n",
        "\n",
        "              # pdb.set_trace()\n",
        "              stratum_losses_L2, stratum_losses_ABS = loss_calc_method(sample_batch, \n",
        "                                                                       loss_transforms = [ key_loss_func, torch.abs ], \n",
        "                                                                       keep_batch = False )\n",
        "              # if np.isnan(stratum_losses_L2[0].detach().cpu().item()):\n",
        "              #   pdb.set_trace()\n",
        "              #   pass\n",
        "            \n",
        "              if training_stratum_losses is not None:\n",
        "                training_stratum_losses = torch.vstack([training_stratum_losses, torch.tensor([*to_cpu_detach(stratum_losses_L2), *to_cpu_detach(stratum_losses_ABS)]) ]) \n",
        "              else:\n",
        "                training_stratum_losses = torch.tensor([*stratum_losses_L2, *stratum_losses_ABS], requires_grad=False) \n",
        "\n",
        "              # pdb.set_trace()  \n",
        "              training_value_to_optimize = training_value_to_optimize + stratum_losses_L2[0]\n",
        "\n",
        "            # pdb.set_trace()              \n",
        "            training_loss_for_epoch = torch.sum(training_stratum_losses,0)\n",
        "            loss = training_value_to_optimize\n",
        "\n",
        "            loss_optimized , internal , terminal, losses_equalWeightedByType, \\\n",
        "            loss_abs , internal_abs , terminal_abs ,losses_equalWeightedByType_abs = training_loss_for_epoch            \n",
        "            max_loss_L2 = torch.max(torch.tensor([internal , terminal ]))\n",
        "\n",
        "            self.train_losses[e,:] = training_loss_for_epoch.detach().numpy()\n",
        "\n",
        "            if self.debug == True and (self.validation_sample is not None):\n",
        "              losses_L2_validation, losses_ABS_validation, losses_Huber_valiation = \\\n",
        "                loss_calc_method( self.validation_sample, \n",
        "                                  loss_transforms = [ torch.square, torch.abs, partial(huber_loss_zero_target, delta=huber_delta) ], \n",
        "                                  keep_batch = False )\n",
        "              validation_loss = [*to_cpu_detach(losses_L2_validation),\n",
        "                                              *to_cpu_detach(losses_ABS_validation),\n",
        "                                              *to_cpu_detach(losses_Huber_valiation)]\n",
        "              # validation_loss = validation_loss.pop(5) # the L2 loss is duplicated at index 1                \n",
        "              self.validation_losses[e,:] = validation_loss\n",
        "\n",
        "            if self.use_early_stop:\n",
        "              loss_to_check = losses_equalWeightedByType\n",
        "              if self.monitored_loss_type == \"Train_L2\":\n",
        "                pass\n",
        "              elif self.monitored_loss_type == \"Train_L1\":             \n",
        "                loss_to_check = losses_equalWeightedByType_abs\n",
        "              elif self.monitored_loss_type == \"Train_MAXL2\":             \n",
        "                loss_to_check = max_loss_L2\n",
        "              if loss_to_check < (self.best_loss-self.early_stop_delta):\n",
        "                self.best_loss = loss_to_check\n",
        "                self.early_stop_counter = 0\n",
        "                # print(f\"New Loss to beat for early Stop at epoch {e}, original loss: {losses_equalWeightedByType} with patience {self.early_stop_patience}\")\n",
        "              else:\n",
        "                self.early_stop_counter += 1\n",
        "              if self.early_stop_counter>=self.early_stop_patience:\n",
        "                print(f\"Early Stop at epoch {e}, {self.monitored_loss_type}: {loss_to_check} with patience {self.early_stop_patience}\")\n",
        "                break\n",
        "\n",
        "            loss_avg = loss_avg + float(loss.item())\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            if (e % self.hook_interval == (self.hook_interval-1)) or e == 0:\n",
        "                loss_avg = loss_avg/self.hook_interval\n",
        "                print(\"Epoch {} - lr {} -  key loss: {} - eqWeighted loss: {} - L1 loss {} - Max Loss {}\".format(e , lr , loss, losses_equalWeightedByType, loss_abs, max_loss_L2 ))\n",
        "                print('internal loss:', internal , 'terminal loss:', terminal)\n",
        "                # loss_avg = 0\n",
        "                ## report detailed loss ## ## puting inside no grad??? for memory optimization!\n",
        "                # tl , dl , il , bl, _ = self.model.calculateLoss( 2**6 )  # note that this is the standard loss!!\n",
        "                self.history_tl.append( loss_avg )\n",
        "                self.history_internal.append( internal )\n",
        "                self.history_terminal.append( terminal )\n",
        "                if self.debug == True and (self.validation_sample is not None):\n",
        "                    mean = []\n",
        "                    for l in self.hooks:\n",
        "                        mean.append(torch.mean( self.hooks[l] ).item())\n",
        "                    self.history_mean_hooks.append( mean )\n",
        "                    xinternal, xterminal, xinitial, xnonzero = self.validation_sample\n",
        "                    xinternal_res = self.model.net(xinternal).detach()\n",
        "                    xterminal_res = self.model.net(xterminal).detach()\n",
        "\n",
        "                    # pdb.set_trace()\n",
        "                    df_internal = self.create_result_df(e, xinternal, xinternal_res, \"INTERNAL\")\n",
        "                    df_terminal = self.create_result_df(e, xterminal, xterminal_res, \"TERMINAL\")\n",
        "                    \n",
        "                    if self.history_surfaces_hooks is None:\n",
        "                      self.history_surfaces_hooks = pd.concat([df_internal, df_terminal],axis=0)\n",
        "                    else:\n",
        "                      self.history_surfaces_hooks = pd.concat([self.history_surfaces_hooks,pd.concat([df_internal, df_terminal],axis=0) ], axis=0)\n",
        "\n",
        "        self.stop_epoch = e\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy05I1QFh7EM"
      },
      "source": [
        "### Test Case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tf-VoPhf2AJu",
        "outputId": "60ec9ee1-b172-477d-f02e-46306c2179e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 2/10000 [00:00<14:08, 11.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 0 - lr 0.001 - Total Loss: 13933.87109375 - Internal Loss: 2817.010986328125 - Terminal Loss: 11116.860107421875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 100/10000 [00:10<23:06,  7.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 99 - lr 0.001 -  key loss: -3.2058395385742187\n",
            "Unet Epoch 99 - lr 0.001 - Total Loss: 2181.150390625 - Internal Loss: 951.0221557617188 - Terminal Loss: 1230.1282348632812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|â         | 202/10000 [00:17<08:21, 19.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 199 - lr 0.001 -  key loss: -15.02324951171875\n",
            "Unet Epoch 199 - lr 0.001 - Total Loss: 2043.8809814453125 - Internal Loss: 738.7630615234375 - Terminal Loss: 1305.117919921875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|â         | 302/10000 [00:22<09:01, 17.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 299 - lr 0.001 -  key loss: 0.09613245964050293\n",
            "Unet Epoch 299 - lr 0.001 - Total Loss: 2281.268310546875 - Internal Loss: 960.8191528320312 - Terminal Loss: 1320.4491577148438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|â         | 402/10000 [00:28<09:21, 17.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 399 - lr 0.001 -  key loss: 0.3381613159179688\n",
            "Unet Epoch 399 - lr 0.001 - Total Loss: 2124.94287109375 - Internal Loss: 766.6758422851562 - Terminal Loss: 1358.2670288085938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|â         | 502/10000 [00:34<09:36, 16.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 499 - lr 0.001 -  key loss: -0.7778357696533204\n",
            "Unet Epoch 499 - lr 0.001 - Total Loss: 1664.747314453125 - Internal Loss: 232.86070251464844 - Terminal Loss: 1431.8866119384766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|â         | 602/10000 [00:41<10:27, 14.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 599 - lr 0.001 -  key loss: -1.4922836303710938\n",
            "Unet Epoch 599 - lr 0.001 - Total Loss: 1407.496337890625 - Internal Loss: 140.98849487304688 - Terminal Loss: 1266.5078430175781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|â         | 700/10000 [00:48<15:05, 10.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 699 - lr 0.001 -  key loss: -3.2833966064453124\n",
            "Unet Epoch 699 - lr 0.001 - Total Loss: 1159.0501708984375 - Internal Loss: 290.7238464355469 - Terminal Loss: 868.3263244628906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|â         | 801/10000 [00:55<11:03, 13.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 799 - lr 0.001 -  key loss: -5.298038330078125\n",
            "Unet Epoch 799 - lr 0.001 - Total Loss: 660.5263061523438 - Internal Loss: 466.1717224121094 - Terminal Loss: 194.35458374023438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  9%|â         | 901/10000 [01:02<11:02, 13.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 899 - lr 0.001 -  key loss: -3.1878958129882813\n",
            "Unet Epoch 899 - lr 0.001 - Total Loss: 336.6278991699219 - Internal Loss: 226.85018920898438 - Terminal Loss: 109.7777099609375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|â         | 1001/10000 [01:10<11:08, 13.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 999 - lr 0.001 -  key loss: -2.6459054565429687\n",
            "Unet Epoch 999 - lr 0.001 - Total Loss: 302.1993103027344 - Internal Loss: 133.99755859375 - Terminal Loss: 168.20175170898438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 11%|â         | 1101/10000 [01:18<12:06, 12.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 1099 - lr 0.001 -  key loss: -2.258830718994141\n",
            "Unet Epoch 1099 - lr 0.001 - Total Loss: 302.41888427734375 - Internal Loss: 101.97592163085938 - Terminal Loss: 200.44296264648438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 12%|ââ        | 1201/10000 [01:26<12:00, 12.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 1199 - lr 0.001 -  key loss: -2.2038002014160156\n",
            "Unet Epoch 1199 - lr 0.001 - Total Loss: 238.57540893554688 - Internal Loss: 94.86198425292969 - Terminal Loss: 143.7134246826172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|ââ        | 1301/10000 [01:35<12:14, 11.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 1299 - lr 0.001 -  key loss: -2.228310089111328\n",
            "Unet Epoch 1299 - lr 0.001 - Total Loss: 116.62547302246094 - Internal Loss: 91.19268798828125 - Terminal Loss: 25.432785034179688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|ââ        | 1400/10000 [01:45<28:27,  5.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 1399 - lr 0.001 -  key loss: -2.289422607421875\n",
            "Unet Epoch 1399 - lr 0.001 - Total Loss: 189.93333435058594 - Internal Loss: 88.09134674072266 - Terminal Loss: 101.84198760986328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|ââ        | 1500/10000 [01:54<12:44, 11.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 1499 - lr 0.001 -  key loss: -2.1488027954101563\n",
            "Unet Epoch 1499 - lr 0.001 - Total Loss: 203.29624938964844 - Internal Loss: 83.74542236328125 - Terminal Loss: 119.55082702636719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|ââ        | 1601/10000 [02:04<13:03, 10.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 1599 - lr 0.001 -  key loss: -2.2167903137207032\n",
            "Unet Epoch 1599 - lr 0.001 - Total Loss: 176.30726623535156 - Internal Loss: 81.21533966064453 - Terminal Loss: 95.09192657470703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|ââ        | 1701/10000 [02:14<13:21, 10.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 1699 - lr 0.001 -  key loss: -2.2222972106933594\n",
            "Unet Epoch 1699 - lr 0.001 - Total Loss: 189.0308074951172 - Internal Loss: 78.45457458496094 - Terminal Loss: 110.57623291015625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 18%|ââ        | 1801/10000 [02:24<13:24, 10.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 1799 - lr 0.001 -  key loss: -2.213509368896484\n",
            "Unet Epoch 1799 - lr 0.001 - Total Loss: 159.65713500976562 - Internal Loss: 76.57215118408203 - Terminal Loss: 83.0849838256836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|ââ        | 1901/10000 [02:35<14:09,  9.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 1899 - lr 0.001 -  key loss: -2.237061004638672\n",
            "Unet Epoch 1899 - lr 0.001 - Total Loss: 257.14111328125 - Internal Loss: 77.43980407714844 - Terminal Loss: 179.70130920410156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|ââ        | 2001/10000 [02:46<15:10,  8.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 1999 - lr 0.001 -  key loss: -2.2253636169433593\n",
            "Unet Epoch 1999 - lr 0.001 - Total Loss: 290.77764892578125 - Internal Loss: 76.4031982421875 - Terminal Loss: 214.37445068359375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 21%|ââ        | 2101/10000 [02:57<14:19,  9.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 2099 - lr 0.001 -  key loss: -2.234783172607422\n",
            "Unet Epoch 2099 - lr 0.001 - Total Loss: 111.12311553955078 - Internal Loss: 77.03851318359375 - Terminal Loss: 34.08460235595703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|âââ       | 2201/10000 [03:09<19:52,  6.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 2199 - lr 0.001 -  key loss: -2.248057403564453\n",
            "Unet Epoch 2199 - lr 0.001 - Total Loss: 103.26824951171875 - Internal Loss: 75.35990142822266 - Terminal Loss: 27.908348083496094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 23%|âââ       | 2301/10000 [03:21<15:12,  8.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 2299 - lr 0.001 -  key loss: -2.2009629821777343\n",
            "Unet Epoch 2299 - lr 0.001 - Total Loss: 204.65280151367188 - Internal Loss: 74.34790802001953 - Terminal Loss: 130.30489349365234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 24%|âââ       | 2401/10000 [03:33<15:55,  7.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 2399 - lr 0.001 -  key loss: -2.2727223205566407\n",
            "Unet Epoch 2399 - lr 0.001 - Total Loss: 138.5377197265625 - Internal Loss: 74.57405090332031 - Terminal Loss: 63.96366882324219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|âââ       | 2500/10000 [03:46<15:23,  8.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 2499 - lr 0.001 -  key loss: -2.234851531982422\n",
            "Unet Epoch 2499 - lr 0.001 - Total Loss: 135.87557983398438 - Internal Loss: 75.3569107055664 - Terminal Loss: 60.51866912841797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 26%|âââ       | 2600/10000 [03:59<15:31,  7.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 2599 - lr 0.001 -  key loss: -2.147710266113281\n",
            "Unet Epoch 2599 - lr 0.001 - Total Loss: 213.7167205810547 - Internal Loss: 73.44833374023438 - Terminal Loss: 140.2683868408203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|âââ       | 2700/10000 [04:12<15:51,  7.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 2699 - lr 0.001 -  key loss: -2.179795684814453\n",
            "Unet Epoch 2699 - lr 0.001 - Total Loss: 107.19729614257812 - Internal Loss: 69.81004333496094 - Terminal Loss: 37.38725280761719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 28%|âââ       | 2800/10000 [04:26<15:49,  7.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 2799 - lr 0.001 -  key loss: -2.2232879638671874\n",
            "Unet Epoch 2799 - lr 0.001 - Total Loss: 112.13529968261719 - Internal Loss: 77.66442108154297 - Terminal Loss: 34.47087860107422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 29%|âââ       | 2900/10000 [04:40<15:39,  7.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 2899 - lr 0.001 -  key loss: -2.1254808044433595\n",
            "Unet Epoch 2899 - lr 0.001 - Total Loss: 169.8920440673828 - Internal Loss: 67.65679168701172 - Terminal Loss: 102.2352523803711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|âââ       | 3000/10000 [04:54<15:43,  7.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 2999 - lr 0.001 -  key loss: -2.1444163513183594\n",
            "Unet Epoch 2999 - lr 0.001 - Total Loss: 119.41158294677734 - Internal Loss: 68.57015991210938 - Terminal Loss: 50.84142303466797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 31%|âââ       | 3100/10000 [05:08<16:06,  7.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 3099 - lr 0.001 -  key loss: -2.113972015380859\n",
            "Unet Epoch 3099 - lr 0.001 - Total Loss: 157.47103881835938 - Internal Loss: 69.3145980834961 - Terminal Loss: 88.15644073486328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 32%|ââââ      | 3200/10000 [05:24<16:36,  6.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 3199 - lr 0.001 -  key loss: -2.182001953125\n",
            "Unet Epoch 3199 - lr 0.001 - Total Loss: 175.597412109375 - Internal Loss: 69.0743637084961 - Terminal Loss: 106.5230484008789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|ââââ      | 3300/10000 [05:39<19:56,  5.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 3299 - lr 0.001 -  key loss: -2.1421212768554687\n",
            "Unet Epoch 3299 - lr 0.001 - Total Loss: 241.62525939941406 - Internal Loss: 67.64163970947266 - Terminal Loss: 173.9836196899414\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|ââââ      | 3400/10000 [05:55<17:17,  6.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 3399 - lr 0.001 -  key loss: -2.2041290283203123\n",
            "Unet Epoch 3399 - lr 0.001 - Total Loss: 141.90159606933594 - Internal Loss: 66.53838348388672 - Terminal Loss: 75.36321258544922\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|ââââ      | 3500/10000 [06:10<17:07,  6.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 3499 - lr 0.001 -  key loss: -2.1087892150878904\n",
            "Unet Epoch 3499 - lr 0.001 - Total Loss: 137.19725036621094 - Internal Loss: 62.21022033691406 - Terminal Loss: 74.98703002929688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 36%|ââââ      | 3600/10000 [06:27<16:53,  6.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 3599 - lr 0.001 -  key loss: -2.2131413269042968\n",
            "Unet Epoch 3599 - lr 0.001 - Total Loss: 294.6875 - Internal Loss: 67.09720611572266 - Terminal Loss: 227.59029388427734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 37%|ââââ      | 3700/10000 [06:43<17:14,  6.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 3699 - lr 0.001 -  key loss: -2.166279754638672\n",
            "Unet Epoch 3699 - lr 0.001 - Total Loss: 94.18280029296875 - Internal Loss: 65.51910400390625 - Terminal Loss: 28.6636962890625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 38%|ââââ      | 3800/10000 [07:01<16:49,  6.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 3799 - lr 0.001 -  key loss: -2.0600621032714845\n",
            "Unet Epoch 3799 - lr 0.001 - Total Loss: 106.669189453125 - Internal Loss: 62.60390090942383 - Terminal Loss: 44.06528854370117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 39%|ââââ      | 3900/10000 [07:19<18:20,  5.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 3899 - lr 0.001 -  key loss: -2.07461669921875\n",
            "Unet Epoch 3899 - lr 0.001 - Total Loss: 134.135498046875 - Internal Loss: 58.93059158325195 - Terminal Loss: 75.20490646362305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|ââââ      | 4000/10000 [07:36<17:34,  5.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 3999 - lr 0.001 -  key loss: -2.177296142578125\n",
            "Unet Epoch 3999 - lr 0.001 - Total Loss: 101.09931182861328 - Internal Loss: 66.98845672607422 - Terminal Loss: 34.11085510253906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|ââââ      | 4100/10000 [07:54<17:37,  5.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 4099 - lr 0.001 -  key loss: -2.086531982421875\n",
            "Unet Epoch 4099 - lr 0.001 - Total Loss: 92.31415557861328 - Internal Loss: 58.70382308959961 - Terminal Loss: 33.61033248901367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|âââââ     | 4200/10000 [08:13<17:18,  5.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 4199 - lr 0.001 -  key loss: -2.0932051086425782\n",
            "Unet Epoch 4199 - lr 0.001 - Total Loss: 144.91168212890625 - Internal Loss: 64.91254425048828 - Terminal Loss: 79.99913787841797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|âââââ     | 4300/10000 [08:31<17:19,  5.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 4299 - lr 0.001 -  key loss: -2.1945254516601564\n",
            "Unet Epoch 4299 - lr 0.001 - Total Loss: 87.81218719482422 - Internal Loss: 63.90350341796875 - Terminal Loss: 23.90868377685547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 44%|âââââ     | 4400/10000 [08:50<20:23,  4.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 4399 - lr 0.001 -  key loss: -2.071182861328125\n",
            "Unet Epoch 4399 - lr 0.001 - Total Loss: 88.57878112792969 - Internal Loss: 55.9247932434082 - Terminal Loss: 32.653987884521484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|âââââ     | 4500/10000 [09:10<17:28,  5.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 4499 - lr 0.001 -  key loss: -2.1049516296386717\n",
            "Unet Epoch 4499 - lr 0.001 - Total Loss: 135.32693481445312 - Internal Loss: 55.284976959228516 - Terminal Loss: 80.04195785522461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 46%|âââââ     | 4600/10000 [09:29<17:22,  5.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 4599 - lr 0.001 -  key loss: -2.1356512451171876\n",
            "Unet Epoch 4599 - lr 0.001 - Total Loss: 112.3032455444336 - Internal Loss: 59.80965805053711 - Terminal Loss: 52.493587493896484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 47%|âââââ     | 4700/10000 [09:49<17:14,  5.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 4699 - lr 0.001 -  key loss: -2.037494964599609\n",
            "Unet Epoch 4699 - lr 0.001 - Total Loss: 138.45188903808594 - Internal Loss: 54.94330596923828 - Terminal Loss: 83.50858306884766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 48%|âââââ     | 4800/10000 [10:10<17:21,  4.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 4799 - lr 0.001 -  key loss: -2.030606994628906\n",
            "Unet Epoch 4799 - lr 0.001 - Total Loss: 97.22090911865234 - Internal Loss: 54.44690704345703 - Terminal Loss: 42.77400207519531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 49%|âââââ     | 4900/10000 [10:30<20:02,  4.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 4899 - lr 0.001 -  key loss: -2.163798370361328\n",
            "Unet Epoch 4899 - lr 0.001 - Total Loss: 112.47064971923828 - Internal Loss: 56.93741226196289 - Terminal Loss: 55.53323745727539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|âââââ     | 5000/10000 [10:52<17:35,  4.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 4999 - lr 0.001 -  key loss: -2.175078582763672\n",
            "Unet Epoch 4999 - lr 0.001 - Total Loss: 160.24354553222656 - Internal Loss: 58.28300476074219 - Terminal Loss: 101.96054077148438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 51%|âââââ     | 5100/10000 [11:13<17:05,  4.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 5099 - lr 0.001 -  key loss: -2.1776191711425783\n",
            "Unet Epoch 5099 - lr 0.001 - Total Loss: 76.39307403564453 - Internal Loss: 58.544517517089844 - Terminal Loss: 17.848556518554688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 52%|ââââââ    | 5200/10000 [11:35<16:46,  4.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 5199 - lr 0.001 -  key loss: -2.119447784423828\n",
            "Unet Epoch 5199 - lr 0.001 - Total Loss: 112.04399108886719 - Internal Loss: 53.427974700927734 - Terminal Loss: 58.61601638793945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 53%|ââââââ    | 5300/10000 [11:57<16:47,  4.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 5299 - lr 0.001 -  key loss: -2.146812744140625\n",
            "Unet Epoch 5299 - lr 0.001 - Total Loss: 108.5523452758789 - Internal Loss: 54.98044204711914 - Terminal Loss: 53.571903228759766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|ââââââ    | 5400/10000 [12:19<17:59,  4.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 5399 - lr 0.001 -  key loss: -1.9983432006835937\n",
            "Unet Epoch 5399 - lr 0.001 - Total Loss: 109.78884887695312 - Internal Loss: 51.081581115722656 - Terminal Loss: 58.70726776123047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|ââââââ    | 5500/10000 [12:42<16:53,  4.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 5499 - lr 0.001 -  key loss: -2.119687194824219\n",
            "Unet Epoch 5499 - lr 0.001 - Total Loss: 176.42002868652344 - Internal Loss: 53.86873245239258 - Terminal Loss: 122.55129623413086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 56%|ââââââ    | 5600/10000 [13:06<16:40,  4.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 5599 - lr 0.001 -  key loss: -2.0883828735351564\n",
            "Unet Epoch 5599 - lr 0.001 - Total Loss: 128.98257446289062 - Internal Loss: 55.69719696044922 - Terminal Loss: 73.2853775024414\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 57%|ââââââ    | 5700/10000 [13:29<16:36,  4.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 5699 - lr 0.001 -  key loss: -2.0262901306152346\n",
            "Unet Epoch 5699 - lr 0.001 - Total Loss: 128.62693786621094 - Internal Loss: 64.56629180908203 - Terminal Loss: 64.0606460571289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 58%|ââââââ    | 5800/10000 [13:53<21:03,  3.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 5799 - lr 0.001 -  key loss: -2.1527874755859373\n",
            "Unet Epoch 5799 - lr 0.001 - Total Loss: 79.1921157836914 - Internal Loss: 53.61290740966797 - Terminal Loss: 25.579208374023438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 59%|ââââââ    | 5900/10000 [14:17<16:46,  4.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 5899 - lr 0.001 -  key loss: -2.0321287536621093\n",
            "Unet Epoch 5899 - lr 0.001 - Total Loss: 106.74336242675781 - Internal Loss: 49.10810089111328 - Terminal Loss: 57.63526153564453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|ââââââ    | 6000/10000 [14:42<16:19,  4.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 5999 - lr 0.001 -  key loss: -2.118735809326172\n",
            "Unet Epoch 5999 - lr 0.001 - Total Loss: 73.2381591796875 - Internal Loss: 52.55448913574219 - Terminal Loss: 20.683670043945312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 61%|ââââââ    | 6100/10000 [15:07<15:57,  4.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 6099 - lr 0.001 -  key loss: -2.133356018066406\n",
            "Unet Epoch 6099 - lr 0.001 - Total Loss: 98.65629577636719 - Internal Loss: 56.379634857177734 - Terminal Loss: 42.27666091918945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 62%|âââââââ   | 6200/10000 [15:33<15:45,  4.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 6199 - lr 0.001 -  key loss: -2.120349578857422\n",
            "Unet Epoch 6199 - lr 0.001 - Total Loss: 128.1955108642578 - Internal Loss: 52.97798538208008 - Terminal Loss: 75.21752548217773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 63%|âââââââ   | 6300/10000 [15:58<16:48,  3.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 6299 - lr 0.001 -  key loss: -2.152960205078125\n",
            "Unet Epoch 6299 - lr 0.001 - Total Loss: 141.74452209472656 - Internal Loss: 62.245967864990234 - Terminal Loss: 79.49855422973633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 64%|âââââââ   | 6400/10000 [16:24<15:20,  3.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 6399 - lr 0.001 -  key loss: -2.1056503295898437\n",
            "Unet Epoch 6399 - lr 0.001 - Total Loss: 70.94851684570312 - Internal Loss: 50.93608474731445 - Terminal Loss: 20.012432098388672\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|âââââââ   | 6500/10000 [16:51<15:05,  3.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 6499 - lr 0.001 -  key loss: -2.1975912475585937\n",
            "Unet Epoch 6499 - lr 0.001 - Total Loss: 102.56855010986328 - Internal Loss: 56.762969970703125 - Terminal Loss: 45.805580139160156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 66%|âââââââ   | 6600/10000 [17:17<14:53,  3.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 6599 - lr 0.001 -  key loss: -2.040085906982422\n",
            "Unet Epoch 6599 - lr 0.001 - Total Loss: 112.28623962402344 - Internal Loss: 51.48606491088867 - Terminal Loss: 60.800174713134766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|âââââââ   | 6700/10000 [17:45<18:20,  3.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 6699 - lr 0.001 -  key loss: -2.047288360595703\n",
            "Unet Epoch 6699 - lr 0.001 - Total Loss: 101.79502868652344 - Internal Loss: 48.89408493041992 - Terminal Loss: 52.900943756103516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 68%|âââââââ   | 6800/10000 [18:12<14:25,  3.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 6799 - lr 0.001 -  key loss: -2.0580593872070314\n",
            "Unet Epoch 6799 - lr 0.001 - Total Loss: 84.55146026611328 - Internal Loss: 48.96058654785156 - Terminal Loss: 35.59087371826172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 69%|âââââââ   | 6899/10000 [18:40<14:00,  3.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 6899 - lr 0.001 -  key loss: -2.043651275634766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|âââââââ   | 7000/10000 [19:09<13:52,  3.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 6899 - lr 0.001 - Total Loss: 159.24478149414062 - Internal Loss: 49.6356315612793 - Terminal Loss: 109.60914993286133\n",
            "Pi Epoch 6999 - lr 0.001 -  key loss: -2.021114501953125\n",
            "Unet Epoch 6999 - lr 0.001 - Total Loss: 88.52371215820312 - Internal Loss: 51.61402893066406 - Terminal Loss: 36.90968322753906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 71%|âââââââ   | 7100/10000 [19:38<13:28,  3.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 7099 - lr 0.001 -  key loss: -2.062783050537109\n",
            "Unet Epoch 7099 - lr 0.001 - Total Loss: 114.33183288574219 - Internal Loss: 49.634212493896484 - Terminal Loss: 64.6976203918457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 72%|ââââââââ  | 7200/10000 [20:07<14:22,  3.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 7199 - lr 0.001 -  key loss: -2.1960525512695312\n",
            "Unet Epoch 7199 - lr 0.001 - Total Loss: 77.3996353149414 - Internal Loss: 55.757144927978516 - Terminal Loss: 21.64249038696289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 73%|ââââââââ  | 7300/10000 [20:36<13:03,  3.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 7299 - lr 0.001 -  key loss: -2.122333221435547\n",
            "Unet Epoch 7299 - lr 0.001 - Total Loss: 83.75514221191406 - Internal Loss: 50.19320297241211 - Terminal Loss: 33.56193923950195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|ââââââââ  | 7399/10000 [21:06<12:29,  3.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 7399 - lr 0.001 -  key loss: -2.0806666564941407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|ââââââââ  | 7400/10000 [21:06<12:33,  3.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 7399 - lr 0.001 - Total Loss: 147.31190490722656 - Internal Loss: 47.73302459716797 - Terminal Loss: 99.5788803100586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|ââââââââ  | 7499/10000 [21:36<12:26,  3.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 7499 - lr 0.001 -  key loss: -2.067485656738281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|ââââââââ  | 7500/10000 [21:36<12:33,  3.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 7499 - lr 0.001 - Total Loss: 114.22896575927734 - Internal Loss: 49.40914535522461 - Terminal Loss: 64.81982040405273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 76%|ââââââââ  | 7599/10000 [22:07<13:51,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 7599 - lr 0.001 -  key loss: -2.091921844482422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|ââââââââ  | 7600/10000 [22:07<13:31,  2.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 7599 - lr 0.001 - Total Loss: 89.11866760253906 - Internal Loss: 46.29659652709961 - Terminal Loss: 42.82207107543945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 77%|ââââââââ  | 7699/10000 [22:38<11:43,  3.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 7699 - lr 0.001 -  key loss: -2.111949005126953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 77%|ââââââââ  | 7700/10000 [22:38<11:43,  3.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 7699 - lr 0.001 - Total Loss: 235.15126037597656 - Internal Loss: 51.337406158447266 - Terminal Loss: 183.8138542175293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 78%|ââââââââ  | 7799/10000 [23:09<11:25,  3.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 7799 - lr 0.001 -  key loss: -2.113259735107422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|ââââââââ  | 7800/10000 [23:10<11:22,  3.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 7799 - lr 0.001 - Total Loss: 90.35955810546875 - Internal Loss: 62.3994026184082 - Terminal Loss: 27.960155487060547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 79%|ââââââââ  | 7899/10000 [23:41<10:57,  3.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 7899 - lr 0.001 -  key loss: -2.0838078308105468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 79%|ââââââââ  | 7900/10000 [23:41<11:01,  3.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 7899 - lr 0.001 - Total Loss: 128.73133850097656 - Internal Loss: 45.62995529174805 - Terminal Loss: 83.10138320922852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|ââââââââ  | 7999/10000 [24:14<11:34,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 7999 - lr 0.001 -  key loss: -2.0942352294921873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|ââââââââ  | 8000/10000 [24:14<11:22,  2.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 7999 - lr 0.001 - Total Loss: 136.18910217285156 - Internal Loss: 47.78428649902344 - Terminal Loss: 88.40481567382812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 81%|ââââââââ  | 8099/10000 [24:47<10:20,  3.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 8099 - lr 0.001 -  key loss: -1.953986358642578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 81%|ââââââââ  | 8100/10000 [24:47<10:20,  3.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 8099 - lr 0.001 - Total Loss: 100.98052215576172 - Internal Loss: 43.98647689819336 - Terminal Loss: 56.99404525756836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|âââââââââ | 8199/10000 [25:21<09:46,  3.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 8199 - lr 0.001 -  key loss: -2.1069017028808594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|âââââââââ | 8200/10000 [25:21<09:52,  3.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 8199 - lr 0.001 - Total Loss: 153.5801239013672 - Internal Loss: 49.229854583740234 - Terminal Loss: 104.35026931762695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 83%|âââââââââ | 8299/10000 [25:54<09:19,  3.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 8299 - lr 0.001 -  key loss: -2.185935211181641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|âââââââââ | 8300/10000 [25:55<09:22,  3.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 8299 - lr 0.001 - Total Loss: 82.8732681274414 - Internal Loss: 53.17253875732422 - Terminal Loss: 29.700729370117188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|âââââââââ | 8399/10000 [26:29<09:29,  2.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 8399 - lr 0.001 -  key loss: -2.059789123535156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|âââââââââ | 8400/10000 [26:29<09:15,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 8399 - lr 0.001 - Total Loss: 156.96078491210938 - Internal Loss: 47.70616149902344 - Terminal Loss: 109.25462341308594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 85%|âââââââââ | 8499/10000 [27:03<08:26,  2.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 8499 - lr 0.001 -  key loss: -2.144239044189453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|âââââââââ | 8500/10000 [27:03<08:34,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 8499 - lr 0.001 - Total Loss: 208.17987060546875 - Internal Loss: 46.262451171875 - Terminal Loss: 161.91741943359375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 86%|âââââââââ | 8599/10000 [27:38<07:55,  2.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 8599 - lr 0.001 -  key loss: -2.103179931640625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|âââââââââ | 8600/10000 [27:38<07:55,  2.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 8599 - lr 0.001 - Total Loss: 64.20518493652344 - Internal Loss: 48.96402359008789 - Terminal Loss: 15.241161346435547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 87%|âââââââââ | 8699/10000 [28:13<07:29,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 8699 - lr 0.001 -  key loss: -2.133716278076172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|âââââââââ | 8700/10000 [28:13<07:33,  2.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 8699 - lr 0.001 - Total Loss: 73.07990264892578 - Internal Loss: 51.641902923583984 - Terminal Loss: 21.437999725341797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 88%|âââââââââ | 8799/10000 [28:49<07:58,  2.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 8799 - lr 0.001 -  key loss: -2.0869793701171875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|âââââââââ | 8800/10000 [28:49<07:42,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 8799 - lr 0.001 - Total Loss: 65.8731460571289 - Internal Loss: 49.32503128051758 - Terminal Loss: 16.548114776611328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 89%|âââââââââ | 8899/10000 [29:25<06:29,  2.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 8899 - lr 0.001 -  key loss: -2.062996673583984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 89%|âââââââââ | 8900/10000 [29:25<06:35,  2.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 8899 - lr 0.001 - Total Loss: 62.68059539794922 - Internal Loss: 43.26577377319336 - Terminal Loss: 19.41482162475586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|âââââââââ | 8999/10000 [30:01<06:04,  2.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 8999 - lr 0.001 -  key loss: -2.066039733886719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|âââââââââ | 9000/10000 [30:01<06:00,  2.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 8999 - lr 0.001 - Total Loss: 120.07515716552734 - Internal Loss: 46.862274169921875 - Terminal Loss: 73.21288299560547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 91%|âââââââââ | 9099/10000 [30:38<05:38,  2.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 9099 - lr 0.001 -  key loss: -2.085904998779297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 91%|âââââââââ | 9100/10000 [30:38<05:40,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 9099 - lr 0.001 - Total Loss: 94.88055419921875 - Internal Loss: 47.2469482421875 - Terminal Loss: 47.63360595703125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|ââââââââââ| 9199/10000 [31:17<05:20,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 9199 - lr 0.001 -  key loss: -2.0149111938476563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|ââââââââââ| 9200/10000 [31:17<05:18,  2.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 9199 - lr 0.001 - Total Loss: 69.26051330566406 - Internal Loss: 46.8415412902832 - Terminal Loss: 22.41897201538086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 93%|ââââââââââ| 9299/10000 [31:55<04:25,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 9299 - lr 0.001 -  key loss: -2.0111238098144533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|ââââââââââ| 9300/10000 [31:55<04:25,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 9299 - lr 0.001 - Total Loss: 105.21073150634766 - Internal Loss: 47.769386291503906 - Terminal Loss: 57.44134521484375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 94%|ââââââââââ| 9399/10000 [32:33<03:43,  2.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 9399 - lr 0.001 -  key loss: -2.102628021240234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|ââââââââââ| 9400/10000 [32:33<03:44,  2.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 9399 - lr 0.001 - Total Loss: 109.00515747070312 - Internal Loss: 40.902652740478516 - Terminal Loss: 68.10250473022461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|ââââââââââ| 9499/10000 [33:12<03:06,  2.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 9499 - lr 0.001 -  key loss: -2.0462986755371095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|ââââââââââ| 9500/10000 [33:12<03:09,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 9499 - lr 0.001 - Total Loss: 85.35206604003906 - Internal Loss: 44.9636116027832 - Terminal Loss: 40.38845443725586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|ââââââââââ| 9599/10000 [33:51<02:40,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 9599 - lr 0.001 -  key loss: -2.017691345214844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|ââââââââââ| 9600/10000 [33:51<02:38,  2.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 9599 - lr 0.001 - Total Loss: 77.89183807373047 - Internal Loss: 39.2531852722168 - Terminal Loss: 38.63865280151367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 97%|ââââââââââ| 9699/10000 [34:30<01:57,  2.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 9699 - lr 0.001 -  key loss: -2.0749139404296875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 97%|ââââââââââ| 9700/10000 [34:31<01:56,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 9699 - lr 0.001 - Total Loss: 146.99928283691406 - Internal Loss: 41.10481643676758 - Terminal Loss: 105.89446640014648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|ââââââââââ| 9799/10000 [35:10<01:18,  2.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 9799 - lr 0.001 -  key loss: -2.093811340332031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98%|ââââââââââ| 9800/10000 [35:11<01:19,  2.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 9799 - lr 0.001 - Total Loss: 69.91094207763672 - Internal Loss: 38.62795639038086 - Terminal Loss: 31.28298568725586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 99%|ââââââââââ| 9899/10000 [35:51<00:41,  2.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 9899 - lr 0.001 -  key loss: -1.9717816162109374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 99%|ââââââââââ| 9900/10000 [35:51<00:40,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 9899 - lr 0.001 - Total Loss: 127.7676010131836 - Internal Loss: 37.18894577026367 - Terminal Loss: 90.57865524291992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|ââââââââââ| 9999/10000 [36:32<00:00,  2.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi Epoch 9999 - lr 0.001 -  key loss: -2.0854345703125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|ââââââââââ| 10000/10000 [36:32<00:00,  4.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unet Epoch 9999 - lr 0.001 - Total Loss: 76.70756530761719 - Internal Loss: 43.27777862548828 - Terminal Loss: 33.429786682128906\n",
            "time: 36min 37s (started: 2022-07-08 21:53:48 +00:00)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "seed = 123\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# u_net, pi_net, pi_net_epoch, pi_net_lr\n",
        "eqLossFn= 'calculateLoss'\n",
        "sample_method= \"U\"\n",
        "lr = 1e-3\n",
        "lr_for_pi = 1e-3\n",
        "max_pi_epochs = 4 # has to be low!!!\n",
        "\n",
        "#u_net = MertonAlternativePiNet( in_size = 5 , out_size = 1, neurons = 64, depth=2 )\n",
        "u_net = MertonUtilityNet(NL=2, NN=64)\n",
        "u_net.to(torch.device(\"cuda:0\")) \n",
        "\n",
        "pi_net = MertonAlternativePiNet( in_size = 5 , out_size = 1, neurons = 64, depth=2 )\n",
        "#pi_net = MertonPiNet(NL=2, NN=64)\n",
        "pi_net.to(torch.device(\"cuda:0\"))\n",
        "\n",
        "mequation = MertonEquation(u_net, pi_net, max_pi_epochs, lr_for_pi)\n",
        "#mequation.adapt_pi_epochs = True\n",
        "\n",
        "trainMertonAlloc = TrainHJBMertonWithDGM(u_net, mequation, BATCH_SIZE=2048, debug=False )\n",
        "trainMertonAlloc.hook_interval = 100\n",
        "\n",
        "trainMertonAlloc.train(epoch = 10000, lr = lr, eqLossFn = eqLossFn, sample_method_X = sample_method, key_loss_func=torch.abs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def attach_pi_used(x, pi_net, requires_grad=True):\n",
        "  pi_used = pi_net(x)  \n",
        "  # pi_used = pi_used[:,0].reshape(-1,1)\n",
        "  pi_used = pi_used.detach().reshape(-1,1)\n",
        "  \n",
        "  before_x = x.detach().clone()\n",
        "  new_x =  Variable(torch.cat((x, pi_used), dim=1), requires_grad=requires_grad)\n",
        "  return before_x, new_x"
      ],
      "metadata": {
        "id": "vHmkskwIFYO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kowxNupDAwOI"
      },
      "source": [
        "List of params for successfull run\n",
        "\n",
        "\n",
        "\n",
        "*   Loss = L1\n",
        "*   Util Net + piNet: depth = 3, NN= 50\n",
        "*   lr = 0.005\n",
        "*   lr_for_pi = 0.002\n",
        "*   max_pi_epochs = 5\n",
        "*   trainMertonAlloc.use_early_stop = False\n",
        "*   epoch = 6100\n",
        "*   loss weights = 1\\*intC + 1\\*terminal\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Htx(x, gamma=1):\n",
        "    # 3.6a in https://arxiv.org/abs/1912.01455v3\n",
        "    #  wealth * gamma * exp(r*tau)\n",
        "    part_a = x[:,1].reshape(-1,1)*gamma*torch.exp(x[:,3].reshape(-1,1) * x[:,0].reshape(-1,1))\n",
        "    #  0.5* tau *[(mu - r)/sigma]^2\n",
        "    part_b = 0.5 * x[:,0].reshape(-1,1) * ((x[:,2].reshape(-1,1) - x[:,3].reshape(-1,1))/(x[:,4].reshape(-1,1)))**2 \n",
        "    return -1.0*torch.exp(-part_a - part_b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kDWNbnM6ihO",
        "outputId": "be4389dd-233a-42a3-eef5-2a7b03193a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.87 ms (started: 2022-07-08 22:33:10 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u35g5sAPbsG_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f23bb626-9694-4ee2-ba9a-1c494003d37a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 10.3 ms (started: 2022-07-08 22:33:10 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# check control for closed form:\n",
        "# PI(x,t) = [(mu-r)/(gamma*sigma^2)]*exp(-r*(T-t))\n",
        "# ((mu-r)/(gamma*(sigma**2)))*np.exp(-r*(1.0-time))\n",
        "# gamma = 1.0 # time = 0.0 # mu = 0.05 # r = 0.02 # sigma = 0.25   # PI\n",
        "\n",
        "gamma = 1\n",
        "internal_sample, terminal_sample = mequation.sample(size=10000, to_cpu=False)\n",
        "#mask = (internal_sample[:,0] > 0.1) & (internal_sample[:,4] > 0.1)\n",
        "#internal_sample = internal_sample[mask.reshape(-1),:]\n",
        "\n",
        "# tau, wealth, mu, r, sigma\n",
        "tau = internal_sample[:,0].cpu().detach()\n",
        "wealth = internal_sample[:,1].cpu().detach()\n",
        "mu = internal_sample[:,2].cpu().detach()\n",
        "r = internal_sample[:,3].cpu().detach()\n",
        "sigma = internal_sample[:,4].cpu().detach()\n",
        "\n",
        "# mequation.pi_net(internal_sample)[:,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Woj8BoAK5OLC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "924179ba-0158-4790-d979-cb8154d5b1ee"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ggplot: (8746915137901)>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 601 ms (started: 2022-07-08 22:33:10 +00:00)\n"
          ]
        }
      ],
      "source": [
        "u_internal_sample = internal_sample\n",
        "\n",
        "u_net_results = trainMertonAlloc.net(u_internal_sample).detach().cpu().numpy().reshape(-1).tolist()\n",
        "htx_results = Htx(u_internal_sample, gamma).cpu().detach().numpy().reshape(-1).tolist()\n",
        "dataf2 = pd.DataFrame( { 'u_net': u_net_results, 'closed_form': htx_results } )\n",
        "ggplot(dataf2, aes(x='u_net', y='closed_form')) + geom_point()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean((dataf2['u_net'] - dataf2['closed_form']) ** 2)"
      ],
      "metadata": {
        "id": "9cVkQKOKFQ9K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a4f906d-8e4b-45fc-b622-2429823732b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.89245682532021e-05"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4.93 ms (started: 2022-07-08 22:33:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHfTC6GljYnd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "85b3f877-c362-45fe-cc00-0c0ac5650499"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ggplot: (8746914994253)>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 710 ms (started: 2022-07-08 22:33:28 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# plot the control function vs the closed form (ideally straight line...)\n",
        "dataf = pd.DataFrame( { 'pi_net': mequation.pi_net(internal_sample).cpu().detach().numpy().reshape(-1).tolist(), \n",
        "                       'closed_form': (((mu-r)/(gamma*(sigma**2)))*np.exp(-r*tau)).numpy().tolist() } )\n",
        "\n",
        "temp = dataf[dataf['closed_form'] < 0.1]\n",
        "# plt.scatter(temp['pi_net'], temp['closed_form'])\n",
        "ggplot(dataf, aes(x='pi_net', y='closed_form')) + geom_point()\n",
        "#g.draw()\n",
        "# ggplot(temp, aes(x='pi_net', y='closed_form')) + geom_point()\n",
        "\n",
        "\n",
        "# plt.yscale('log')\n",
        "# plt.xscale('log')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean((dataf['pi_net'] - dataf['closed_form']) ** 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMx9wMiHyX0z",
        "outputId": "bea23916-2ae3-4840-8515-03a01eb2f4c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0264241100276824"
            ]
          },
          "metadata": {},
          "execution_count": 38
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.05 ms (started: 2022-07-08 22:33:32 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RB73t2QYDYI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FamKRf-ODYLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "filepath = '/content/drive/MyDrive/DGM'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LX8HJdwnp7HT",
        "outputId": "50601e5c-8757-4171-b2d2-ee391a3c3fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "time: 24 s (started: 2022-07-08 14:30:39 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(trainMertonAlloc.net.state_dict(), os.path.join(filepath, 'unet_trained_15k_epochs.pt'))\n",
        "#torch.save(mequation.pi_net.state_dict(), os.path.join(filepath, 'pi_net_trained_15k_epochs_4ep.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmruF-Yup7Jy",
        "outputId": "d1258b09-23a5-4628-d5c3-30e81102592f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 14.2 ms (started: 2022-07-08 07:19:57 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "u_net = MertonUtilityNet(NL=2, NN=64)\n",
        "u_net.load_state_dict(torch.load(os.path.join(filepath, 'unet_trained_15k_epochs.pt')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdX5wjEWrprT",
        "outputId": "136917c3-a59e-4564-a552-44dab42bf972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 589 ms (started: 2022-07-08 14:31:30 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "u_net.to('cuda:0')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aartSECarp28",
        "outputId": "65eab182-a2c9-43e8-b251-2006d479c588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MertonUtilityNet(\n",
              "  (fc_input): Linear(in_features=5, out_features=64, bias=True)\n",
              "  (linears): ModuleList(\n",
              "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (1): Linear(in_features=64, out_features=64, bias=True)\n",
              "  )\n",
              "  (fc_output): Linear(in_features=64, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 63
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4.95 ms (started: 2022-07-08 14:31:57 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xdIWxyQuOV8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ALwOKd1HOV-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f85Kg2zvhMdM"
      },
      "outputs": [],
      "source": [
        "mequation = MertonEquation(MertonUtilityNet( NL = 1 , NN = 3 ), MertonAlternativePiNet( in_size = 5 , out_size = 1, neurons = 100, depth=5 ), 1, 10000.0)\n",
        "# val_sample_to_use = tuple([ x.cpu().detach() for x in mequation.sample(sample_method_X=\"U\", size=1) ] )\n",
        "val_sample_to_use = mequation.sample(sample_method_X=\"U\", size=1) \n",
        "# # gamma = 1.0 # time = 0.0 # mu = 0.05 # r = 0.02 # sigma = 0.25   \n",
        "val_sample_to_use[0][0,0] = 0.0\n",
        "val_sample_to_use[0][0,2] = 0.05\n",
        "val_sample_to_use[0][0,3] = 0.02\n",
        "val_sample_to_use[0][0,4] = 0.25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7ce6_TPrsG4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def u(q):\n",
        "  x = q[:,1]\n",
        "  y = q[:, 0]\n",
        "  return x**2 + y**2\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "t = torch.randn(3, requires_grad=True)\n",
        "u_val = u(torch.cat((x, t), axis=1))\n",
        "\n",
        "print(x,t,u_val)\n",
        "print(torch.cat((t,x)))\n",
        "# 1st derivatives\n",
        "dt = torch.autograd.grad(u_val, x, grad_outputs=torch.ones_like(u_val), create_graph=True, allow_unused=True)\n",
        "print(dt[0])\n",
        "dt = torch.autograd.grad(u_val.sum(), torch.cat((t,x)), create_graph=True)\n",
        "print(dt[0])\n",
        "dx = torch.autograd.grad(u_val.sum(), x, create_graph=True)[0]\n",
        "\n",
        "# 2nd derivatives (higher orders require `create_graph=True`)\n",
        "ddx = torch.autograd.grad(dx.sum(), x)[0]\n",
        "ddx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUOACTjkXBM0"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "2+2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsREgS8YUFl6",
        "outputId": "c5ce70ae-b647-4886-aabf-3121172c7a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wckhSaNdUGlc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "BErSeefeQwQi",
        "bvy0WvxDGCxk",
        "N-GO35FcJPP6",
        "fyFbPZr7I5RE",
        "RNhAbZ727RC_",
        "CNsqOm1ithSG",
        "vkqBrz2JplP9",
        "65nooklCbsdy",
        "oy05I1QFh7EM"
      ],
      "name": "DGM_HJB_IU_working_version.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}