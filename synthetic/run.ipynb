{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3726dc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import imp\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils.monitor import Monitor\n",
    "from envs.mo_env import MultiObjectiveEnv\n",
    "# from gym_env_moll.multiobjective import LunarLander\n",
    "# import gym\n",
    "import json\n",
    "\n",
    "\n",
    "use_cuda =  torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "\n",
    "def generate_next_preference(preference, alpha=10000):\n",
    "    preference = np.array(preference)\n",
    "    preference += 1e-6\n",
    "    \n",
    "    return FloatTensor(np.random.dirichlet(alpha*preference))\n",
    "\n",
    "def init_log_file(log_file_str):\n",
    "    with open(log_file_str, mode='w+') as log_file:\n",
    "        log_file.write('[\\n')\n",
    "\n",
    "def write_log(log_file_str, data, is_json = False):\n",
    "    with open(log_file_str, mode='a+') as log_file:\n",
    "        if is_json:\n",
    "            json.dump(data, log_file)\n",
    "        else:\n",
    "            log_file.write(data)\n",
    "\n",
    "def train(env, agent, args):\n",
    "    log_file_str = './logs/multihead3_Q_log_' + args.env_name + '_' + str(datetime.today().strftime(\"%Y_%m_%d\")) + '.json'\n",
    "    save_loc = './saved_models/'\n",
    "    save_file_name = 'multihead3_Q_log_' + args.env_name + '_' + str(datetime.today().strftime(\"%Y_%m_%d\"))\n",
    "\n",
    "    init_log_file(log_file_str)\n",
    "    fixed_probe = FloatTensor([0.8, 0.2, 0.0, 0.0, 0.0, 0.0])\n",
    "    env.reset()\n",
    "    alpha = args.alpha\n",
    "\n",
    "    dirichet_param = 0.1\n",
    "    dirichet_param_schedule = 0.9/(args.episode_num - 1000)\n",
    "\n",
    "    max_steps_in_env = 100\n",
    "    for num_eps in range(60):\n",
    "        terminal = False\n",
    "        env.reset()\n",
    "        q_loss = 0\n",
    "        exploration_loss = 0\n",
    "        cnt = 0\n",
    "        tot_reward = 0\n",
    "\n",
    "        probe = np.random.randn(6)\n",
    "        probe = FloatTensor(np.abs(probe)/np.linalg.norm(probe, ord=1))\n",
    "    \n",
    "        # probe = generate_next_preference(np.random.uniform(size=len(env.reward_spec)), alpha = 1)\n",
    "        # probe = generate_next_preference(np.ones(shape=len(env.reward_spec))*dirichet_param, alpha = 1)\n",
    "        \n",
    "        # if dirichet_param < 0.99:\n",
    "        #     dirichet_param += dirichet_param_schedule\n",
    "        # else:\n",
    "        #     dirichet_param = 0.99\n",
    "        \n",
    "\n",
    "        # if num_eps % 100 == 0:\n",
    "        #     probe = FloatTensor([0.98, 0.02])\n",
    "        #     probe = generate_next_preference(probe, 200)\n",
    "        \n",
    "        write_log(log_file_str, '[')\n",
    "\n",
    "        while not terminal:\n",
    "            state = env.observe()\n",
    "            action = agent.act(state, probe)\n",
    "            next_state, reward, terminal = env.step(action)\n",
    "            next_preference = generate_next_preference(probe, alpha)\n",
    "            \n",
    "            agent.memorize(state, action, next_state, reward, terminal, probe, next_preference)\n",
    "            loss = agent.learn()\n",
    "            q_loss += loss[0]\n",
    "            exploration_loss += loss[1]\n",
    "\n",
    "            if cnt > max_steps_in_env:\n",
    "                terminal = True\n",
    "                agent.reset()\n",
    "            \n",
    "            tot_reward = tot_reward + (fixed_probe.cpu().numpy().dot(reward)) * np.power(args.gamma, cnt)\n",
    "            probe = next_preference\n",
    "            cnt = cnt + 1\n",
    "\n",
    "            if reward[0] > 8:\n",
    "                print(reward, state)\n",
    "\n",
    "            if args.log and (num_eps % 50) == 0:\n",
    "                _, Q, q = agent.predict(probe, state)\n",
    "\n",
    "                log = {\n",
    "                    'state':state.tolist(),\n",
    "                    'action':action,\n",
    "                    'reward':reward.tolist(),\n",
    "                    'terminal':terminal,\n",
    "                    'probe':probe.detach().numpy().tolist(),\n",
    "                    'q_val': q.tolist(),\n",
    "                    'cnt': cnt,\n",
    "                    'num_eps': num_eps\n",
    "                }\n",
    "\n",
    "                print('probe', probe.detach().numpy().tolist())\n",
    "                print('state', log['state'])\n",
    "                print('action', log['action'])\n",
    "                print('reward', log['reward'])\n",
    "                print('q_val', log['q_val'])\n",
    "                print('Q_val', Q.detach().numpy().tolist())\n",
    "                print('tot_reward', tot_reward)\n",
    "                print('cnt', log['cnt'])\n",
    "                print('num_eps', log['num_eps'])\n",
    "                print('eps', agent.epsilon)\n",
    "                print('---------------------------------------')\n",
    "\n",
    "                write_log(log_file_str, log, True)\n",
    "\n",
    "                if not terminal:\n",
    "                    write_log(log_file_str, ',\\n')\n",
    "                else:\n",
    "                    write_log(log_file_str, '\\n],\\n')\n",
    "\n",
    "\n",
    "        _, Q, q = agent.predict(fixed_probe)\n",
    "\n",
    "        if args.env_name == \"dst\":\n",
    "            act_1 = q[0, 3]\n",
    "            act_2 = q[0, 1]\n",
    "        elif args.env_name in ['ft', 'ft5', 'ft7']:\n",
    "            act_1 = q[0, 1]\n",
    "            act_2 = q[0, 0]\n",
    "\n",
    "        if args.method == \"crl-naive\":\n",
    "            act_1 = act_1.data.cpu()\n",
    "            act_2 = act_2.data.cpu()\n",
    "        elif args.method == \"crl-envelope\":\n",
    "            act_1 = probe.dot(act_1.data)\n",
    "            act_2 = probe.dot(act_2.data)\n",
    "        elif args.method == \"crl-energy\":\n",
    "            act_1 = probe.dot(act_1.data)\n",
    "            act_2 = probe.dot(act_2.data)\n",
    "        print(\"eps %d reward (1) %0.2f, the Q is %0.2f | %0.2f; the probe is %0.2f | %0.2f; dirichet: %0.3f; q_loss: %0.4f; exploration_loss: %0.4f\" % (\n",
    "            num_eps,\n",
    "            tot_reward,\n",
    "            act_1,\n",
    "            act_2,\n",
    "            probe[0],\n",
    "            probe[1],\n",
    "            dirichet_param,\n",
    "            q_loss / cnt,\n",
    "            exploration_loss/cnt))\n",
    "\n",
    "\n",
    "        if (num_eps+1) % 500 == 0:\n",
    "            agent.save(save_loc, save_file_name+\"_eps_\"+str(num_eps))\n",
    "\n",
    "    \n",
    "    agent.save(save_loc, save_file_name+\"_eps_\"+str(num_eps))\n",
    "    return agent\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='MORL')\n",
    "# CONFIG\n",
    "parser.add_argument('--env-name', default='ft', metavar='ENVNAME',\n",
    "                    help='environment to train on: dst | ft | ft5 | ft7')\n",
    "parser.add_argument('--method', default='crl-naive', metavar='METHODS',\n",
    "                    help='methods: crl-naive | crl-envelope | crl-energy')\n",
    "parser.add_argument('--model', default='linear', metavar='MODELS',\n",
    "                    help='linear | cnn | cnn + lstm')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, metavar='GAMMA',\n",
    "                    help='gamma for infinite horizonal MDPs')\n",
    "# TRAINING\n",
    "parser.add_argument('--mem-size', type=int, default=4000, metavar='M',\n",
    "                    help='max size of the replay memory')\n",
    "parser.add_argument('--batch-size', type=int, default=256, metavar='B',\n",
    "                    help='batch size')\n",
    "parser.add_argument('--lr', type=float, default=1e-3, metavar='LR',\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--epsilon', type=float, default=0.5, metavar='EPS',\n",
    "                    help='epsilon greedy exploration')\n",
    "parser.add_argument('--epsilon-decay', default=True, action='store_true',\n",
    "                    help='linear epsilon decay to zero')\n",
    "parser.add_argument('--weight-num', type=int, default=16, metavar='WN',\n",
    "                    help='number of sampled weights per iteration')\n",
    "parser.add_argument('--episode-num', type=int, default=10000, metavar='EN',\n",
    "                    help='number of episodes for training')\n",
    "parser.add_argument('--optimizer', default='Adam', metavar='OPT',\n",
    "                    help='optimizer: Adam | RMSprop')\n",
    "parser.add_argument('--update-freq', type=int, default=100, metavar='OPT',\n",
    "                    help='optimizer: Adam | RMSprop')\n",
    "parser.add_argument('--beta', type=float, default=0.01, metavar='BETA',\n",
    "                    help='(initial) beta for evelope algorithm, default = 0.01')\n",
    "parser.add_argument('--homotopy', default=False, action='store_true',\n",
    "                    help='use homotopy optimization method')\n",
    "# LOG & SAVING\n",
    "parser.add_argument('--serialize', default=False, action='store_true',\n",
    "                    help='serialize a model')\n",
    "parser.add_argument('--save', default='crl/naive/saved/', metavar='SAVE',\n",
    "                    help='path for saving trained models')\n",
    "parser.add_argument('--name', default='', metavar='name',\n",
    "                    help='specify a name for saving the model')\n",
    "parser.add_argument('--log', default='crl/naive/logs/', metavar='LOG',\n",
    "                    help='path for recording training informtion')\n",
    "\n",
    "use_cuda =  torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    " # setup the environment\n",
    "    # args.env_name = 'Lunar'\n",
    "# env = gym.make('gym.envs.multiobjective/LunarLander')\n",
    "env = MultiObjectiveEnv(args.env_name)\n",
    "# get state / action / reward sizes\n",
    "state_size = len(env.state_spec)\n",
    "action_size = env.action_spec[2][1] - env.action_spec[2][0]\n",
    "reward_size = len(env.reward_spec)\n",
    "\n",
    "# generate an agent for initial training\n",
    "agent = None\n",
    "\n",
    "args.alpha = 4000\n",
    "\n",
    "from crl.envelope.meta_mod import MetaAgent\n",
    "# from crl.envelope.models.multiheadoutput import EnvelopeLinearCQN\n",
    "from crl.envelope.models.multihead3 import EnvelopeLinearCQN\n",
    "from crl.envelope.exemplar import Exemplar\n",
    "\n",
    "if args.serialize:\n",
    "    model = torch.load(\"{}{}.pkl\".format(args.save,\n",
    "                                 \"m.{}_e.{}_n.{}\".format(args.model, args.env_name, args.name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "f955a1bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.60557752, 0.26519936, 0.13173717]),\n",
       " array([0.59234412, 0.26514629, 0.11415272]),\n",
       " (3,),\n",
       " (3,))"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_next_preference(preference, alpha=10000):\n",
    "    cov = np.identity(preference.shape[0])*0.0001\n",
    "    y = np.random.multivariate_normal(preference, cov, 1)[0]\n",
    "    while np.any(y < 0):\n",
    "        y = np.random.multivariate_normal(preference, cov, 1)[0]\n",
    "    return y\n",
    "x = np.random.uniform(size=3)\n",
    "y = generate_next_preference(x)\n",
    "y, x, y.shape, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c1f6207d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probe [0.08870701491832733, 0.1992291361093521, 0.109388068318367, 0.23882588744163513, 0.22652246057987213, 0.13732744753360748]\n",
      "state [0, 0]\n",
      "action 0\n",
      "reward [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "q_val [[-0.020355885848402977, -0.06106964498758316]]\n",
      "Q_val [[[-0.05712473765015602, -0.05365177243947983, 0.054477281868457794, -0.04658041149377823, 0.030275505036115646, 0.002545067109167576], [-0.048632439225912094, -0.0428861528635025, -0.04601268470287323, 0.03273971751332283, -0.049255602061748505, -0.009033589623868465]]]\n",
      "tot_reward 0.0\n",
      "cnt 1\n",
      "num_eps 0\n",
      "eps 0.5\n",
      "---------------------------------------\n",
      "probe [0.07985246181488037, 0.20434263348579407, 0.11487565189599991, 0.23212654888629913, 0.21867218613624573, 0.1501305252313614]\n",
      "state [1, 0]\n",
      "action 0\n",
      "reward [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "q_val [[-0.020011883229017258, -0.060004182159900665]]\n",
      "Q_val [[[-0.05555865168571472, -0.05425175279378891, 0.04707087576389313, -0.050934769213199615, 0.03638920560479164, 0.001265830360352993], [-0.05315224826335907, -0.03947099298238754, -0.04178640991449356, 0.02745375595986843, -0.05488245561718941, -0.01234879344701767]]]\n",
      "tot_reward 0.0\n",
      "cnt 2\n",
      "num_eps 0\n",
      "eps 0.5\n",
      "---------------------------------------\n",
      "probe [0.08026572316884995, 0.19305913150310516, 0.11814416944980621, 0.23770642280578613, 0.22425654530525208, 0.14656800031661987]\n",
      "state [2, 0]\n",
      "action 1\n",
      "reward [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "q_val [[-0.020069118589162827, -0.06017795205116272]]\n",
      "Q_val [[[-0.06335513293743134, -0.053780362010002136, 0.041974544525146484, -0.05724029615521431, 0.044098544865846634, 0.005277451127767563], [-0.06262218207120895, -0.034368932247161865, -0.040710702538490295, 0.021684184670448303, -0.0626690536737442, -0.02281506173312664]]]\n",
      "tot_reward 0.0\n",
      "cnt 3\n",
      "num_eps 0\n",
      "eps 0.5\n",
      "---------------------------------------\n",
      "probe [0.08378458023071289, 0.19332900643348694, 0.12366263568401337, 0.23208549618721008, 0.2252262979745865, 0.14191198348999023]\n",
      "state [3, 1]\n",
      "action 1\n",
      "reward [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "q_val [[-0.020934417843818665, -0.06037174165248871]]\n",
      "Q_val [[[-0.07502374798059464, -0.04672226682305336, 0.035875990986824036, -0.07358811795711517, 0.05676385760307312, 0.01411980390548706], [-0.06960500776767731, -0.02494725026190281, -0.04438617825508118, 0.020563041791319847, -0.06450647115707397, -0.029666418209671974]]]\n",
      "tot_reward 0.0\n",
      "cnt 4\n",
      "num_eps 0\n",
      "eps 0.5\n",
      "---------------------------------------\n",
      "probe [0.08331673592329025, 0.19861195981502533, 0.12824909389019012, 0.22312399744987488, 0.2345787137746811, 0.13211949169635773]\n",
      "state [4, 3]\n",
      "action 0\n",
      "reward [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "q_val [[-0.021496213972568512, -0.06020558625459671]]\n",
      "Q_val [[[-0.08637963235378265, -0.03487817943096161, 0.018427617847919464, -0.0921010822057724, 0.06933985650539398, 0.014097809791564941], [-0.07534036040306091, -0.016184359788894653, -0.04723230004310608, 0.021721618250012398, -0.04330742731690407, -0.02524339035153389]]]\n",
      "tot_reward 0.0\n",
      "cnt 5\n",
      "num_eps 0\n",
      "eps 0.5\n",
      "---------------------------------------\n",
      "probe [0.08681288361549377, 0.19889166951179504, 0.13110285997390747, 0.21596577763557434, 0.23824800550937653, 0.12897881865501404]\n",
      "state [5, 6]\n",
      "action 0\n",
      "reward [2.39054781, 1.97492965, 4.51911017, 0.07046741, 1.74139824, 8.18077893]\n",
      "q_val [[-0.02332179993391037, -0.05962080508470535]]\n",
      "Q_val [[[-0.11031368374824524, -0.013858914375305176, 0.011877220124006271, -0.10809210687875748, 0.09235375374555588, 0.019857097417116165], [-0.09679462015628815, 0.0002210773527622223, -0.041447192430496216, 0.0315362811088562, -0.013120301067829132, -0.030000291764736176]]]\n",
      "tot_reward 2.194337466874862\n",
      "cnt 6\n",
      "num_eps 0\n",
      "eps 0.499955\n",
      "---------------------------------------\n",
      "eps 0 reward (1) 2.19, the Q is -0.06 | -0.03; the probe is 0.09 | 0.20; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 1 reward (1) 3.02, the Q is -0.06 | -0.03; the probe is 0.25 | 0.03; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "[8.30192712 0.40973443 1.69099424 4.54961192 2.64473811 0.59753994] [ 5 22]\n",
      "eps 2 reward (1) 6.39, the Q is -0.06 | -0.03; the probe is 0.23 | 0.10; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 3 reward (1) 3.52, the Q is -0.06 | -0.03; the probe is 0.09 | 0.26; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 4 reward (1) 1.36, the Q is -0.06 | -0.03; the probe is 0.09 | 0.30; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 5 reward (1) 4.13, the Q is -0.06 | -0.03; the probe is 0.01 | 0.09; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 6 reward (1) 1.79, the Q is -0.06 | -0.03; the probe is 0.01 | 0.08; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 7 reward (1) 2.48, the Q is -0.06 | -0.03; the probe is 0.19 | 0.10; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 8 reward (1) 1.74, the Q is -0.06 | -0.03; the probe is 0.17 | 0.17; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 9 reward (1) 1.14, the Q is -0.06 | -0.03; the probe is 0.12 | 0.10; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 10 reward (1) 1.88, the Q is -0.06 | -0.03; the probe is 0.01 | 0.13; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 11 reward (1) 4.58, the Q is -0.06 | -0.03; the probe is 0.44 | 0.24; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 12 reward (1) 4.42, the Q is -0.06 | -0.03; the probe is 0.29 | 0.22; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 13 reward (1) 3.52, the Q is -0.06 | -0.03; the probe is 0.16 | 0.35; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 14 reward (1) 4.65, the Q is -0.06 | -0.03; the probe is 0.18 | 0.23; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 15 reward (1) 1.37, the Q is -0.06 | -0.03; the probe is 0.22 | 0.12; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 16 reward (1) 4.00, the Q is -0.06 | -0.03; the probe is 0.32 | 0.14; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 17 reward (1) 3.52, the Q is -0.06 | -0.03; the probe is 0.04 | 0.06; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 18 reward (1) 1.88, the Q is -0.06 | -0.03; the probe is 0.33 | 0.03; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 19 reward (1) 0.57, the Q is -0.06 | -0.03; the probe is 0.44 | 0.05; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 20 reward (1) 3.71, the Q is -0.06 | -0.03; the probe is 0.05 | 0.33; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 21 reward (1) 2.06, the Q is -0.06 | -0.03; the probe is 0.08 | 0.20; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 22 reward (1) 3.73, the Q is -0.06 | -0.03; the probe is 0.00 | 0.20; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 23 reward (1) 3.12, the Q is -0.06 | -0.03; the probe is 0.05 | 0.14; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 24 reward (1) 2.48, the Q is -0.06 | -0.03; the probe is 0.23 | 0.21; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 25 reward (1) 2.02, the Q is -0.06 | -0.03; the probe is 0.19 | 0.10; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 26 reward (1) 4.65, the Q is -0.06 | -0.03; the probe is 0.12 | 0.00; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 27 reward (1) 4.05, the Q is -0.06 | -0.03; the probe is 0.02 | 0.06; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 28 reward (1) 1.10, the Q is -0.06 | -0.03; the probe is 0.21 | 0.06; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 29 reward (1) 4.27, the Q is -0.06 | -0.03; the probe is 0.10 | 0.16; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 30 reward (1) 1.05, the Q is -0.06 | -0.03; the probe is 0.11 | 0.32; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 31 reward (1) 1.96, the Q is -0.06 | -0.03; the probe is 0.22 | 0.07; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 32 reward (1) 5.66, the Q is -0.06 | -0.03; the probe is 0.10 | 0.26; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 33 reward (1) 4.62, the Q is -0.06 | -0.03; the probe is 0.26 | 0.00; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 34 reward (1) 2.59, the Q is -0.06 | -0.03; the probe is 0.25 | 0.11; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 35 reward (1) 2.59, the Q is -0.06 | -0.03; the probe is 0.07 | 0.04; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 36 reward (1) 4.62, the Q is -0.06 | -0.03; the probe is 0.13 | 0.39; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 37 reward (1) 3.99, the Q is -0.06 | -0.03; the probe is 0.13 | 0.56; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 38 reward (1) 1.88, the Q is -0.06 | -0.03; the probe is 0.25 | 0.10; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 39 reward (1) 1.37, the Q is -0.06 | -0.03; the probe is 0.12 | 0.05; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 40 reward (1) 4.13, the Q is -0.06 | -0.03; the probe is 0.30 | 0.14; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 41 reward (1) 1.14, the Q is -0.06 | -0.03; the probe is 0.06 | 0.37; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps 42 reward (1) 0.86, the Q is -0.06 | -0.03; the probe is 0.12 | 0.05; dirichet: 0.100; q_loss: 0.8620; exploration_loss: 0.0506\n",
      "eps 43 reward (1) 2.48, the Q is -0.06 | -0.03; the probe is 0.10 | 0.16; dirichet: 0.100; q_loss: 1.9171; exploration_loss: -5.4208\n",
      "eps 44 reward (1) 2.02, the Q is -0.06 | -0.03; the probe is 0.45 | 0.09; dirichet: 0.100; q_loss: 1.5937; exploration_loss: -316.9390\n",
      "[8.30192712 0.40973443 1.69099424 4.54961192 2.64473811 0.59753994] [ 5 22]\n",
      "eps 45 reward (1) 6.39, the Q is -0.06 | -0.03; the probe is 0.24 | 0.27; dirichet: 0.100; q_loss: 1.5179; exploration_loss: -12961.9158\n",
      "eps 46 reward (1) 3.52, the Q is -0.06 | -0.03; the probe is 0.05 | 0.21; dirichet: 0.100; q_loss: 1.4545; exploration_loss: -293900.3483\n",
      "eps 47 reward (1) 4.00, the Q is -0.06 | -0.03; the probe is 0.26 | 0.03; dirichet: 0.100; q_loss: 1.3064; exploration_loss: -3930411.9375\n",
      "eps 48 reward (1) 5.71, the Q is -0.06 | -0.03; the probe is 0.13 | 0.20; dirichet: 0.100; q_loss: 1.1665; exploration_loss: -33395060.1667\n",
      "eps 49 reward (1) 1.05, the Q is -0.06 | -0.03; the probe is 0.27 | 0.13; dirichet: 0.100; q_loss: 1.0783; exploration_loss: -198624309.3333\n",
      "probe [0.17995603382587433, 0.12943358719348907, 0.17759078741073608, 0.08394630998373032, 0.058086201548576355, 0.37098705768585205]\n",
      "state [0, 0]\n",
      "action 1\n",
      "reward [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "q_val [[-0.023356623947620392, -0.04650353267788887]]\n",
      "Q_val [[[-0.05782477557659149, -0.05108504369854927, 0.05400106683373451, -0.046675123274326324, 0.03147425875067711, 0.002167751081287861], [-0.04856035113334656, -0.04307277128100395, -0.04664384573698044, 0.03248964622616768, -0.050354477018117905, -0.009971633553504944]]]\n",
      "tot_reward 0.0\n",
      "cnt 1\n",
      "num_eps 50\n",
      "eps 0.49774999999999914\n",
      "---------------------------------------\n",
      "probe [0.17730861902236938, 0.1251930147409439, 0.19069698452949524, 0.08143068104982376, 0.05641114339232445, 0.36895954608917236]\n",
      "state [1, 1]\n",
      "action 1\n",
      "reward [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "q_val [[-0.02354196459054947, -0.0468071773648262]]\n",
      "Q_val [[[-0.06260806322097778, -0.04468521475791931, 0.0441473089158535, -0.05404295027256012, 0.036838285624980927, 0.004580210894346237], [-0.04655639827251434, -0.0335891917347908, -0.04647163301706314, 0.029008125886321068, -0.055377762764692307, -0.014347407035529613]]]\n",
      "tot_reward 0.0\n",
      "cnt 2\n",
      "num_eps 50\n",
      "eps 0.49774999999999914\n",
      "---------------------------------------\n",
      "probe [0.17962995171546936, 0.12382824718952179, 0.19812458753585815, 0.08216037601232529, 0.05932549387216568, 0.3569313585758209]\n",
      "state [2, 3]\n",
      "action 0\n",
      "reward [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "q_val [[-0.024531379342079163, -0.046642884612083435]]\n",
      "Q_val [[[-0.07390870898962021, -0.03157142177224159, 0.03550233691930771, -0.06915801763534546, 0.05195508152246475, 0.011147934943437576], [-0.06164054572582245, -0.02322402223944664, -0.043561745434999466, 0.028413208201527596, -0.043766554445028305, -0.02344636619091034]]]\n",
      "tot_reward 0.0\n",
      "cnt 3\n",
      "num_eps 50\n",
      "eps 0.49774999999999914\n",
      "---------------------------------------\n",
      "probe [0.17343734204769135, 0.12346765398979187, 0.20652176439762115, 0.08280494064092636, 0.06550458073616028, 0.3482637405395508]\n",
      "state [3, 6]\n",
      "action 0\n",
      "reward [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "q_val [[-0.026200655847787857, -0.04558728262782097]]\n",
      "Q_val [[[-0.10601334273815155, -0.016519423574209213, 0.030817337334156036, -0.1002868041396141, 0.07339181751012802, 0.01591922715306282], [-0.0904720276594162, -0.006350807845592499, -0.03987486660480499, 0.04059420898556709, -0.012293204665184021, -0.03438431769609451]]]\n",
      "tot_reward 0.0\n",
      "cnt 4\n",
      "num_eps 50\n",
      "eps 0.49774999999999914\n",
      "---------------------------------------\n",
      "probe [0.17565643787384033, 0.12577606737613678, 0.2058662325143814, 0.08289345353841782, 0.0646805614233017, 0.34512725472450256]\n",
      "state [4, 12]\n",
      "action 1\n",
      "reward [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "q_val [[-0.029048101976513863, -0.04112762585282326]]\n",
      "Q_val [[[-0.16812632977962494, 0.014846064150333405, 0.031886860728263855, -0.18005487322807312, 0.11391901969909668, 0.01333765871822834], [-0.14499697089195251, 0.04190516471862793, -0.03456265479326248, 0.0694970190525055, 0.03230639547109604, -0.053736284375190735]]]\n",
      "tot_reward 0.0\n",
      "cnt 5\n",
      "num_eps 50\n",
      "eps 0.49774999999999914\n",
      "---------------------------------------\n",
      "probe [0.17566201090812683, 0.13489152491092682, 0.19852152466773987, 0.07945418357849121, 0.06363330781459808, 0.3478374481201172]\n",
      "state [5, 25]\n",
      "action 0\n",
      "reward [0.93689572, 0.77924846, 2.83896436, 1.98294555, 8.45958836, 3.86763124]\n",
      "q_val [[-0.029703397303819656, -0.03933184966444969]]\n",
      "Q_val [[[-0.28983980417251587, 0.08413051813840866, 0.059251971542835236, -0.3665555715560913, 0.21982210874557495, 0.001629658043384552], [-0.26509973406791687, 0.16248080134391785, -0.03032664582133293, 0.12000531703233719, 0.12163012474775314, -0.09884421527385712]]]\n",
      "tot_reward 0.8609943252129117\n",
      "cnt 6\n",
      "num_eps 50\n",
      "eps 0.4977049999999991\n",
      "---------------------------------------\n",
      "eps 50 reward (1) 0.86, the Q is -0.06 | -0.03; the probe is 0.18 | 0.13; dirichet: 0.100; q_loss: 1.4514; exploration_loss: -903818106.6667\n",
      "eps 51 reward (1) 1.26, the Q is -0.06 | -0.03; the probe is 0.06 | 0.34; dirichet: 0.100; q_loss: 1.4816; exploration_loss: -3318775018.6667\n",
      "eps 52 reward (1) 5.65, the Q is -0.06 | -0.03; the probe is 0.02 | 0.29; dirichet: 0.100; q_loss: 1.0189; exploration_loss: -10420784042.6667\n",
      "eps 53 reward (1) 2.48, the Q is -0.06 | -0.03; the probe is 0.03 | 0.05; dirichet: 0.100; q_loss: 0.9935; exploration_loss: -29144858624.0000\n",
      "eps 54 reward (1) 1.26, the Q is -0.06 | -0.03; the probe is 0.03 | 0.37; dirichet: 0.100; q_loss: 1.0678; exploration_loss: -73601131861.3333\n",
      "eps 55 reward (1) 2.78, the Q is -0.06 | -0.03; the probe is 0.03 | 0.03; dirichet: 0.100; q_loss: 1.1828; exploration_loss: -170297576106.6667\n",
      "eps 56 reward (1) 3.99, the Q is -0.06 | -0.03; the probe is 0.19 | 0.10; dirichet: 0.100; q_loss: 1.2595; exploration_loss: -365402854741.3333\n",
      "eps 57 reward (1) 1.79, the Q is -0.06 | -0.03; the probe is 0.08 | 0.16; dirichet: 0.100; q_loss: 1.2159; exploration_loss: -734453945685.3334\n",
      "eps 58 reward (1) 4.58, the Q is -0.06 | -0.03; the probe is 0.10 | 0.08; dirichet: 0.100; q_loss: 1.1608; exploration_loss: -1396284009130.6667\n",
      "eps 59 reward (1) 1.37, the Q is 0.18 | 0.34; the probe is 0.00 | 0.11; dirichet: 0.100; q_loss: 2.1269; exploration_loss: -2526007569066.6665\n"
     ]
    }
   ],
   "source": [
    "model = EnvelopeLinearCQN(state_size, action_size, reward_size)\n",
    "exemplar_model = Exemplar(reward_size, reward_size, 1e-3, -1, device, 3)\n",
    "agent = MetaAgent(model, exemplar_model, args, is_train=True)  \n",
    "\n",
    "agent = train(env, agent, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "170e4f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(-3.175422e+12, dtype=float32),\n",
       " array([2.3442350e+11, 2.2944825e+11, 2.3466462e+11, 2.3141741e+11,\n",
       "        2.3584940e+11, 2.3442350e+11, 2.2944825e+11, 2.3466462e+11,\n",
       "        2.3141741e+11, 2.3584940e+11], dtype=float32),\n",
       " array([2.9142472e+12, 2.8992476e+12, 2.9603833e+12, 2.9534338e+12,\n",
       "        2.9773338e+12, 2.9434330e+12, 2.9354788e+12, 2.9251747e+12,\n",
       "        2.9450431e+12, 2.9688379e+12], dtype=float32))"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "minibatch = agent.sample(agent.trans_mem, agent.priority_mem, agent.batch_size)\n",
    "batchify = lambda x: list(x) * agent.weight_num\n",
    "state_batch = batchify(map(lambda x: x.s.unsqueeze(0), minibatch))\n",
    "action_batch = batchify(map(lambda x: LongTensor([x.a]), minibatch))\n",
    "reward_batch = batchify(map(lambda x: x.r.unsqueeze(0), minibatch))\n",
    "next_state_batch = batchify(map(lambda x: x.s_.unsqueeze(0), minibatch))\n",
    "terminal_batch = batchify(map(lambda x: x.d, minibatch))\n",
    "\n",
    "# w_batch = batchify(map(lambda x: x.w, minibatch))\n",
    "# w_batch = Variable(torch.stack(w_batch), requires_grad=False).type(FloatTensor)\n",
    "\n",
    "w_batch = list(map(lambda x: x.w, minibatch))\n",
    "w_batch = Variable(torch.stack(w_batch), requires_grad=False).type(FloatTensor)\n",
    "next_w_batch = list(map(lambda x: x.w_, minibatch))\n",
    "next_w_batch = Variable(torch.stack(next_w_batch), requires_grad=False).type(FloatTensor)\n",
    "w_batch, next_w_batch = agent.generate_neighbours(w_batch, next_w_batch, agent.weight_num)\n",
    "\n",
    "exemplar_batch_size = 10\n",
    "index_list = np.random.randint(0, w_batch.shape[0], size=exemplar_batch_size)\n",
    "\n",
    "# sample1 = torch.cat((torch.cat(state_batch, dim=0)[index_list], w_batch[index_list]), dim=1)\n",
    "sample1 = w_batch[index_list]\n",
    "positive = sample1[0:int(sample1.shape[0]/2)]\n",
    "negative = sample1[int(sample1.shape[0]/2):]\n",
    "\n",
    "sample1 = torch.cat((positive, positive), axis=0)\n",
    "sample2 = torch.cat((positive, negative), axis=0)\n",
    "\n",
    "target = torch.cat((torch.ones((positive.shape[0], 1)), torch.zeros((negative.shape[0],1))))\n",
    "\n",
    "exploration_loss = agent.exemplar_exploration.update(sample1, sample2, target)\n",
    "exploration_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1e034b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2704, 0.0390, 0.1751, 0.4148, 0.0037, 0.0969],\n",
       "        [0.1703, 0.3134, 0.0524, 0.0937, 0.0288, 0.3414],\n",
       "        [0.2636, 0.2194, 0.2767, 0.0560, 0.0835, 0.1007],\n",
       "        [0.2969, 0.2855, 0.0879, 0.0326, 0.1278, 0.1693],\n",
       "        [0.4453, 0.0721, 0.2340, 0.1131, 0.1216, 0.0139],\n",
       "        [0.2704, 0.0390, 0.1751, 0.4148, 0.0037, 0.0969],\n",
       "        [0.1703, 0.3134, 0.0524, 0.0937, 0.0288, 0.3414],\n",
       "        [0.2636, 0.2194, 0.2767, 0.0560, 0.0835, 0.1007],\n",
       "        [0.2969, 0.2855, 0.0879, 0.0326, 0.1278, 0.1693],\n",
       "        [0.4453, 0.0721, 0.2340, 0.1131, 0.1216, 0.0139]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6c4e72b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7042e-01, 3.8990e-02, 1.7514e-01, 4.1480e-01, 3.7394e-03, 9.6915e-02],\n",
       "        [1.7032e-01, 3.1336e-01, 5.2384e-02, 9.3736e-02, 2.8825e-02, 3.4138e-01],\n",
       "        [2.6359e-01, 2.1941e-01, 2.7675e-01, 5.6042e-02, 8.3490e-02, 1.0073e-01],\n",
       "        [2.9694e-01, 2.8548e-01, 8.7858e-02, 3.2611e-02, 1.2781e-01, 1.6930e-01],\n",
       "        [4.4533e-01, 7.2120e-02, 2.3401e-01, 1.1310e-01, 1.2156e-01, 1.3882e-02],\n",
       "        [1.0452e-01, 1.1155e-01, 1.4873e-01, 1.7704e-01, 2.3103e-01, 2.2714e-01],\n",
       "        [2.5565e-02, 9.8072e-05, 2.1025e-01, 3.5493e-01, 2.2789e-01, 1.8127e-01],\n",
       "        [2.8549e-01, 8.6374e-02, 1.9811e-01, 2.7463e-01, 2.7206e-02, 1.2820e-01],\n",
       "        [2.2068e-01, 5.9762e-02, 2.0818e-01, 9.6294e-02, 1.8529e-01, 2.2979e-01],\n",
       "        [7.5183e-02, 2.0209e-01, 3.5177e-01, 3.1961e-02, 1.8940e-01, 1.4959e-01]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "478ae880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "        0.5000])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.exemplar_exploration.get_prob(torch.Tensor(sample2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a67f591b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-411236.5000,  393350.4688,  384208.4062],\n",
       "         [-406850.9375,  389155.6562,  380111.0938],\n",
       "         [-411448.1250,  393552.9062,  384406.1250],\n",
       "         [-408593.0938,  390822.0625,  381738.7500],\n",
       "         [-412486.1250,  394545.7500,  385375.8750],\n",
       "         [-411236.5000,  393350.4688,  384208.4062],\n",
       "         [-406850.9375,  389155.6562,  380111.0938],\n",
       "         [-411448.1250,  393552.9062,  384406.1250],\n",
       "         [-408593.0938,  390822.0625,  381738.7500],\n",
       "         [-412486.1250,  394545.7500,  385375.8750]], grad_fn=<AddmmBackward0>),\n",
       " tensor([ 0.1171,  0.1172, 49.1794], grad_fn=<ExpBackward0>))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.exemplar_exploration.encoder1(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "dbed2cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1337813.6250,  1471313.8750, -1381276.7500],\n",
       "         [-1334363.7500,  1467519.6250, -1377714.8750],\n",
       "         [-1348354.8750,  1482906.8750, -1392160.5000],\n",
       "         [-1346769.8750,  1481163.8750, -1390524.1250],\n",
       "         [-1352208.8750,  1487145.6250, -1396139.7500],\n",
       "         [-1344490.5000,  1478657.0000, -1388170.6250],\n",
       "         [-1342675.3750,  1476660.7500, -1386296.5000],\n",
       "         [-1340317.6250,  1474067.7500, -1383862.1250],\n",
       "         [-1344859.2500,  1479062.5000, -1388551.3750],\n",
       "         [-1350278.1250,  1485022.2500, -1394146.2500]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([0.1171, 0.1170, 0.1172], grad_fn=<ExpBackward0>))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.exemplar_exploration.encoder2(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "156ba530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7042e-01, 3.8990e-02, 1.7514e-01, 4.1480e-01, 3.7394e-03, 9.6915e-02],\n",
       "        [1.7032e-01, 3.1336e-01, 5.2384e-02, 9.3736e-02, 2.8825e-02, 3.4138e-01],\n",
       "        [2.6359e-01, 2.1941e-01, 2.7675e-01, 5.6042e-02, 8.3490e-02, 1.0073e-01],\n",
       "        [2.9694e-01, 2.8548e-01, 8.7858e-02, 3.2611e-02, 1.2781e-01, 1.6930e-01],\n",
       "        [4.4533e-01, 7.2120e-02, 2.3401e-01, 1.1310e-01, 1.2156e-01, 1.3882e-02],\n",
       "        [1.0452e-01, 1.1155e-01, 1.4873e-01, 1.7704e-01, 2.3103e-01, 2.2714e-01],\n",
       "        [2.5565e-02, 9.8072e-05, 2.1025e-01, 3.5493e-01, 2.2789e-01, 1.8127e-01],\n",
       "        [2.8549e-01, 8.6374e-02, 1.9811e-01, 2.7463e-01, 2.7206e-02, 1.2820e-01],\n",
       "        [2.2068e-01, 5.9762e-02, 2.0818e-01, 9.6294e-02, 1.8529e-01, 2.2979e-01],\n",
       "        [7.5183e-02, 2.0209e-01, 3.5177e-01, 3.1961e-02, 1.8940e-01, 1.4959e-01]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "42041e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 4.4945,  4.9176,  5.6169,  5.1803,  5.3713,  4.9508],\n",
       "        [-0.5287, -0.4582, -0.3218, -0.1352, -0.4411,  0.2590],\n",
       "        [ 5.5103,  5.1233,  5.4005,  4.8351,  4.6252,  5.1524],\n",
       "        [ 4.8499,  3.8873,  3.9201,  4.3542,  3.6390,  3.7796],\n",
       "        [-0.1354, -0.1777,  0.2604, -0.3452,  0.1909, -0.4116],\n",
       "        [ 5.4439,  5.6386,  5.5182,  5.1969,  4.7592,  4.6876]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.exemplar_exploration.encoder1.input_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "69f7255c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4052, -0.2013,  0.1476,  0.0046, -0.1058, -0.1568],\n",
       "        [ 0.6673,  0.0222, -0.4492,  0.0455, -0.2971,  0.2218],\n",
       "        [-0.0672, -0.0889, -0.3498, -0.5678, -0.6732,  0.3505],\n",
       "        [-0.3164,  0.2907, -0.4615, -0.3610,  0.0238,  0.6773],\n",
       "        [ 0.5307,  0.3437, -0.3207, -0.5510,  0.5744,  0.1655],\n",
       "        [-0.1069,  0.1500,  0.8121,  0.5394,  0.0563, -0.1241]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.exemplar_exploration.encoder1.middle_layers[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6186f25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0508,  0.1165, -0.4575, -0.5614, -0.5016, -0.3066],\n",
       "        [-0.6025,  0.4505,  0.3224,  0.1770, -0.4793,  0.5280],\n",
       "        [ 0.4596,  0.3312, -0.0333,  0.6614,  0.4940,  0.1429],\n",
       "        [ 0.5125,  0.3081, -0.7372, -0.3431, -0.1594,  0.3183],\n",
       "        [-0.0357, -0.5719, -0.4612,  0.1695,  0.5028,  0.4007],\n",
       "        [-0.6890,  0.4767, -0.2983, -0.1980, -0.4688,  0.4817]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.exemplar_exploration.encoder1.middle_layers[1].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e3bfa12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2280, -0.0511,  0.0087, -0.1226,  0.1295, -0.4337],\n",
       "        [ 0.7213, -0.6539, -0.5365,  0.8129, -0.0790,  0.1673],\n",
       "        [ 0.7710,  0.1478, -0.3960,  0.1658, -0.3018, -0.0182]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.exemplar_exploration.encoder1.output_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9cdae907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4219,  0.0388,  0.6717,  0.5690,  0.0200,  0.3857],\n",
       "        [-0.2483, -0.8850, -0.5738, -0.2108, -0.6203,  0.4135],\n",
       "        [ 0.3906, -0.6704, -0.3467,  0.4073,  0.5902, -0.4055],\n",
       "        [ 0.0849,  0.4339, -0.4858, -0.1623, -0.0183, -0.3314],\n",
       "        [ 0.2226,  0.7325, -0.3647,  0.0252,  0.2816, -0.7248],\n",
       "        [ 0.3811,  0.2219, -0.3775, -0.5197, -0.0470,  0.2148]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.exemplar_exploration.encoder2.input_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "09999733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0442, -0.6948,  0.0905, -0.5359,  0.3059, -0.6966],\n",
       "        [ 0.4607, -0.4929, -0.6293,  0.3777, -0.0472, -0.6598],\n",
       "        [ 0.5017, -0.2959,  0.6547, -0.4876,  0.4253, -0.1499],\n",
       "        [-0.7312,  0.4837, -0.5116, -0.2307, -0.5350, -0.3990],\n",
       "        [ 0.0530, -0.3585,  0.5933, -0.4743, -0.4582,  0.5733],\n",
       "        [-0.1381,  0.5811,  0.1330,  0.5490,  0.2900, -0.3450]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.exemplar_exploration.encoder2.middle_layers[1].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bd985a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2358,  0.7143, -0.0748,  0.3639,  0.4208, -0.1441],\n",
       "        [ 0.6619, -0.5357, -0.3894,  0.5004,  0.4789,  0.0025],\n",
       "        [-0.2524,  0.2116,  0.2926, -0.1054, -0.0636,  0.4401],\n",
       "        [-0.7706,  0.2716, -0.7749, -0.4349, -0.9102,  0.6747],\n",
       "        [-0.4367, -0.2666, -0.1968, -0.6260, -0.5411, -0.2329],\n",
       "        [ 0.2322,  0.3201,  0.5454, -0.6749, -0.2609, -0.1297]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.exemplar_exploration.encoder2.middle_layers[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cf75442a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4517, -0.5651,  0.3831, -0.7905, -0.5621,  0.5282],\n",
       "        [ 0.5757,  0.6838, -0.4832,  0.7202,  0.5067, -0.0118],\n",
       "        [ 0.1186, -0.5849,  0.3356,  0.5019, -0.4892, -0.7697]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.exemplar_exploration.encoder2.output_layer.weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
