{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3726dc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import imp\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils.monitor import Monitor\n",
    "from envs.mo_env import MultiObjectiveEnv\n",
    "# from gym_env_moll.multiobjective import LunarLander\n",
    "# import gym\n",
    "import json\n",
    "\n",
    "\n",
    "use_cuda =  torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "\n",
    "def generate_next_preference(preference, alpha=10000):\n",
    "    preference = np.array(preference)\n",
    "    preference += 1e-6\n",
    "    \n",
    "    return FloatTensor(np.random.dirichlet(alpha*preference))\n",
    "\n",
    "def init_log_file(log_file_str):\n",
    "    with open(log_file_str, mode='w+') as log_file:\n",
    "        log_file.write('[\\n')\n",
    "\n",
    "def write_log(log_file_str, data, is_json = False):\n",
    "    with open(log_file_str, mode='a+') as log_file:\n",
    "        if is_json:\n",
    "            json.dump(data, log_file)\n",
    "        else:\n",
    "            log_file.write(data)\n",
    "\n",
    "def train(env, agent, args):\n",
    "    log_file_str = './logs/multihead3_Q_log_' + args.env_name + '_' + str(datetime.today().strftime(\"%Y_%m_%d\")) + '.json'\n",
    "    save_loc = './saved_models/'\n",
    "    save_file_name = 'multihead3_Q_log_' + args.env_name + '_' + str(datetime.today().strftime(\"%Y_%m_%d\"))\n",
    "\n",
    "    init_log_file(log_file_str)\n",
    "    fixed_probe = FloatTensor([0.8, 0.2, 0.0, 0.0, 0.0, 0.0])\n",
    "    env.reset()\n",
    "    alpha = args.alpha\n",
    "\n",
    "    dirichet_param = 0.1\n",
    "    dirichet_param_schedule = 0.9/(args.episode_num - 1000)\n",
    "\n",
    "    max_steps_in_env = 100\n",
    "    for num_eps in range(60):\n",
    "        terminal = False\n",
    "        env.reset()\n",
    "        q_loss = 0\n",
    "        exploration_loss = 0\n",
    "        cnt = 0\n",
    "        tot_reward = 0\n",
    "\n",
    "        probe = np.random.randn(6)\n",
    "        probe = FloatTensor(np.abs(probe)/np.linalg.norm(probe, ord=1))\n",
    "    \n",
    "        # probe = generate_next_preference(np.random.uniform(size=len(env.reward_spec)), alpha = 1)\n",
    "        # probe = generate_next_preference(np.ones(shape=len(env.reward_spec))*dirichet_param, alpha = 1)\n",
    "        \n",
    "        # if dirichet_param < 0.99:\n",
    "        #     dirichet_param += dirichet_param_schedule\n",
    "        # else:\n",
    "        #     dirichet_param = 0.99\n",
    "        \n",
    "\n",
    "        # if num_eps % 100 == 0:\n",
    "        #     probe = FloatTensor([0.98, 0.02])\n",
    "        #     probe = generate_next_preference(probe, 200)\n",
    "        \n",
    "        write_log(log_file_str, '[')\n",
    "\n",
    "        while not terminal:\n",
    "            state = env.observe()\n",
    "            action = agent.act(state, probe)\n",
    "            next_state, reward, terminal = env.step(action)\n",
    "            next_preference = generate_next_preference(probe, alpha)\n",
    "            \n",
    "            agent.memorize(state, action, next_state, reward, terminal, probe, next_preference)\n",
    "            loss = agent.learn()\n",
    "            q_loss += loss[0]\n",
    "            exploration_loss += loss[1]\n",
    "\n",
    "            if cnt > max_steps_in_env:\n",
    "                terminal = True\n",
    "                agent.reset()\n",
    "            \n",
    "            tot_reward = tot_reward + (fixed_probe.cpu().numpy().dot(reward)) * np.power(args.gamma, cnt)\n",
    "            probe = next_preference\n",
    "            cnt = cnt + 1\n",
    "\n",
    "            if reward[0] > 8:\n",
    "                print(reward, state)\n",
    "\n",
    "            if args.log and (num_eps % 50) == 0:\n",
    "                _, Q, q = agent.predict(probe, state)\n",
    "\n",
    "                log = {\n",
    "                    'state':state.tolist(),\n",
    "                    'action':action,\n",
    "                    'reward':reward.tolist(),\n",
    "                    'terminal':terminal,\n",
    "                    'probe':probe.detach().numpy().tolist(),\n",
    "                    'q_val': q.tolist(),\n",
    "                    'cnt': cnt,\n",
    "                    'num_eps': num_eps\n",
    "                }\n",
    "\n",
    "                print('probe', probe.detach().numpy().tolist())\n",
    "                print('state', log['state'])\n",
    "                print('action', log['action'])\n",
    "                print('reward', log['reward'])\n",
    "                print('q_val', log['q_val'])\n",
    "                print('Q_val', Q.detach().numpy().tolist())\n",
    "                print('tot_reward', tot_reward)\n",
    "                print('cnt', log['cnt'])\n",
    "                print('num_eps', log['num_eps'])\n",
    "                print('eps', agent.epsilon)\n",
    "                print('---------------------------------------')\n",
    "\n",
    "                write_log(log_file_str, log, True)\n",
    "\n",
    "                if not terminal:\n",
    "                    write_log(log_file_str, ',\\n')\n",
    "                else:\n",
    "                    write_log(log_file_str, '\\n],\\n')\n",
    "\n",
    "\n",
    "        _, Q, q = agent.predict(fixed_probe)\n",
    "\n",
    "        if args.env_name == \"dst\":\n",
    "            act_1 = q[0, 3]\n",
    "            act_2 = q[0, 1]\n",
    "        elif args.env_name in ['ft', 'ft5', 'ft7']:\n",
    "            act_1 = q[0, 1]\n",
    "            act_2 = q[0, 0]\n",
    "\n",
    "        if args.method == \"crl-naive\":\n",
    "            act_1 = act_1.data.cpu()\n",
    "            act_2 = act_2.data.cpu()\n",
    "        elif args.method == \"crl-envelope\":\n",
    "            act_1 = probe.dot(act_1.data)\n",
    "            act_2 = probe.dot(act_2.data)\n",
    "        elif args.method == \"crl-energy\":\n",
    "            act_1 = probe.dot(act_1.data)\n",
    "            act_2 = probe.dot(act_2.data)\n",
    "        print(\"eps %d reward (1) %0.2f, the Q is %0.2f | %0.2f; the probe is %0.2f | %0.2f; dirichet: %0.3f; q_loss: %0.4f; exploration_loss: %0.4f\" % (\n",
    "            num_eps,\n",
    "            tot_reward,\n",
    "            act_1,\n",
    "            act_2,\n",
    "            probe[0],\n",
    "            probe[1],\n",
    "            dirichet_param,\n",
    "            q_loss / cnt,\n",
    "            exploration_loss/cnt))\n",
    "\n",
    "\n",
    "        if (num_eps+1) % 500 == 0:\n",
    "            agent.save(save_loc, save_file_name+\"_eps_\"+str(num_eps))\n",
    "\n",
    "    \n",
    "    agent.save(save_loc, save_file_name+\"_eps_\"+str(num_eps))\n",
    "    return agent\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='MORL')\n",
    "# CONFIG\n",
    "parser.add_argument('--env-name', default='ft', metavar='ENVNAME',\n",
    "                    help='environment to train on: dst | ft | ft5 | ft7')\n",
    "parser.add_argument('--method', default='crl-naive', metavar='METHODS',\n",
    "                    help='methods: crl-naive | crl-envelope | crl-energy')\n",
    "parser.add_argument('--model', default='linear', metavar='MODELS',\n",
    "                    help='linear | cnn | cnn + lstm')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, metavar='GAMMA',\n",
    "                    help='gamma for infinite horizonal MDPs')\n",
    "# TRAINING\n",
    "parser.add_argument('--mem-size', type=int, default=4000, metavar='M',\n",
    "                    help='max size of the replay memory')\n",
    "parser.add_argument('--batch-size', type=int, default=256, metavar='B',\n",
    "                    help='batch size')\n",
    "parser.add_argument('--lr', type=float, default=1e-3, metavar='LR',\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--epsilon', type=float, default=0.5, metavar='EPS',\n",
    "                    help='epsilon greedy exploration')\n",
    "parser.add_argument('--epsilon-decay', default=True, action='store_true',\n",
    "                    help='linear epsilon decay to zero')\n",
    "parser.add_argument('--weight-num', type=int, default=16, metavar='WN',\n",
    "                    help='number of sampled weights per iteration')\n",
    "parser.add_argument('--episode-num', type=int, default=10000, metavar='EN',\n",
    "                    help='number of episodes for training')\n",
    "parser.add_argument('--optimizer', default='Adam', metavar='OPT',\n",
    "                    help='optimizer: Adam | RMSprop')\n",
    "parser.add_argument('--update-freq', type=int, default=100, metavar='OPT',\n",
    "                    help='optimizer: Adam | RMSprop')\n",
    "parser.add_argument('--beta', type=float, default=0.01, metavar='BETA',\n",
    "                    help='(initial) beta for evelope algorithm, default = 0.01')\n",
    "parser.add_argument('--homotopy', default=False, action='store_true',\n",
    "                    help='use homotopy optimization method')\n",
    "# LOG & SAVING\n",
    "parser.add_argument('--serialize', default=False, action='store_true',\n",
    "                    help='serialize a model')\n",
    "parser.add_argument('--save', default='crl/naive/saved/', metavar='SAVE',\n",
    "                    help='path for saving trained models')\n",
    "parser.add_argument('--name', default='', metavar='name',\n",
    "                    help='specify a name for saving the model')\n",
    "parser.add_argument('--log', default='crl/naive/logs/', metavar='LOG',\n",
    "                    help='path for recording training informtion')\n",
    "\n",
    "use_cuda =  torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    " # setup the environment\n",
    "    # args.env_name = 'Lunar'\n",
    "# env = gym.make('gym.envs.multiobjective/LunarLander')\n",
    "env = MultiObjectiveEnv(args.env_name)\n",
    "# get state / action / reward sizes\n",
    "state_size = len(env.state_spec)\n",
    "action_size = env.action_spec[2][1] - env.action_spec[2][0]\n",
    "reward_size = len(env.reward_spec)\n",
    "\n",
    "# generate an agent for initial training\n",
    "agent = None\n",
    "\n",
    "args.alpha = 4000\n",
    "\n",
    "from crl.envelope.meta_mod import MetaAgent\n",
    "# from crl.envelope.models.multiheadoutput import EnvelopeLinearCQN\n",
    "from crl.envelope.models.multihead3 import EnvelopeLinearCQN\n",
    "from crl.envelope.exemplar import Exemplar\n",
    "\n",
    "if args.serialize:\n",
    "    model = torch.load(\"{}{}.pkl\".format(args.save,\n",
    "                                 \"m.{}_e.{}_n.{}\".format(args.model, args.env_name, args.name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c1f6207d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probe [0.25677719712257385, 0.33327680826187134, 0.10112029314041138, 0.20036625862121582, 0.012989888899028301, 0.09546953439712524]\n",
      "state [0, 0]\n",
      "action 0\n",
      "reward [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "q_val [[-0.04470448195934296, 0.03817763179540634]]\n",
      "Q_val [[[0.019332872703671455, 0.051032863557338715, -0.06491155922412872, -0.014705541543662548, -0.025676142424345016, -0.01069581974297762], [0.06739066541194916, -0.010870634578168392, -0.051332782953977585, -0.03402823954820633, -0.04388321191072464, 0.02700396440923214]]]\n",
      "tot_reward 0.0\n",
      "cnt 1\n",
      "num_eps 0\n",
      "eps 0.5\n",
      "---------------------------------------\n",
      "probe [0.2548563480377197, 0.3373427391052246, 0.0946146696805954, 0.20052200555801392, 0.01254075113683939, 0.10012347996234894]\n",
      "state [1, 0]\n",
      "action 1\n",
      "reward [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "q_val [[-0.04460306093096733, 0.03795444220304489]]\n",
      "Q_val [[[0.017160890623927116, 0.0507727712392807, -0.0655260905623436, -0.011765580624341965, -0.025441974401474, -0.013694685883820057], [0.07393142580986023, -0.019200343638658524, -0.05121471732854843, -0.02862662449479103, -0.04528181254863739, 0.022832857444882393]]]\n",
      "tot_reward 0.0\n",
      "cnt 2\n",
      "num_eps 0\n",
      "eps 0.5\n",
      "---------------------------------------\n",
      "probe [0.25467467308044434, 0.33305707573890686, 0.09387555718421936, 0.20321878790855408, 0.01484447717666626, 0.1003294289112091]\n",
      "state [2, 1]\n",
      "action 1\n",
      "reward [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "q_val [[-0.04399183765053749, 0.037263885140419006]]\n",
      "Q_val [[[0.02197832241654396, 0.053894754499197006, -0.07027758657932281, -0.006660550832748413, -0.031525708734989166, -0.014952932484447956], [0.07893064618110657, -0.021150005981326103, -0.04440448433160782, -0.02320304699242115, -0.047453925013542175, 0.0170599352568388]]]\n",
      "tot_reward 0.0\n",
      "cnt 3\n",
      "num_eps 0\n",
      "eps 0.5\n",
      "---------------------------------------\n",
      "probe [0.26372987031936646, 0.325797975063324, 0.08858184516429901, 0.2026902586221695, 0.01751076430082321, 0.10168927907943726]\n",
      "state [3, 3]\n",
      "action 1\n",
      "reward [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "q_val [[-0.042751677334308624, 0.03662920743227005]]\n",
      "Q_val [[[0.03270891308784485, 0.05168354883790016, -0.07628100365400314, -0.006677950266748667, -0.04205695539712906, -0.002662248909473419], [0.09885291755199432, -0.015520941466093063, -0.049411654472351074, -0.0049187541007995605, -0.04530593007802963, 0.01280593778938055]]]\n",
      "tot_reward 0.0\n",
      "cnt 4\n",
      "num_eps 0\n",
      "eps 0.5\n",
      "---------------------------------------\n",
      "probe [0.2616885006427765, 0.33994603157043457, 0.08317215740680695, 0.2055409848690033, 0.01600223407149315, 0.09365011006593704]\n",
      "state [4, 7]\n",
      "action 0\n",
      "reward [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "q_val [[-0.038893237709999084, 0.03563084453344345]]\n",
      "Q_val [[[0.05175638198852539, 0.047071412205696106, -0.10086953639984131, 0.018413281068205833, -0.06225442886352539, 0.02509596198797226], [0.12804976105690002, -0.017502935603260994, -0.05990105867385864, 0.028268039226531982, -0.024852823466062546, 0.022502290084958076]]]\n",
      "tot_reward 0.0\n",
      "cnt 5\n",
      "num_eps 0\n",
      "eps 0.5\n",
      "---------------------------------------\n",
      "probe [0.2627452611923218, 0.3484707772731781, 0.07981328666210175, 0.1963169276714325, 0.014907777309417725, 0.09774597734212875]\n",
      "state [5, 14]\n",
      "action 1\n",
      "reward [0.63888729, 0.28507461, 4.87857435, 6.41971655, 5.85711844, 0.43757381]\n",
      "q_val [[-0.03315108269453049, 0.0345035195350647]]\n",
      "Q_val [[[0.06819809973239899, 0.03487258404493332, -0.14051774144172668, 0.07402443140745163, -0.1005086749792099, 0.05684109777212143], [0.167322039604187, -0.03423605114221573, -0.06629611551761627, 0.07526363432407379, 0.02557002753019333, 0.030232727527618408]]]\n",
      "tot_reward 0.5402809962066992\n",
      "cnt 6\n",
      "num_eps 0\n",
      "eps 0.499955\n",
      "---------------------------------------\n",
      "eps 0 reward (1) 0.54, the Q is 0.04 | -0.05; the probe is 0.26 | 0.35; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 1 reward (1) 3.73, the Q is 0.04 | -0.05; the probe is 0.11 | 0.04; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 2 reward (1) 1.37, the Q is 0.04 | -0.05; the probe is 0.01 | 0.30; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 3 reward (1) 4.05, the Q is 0.04 | -0.05; the probe is 0.27 | 0.06; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 4 reward (1) 2.48, the Q is 0.04 | -0.05; the probe is 0.32 | 0.05; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 5 reward (1) 1.36, the Q is 0.04 | -0.05; the probe is 0.01 | 0.02; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 6 reward (1) 2.19, the Q is 0.04 | -0.05; the probe is 0.08 | 0.17; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "[8.30192712 0.40973443 1.69099424 4.54961192 2.64473811 0.59753994] [ 5 22]\n",
      "eps 7 reward (1) 6.39, the Q is 0.04 | -0.05; the probe is 0.14 | 0.31; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "[8.30192712 0.40973443 1.69099424 4.54961192 2.64473811 0.59753994] [ 5 22]\n",
      "eps 8 reward (1) 6.39, the Q is 0.04 | -0.05; the probe is 0.01 | 0.10; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 9 reward (1) 1.05, the Q is 0.04 | -0.05; the probe is 0.05 | 0.29; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 10 reward (1) 1.26, the Q is 0.04 | -0.05; the probe is 0.15 | 0.03; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 11 reward (1) 1.77, the Q is 0.04 | -0.05; the probe is 0.23 | 0.10; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 12 reward (1) 0.86, the Q is 0.04 | -0.05; the probe is 0.49 | 0.13; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 13 reward (1) 2.06, the Q is 0.04 | -0.05; the probe is 0.19 | 0.04; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 14 reward (1) 3.79, the Q is 0.04 | -0.05; the probe is 0.37 | 0.18; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 15 reward (1) 4.05, the Q is 0.04 | -0.05; the probe is 0.13 | 0.04; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 16 reward (1) 1.74, the Q is 0.04 | -0.05; the probe is 0.05 | 0.23; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 17 reward (1) 0.86, the Q is 0.04 | -0.05; the probe is 0.17 | 0.16; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 18 reward (1) 0.88, the Q is 0.04 | -0.05; the probe is 0.14 | 0.06; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 19 reward (1) 4.75, the Q is 0.04 | -0.05; the probe is 0.20 | 0.25; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 20 reward (1) 3.10, the Q is 0.04 | -0.05; the probe is 0.17 | 0.30; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 21 reward (1) 4.62, the Q is 0.04 | -0.05; the probe is 0.23 | 0.28; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 22 reward (1) 1.74, the Q is 0.04 | -0.05; the probe is 0.32 | 0.26; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 23 reward (1) 3.12, the Q is 0.04 | -0.05; the probe is 0.41 | 0.18; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 24 reward (1) 1.14, the Q is 0.04 | -0.05; the probe is 0.02 | 0.39; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 25 reward (1) 1.36, the Q is 0.04 | -0.05; the probe is 0.23 | 0.24; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 26 reward (1) 1.37, the Q is 0.04 | -0.05; the probe is 0.05 | 0.01; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 27 reward (1) 0.59, the Q is 0.04 | -0.05; the probe is 0.22 | 0.23; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 28 reward (1) 5.71, the Q is 0.04 | -0.05; the probe is 0.20 | 0.14; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 29 reward (1) 0.81, the Q is 0.04 | -0.05; the probe is 0.10 | 0.21; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 30 reward (1) 4.13, the Q is 0.04 | -0.05; the probe is 0.15 | 0.39; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 31 reward (1) 2.19, the Q is 0.04 | -0.05; the probe is 0.18 | 0.02; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 32 reward (1) 2.78, the Q is 0.04 | -0.05; the probe is 0.06 | 0.26; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 33 reward (1) 3.52, the Q is 0.04 | -0.05; the probe is 0.26 | 0.08; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 34 reward (1) 1.37, the Q is 0.04 | -0.05; the probe is 0.11 | 0.02; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 35 reward (1) 4.27, the Q is 0.04 | -0.05; the probe is 0.16 | 0.23; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 36 reward (1) 1.92, the Q is 0.04 | -0.05; the probe is 0.25 | 0.06; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 37 reward (1) 3.02, the Q is 0.04 | -0.05; the probe is 0.13 | 0.07; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 38 reward (1) 3.42, the Q is 0.04 | -0.05; the probe is 0.26 | 0.01; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 39 reward (1) 2.02, the Q is 0.04 | -0.05; the probe is 0.15 | 0.22; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 40 reward (1) 1.26, the Q is 0.04 | -0.05; the probe is 0.34 | 0.10; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n",
      "eps 41 reward (1) 0.86, the Q is 0.04 | -0.05; the probe is 0.25 | 0.18; dirichet: 0.100; q_loss: 0.0000; exploration_loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps 42 reward (1) 5.88, the Q is 0.04 | -0.05; the probe is 0.18 | 0.30; dirichet: 0.100; q_loss: 0.8532; exploration_loss: 0.2917\n",
      "eps 43 reward (1) 2.78, the Q is 0.04 | -0.05; the probe is 0.03 | 0.22; dirichet: 0.100; q_loss: 1.9425; exploration_loss: 0.7428\n",
      "eps 44 reward (1) 0.88, the Q is 0.04 | -0.05; the probe is 0.10 | 0.08; dirichet: 0.100; q_loss: 1.6693; exploration_loss: 0.7838\n",
      "eps 45 reward (1) 3.12, the Q is 0.04 | -0.05; the probe is 0.18 | 0.18; dirichet: 0.100; q_loss: 1.6190; exploration_loss: 0.8724\n",
      "eps 46 reward (1) 0.88, the Q is 0.04 | -0.05; the probe is 0.12 | 0.02; dirichet: 0.100; q_loss: 1.5443; exploration_loss: 0.7686\n",
      "eps 47 reward (1) 5.58, the Q is 0.04 | -0.05; the probe is 0.15 | 0.34; dirichet: 0.100; q_loss: 1.4249; exploration_loss: 0.7190\n",
      "eps 48 reward (1) 4.27, the Q is 0.04 | -0.05; the probe is 0.14 | 0.23; dirichet: 0.100; q_loss: 1.3143; exploration_loss: 0.7120\n",
      "eps 49 reward (1) 5.65, the Q is 0.04 | -0.05; the probe is 0.00 | 0.02; dirichet: 0.100; q_loss: 1.3998; exploration_loss: 0.7213\n",
      "probe [0.12174659222364426, 0.2315063774585724, 0.12223909050226212, 0.14295276999473572, 0.3608492612838745, 0.02070593275129795]\n",
      "state [0, 0]\n",
      "action 0\n",
      "reward [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "q_val [[-0.04709247127175331, 0.02592390403151512]]\n",
      "Q_val [[[0.021547380834817886, 0.055229656398296356, -0.06577540189027786, -0.016739211976528168, -0.024548884481191635, -0.01485992781817913], [0.0704842284321785, -0.015001012943685055, -0.049391090869903564, -0.03326612710952759, -0.041776422411203384, 0.02683299407362938]]]\n",
      "tot_reward 0.0\n",
      "cnt 1\n",
      "num_eps 50\n",
      "eps 0.49774999999999914\n",
      "---------------------------------------\n",
      "probe [0.1246073767542839, 0.23080642521381378, 0.11899460852146149, 0.15273743867874146, 0.3562361001968384, 0.016618048772215843]\n",
      "state [1, 0]\n",
      "action 0\n",
      "reward [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "q_val [[-0.04643799737095833, 0.0262615866959095]]\n",
      "Q_val [[[0.017759282141923904, 0.053140606731176376, -0.06845182180404663, -0.013447005301713943, -0.02616073004901409, -0.017422780394554138], [0.07525157928466797, -0.02121133729815483, -0.04961579293012619, -0.027879435569047928, -0.04435053840279579, 0.022268399596214294]]]\n",
      "tot_reward 0.0\n",
      "cnt 2\n",
      "num_eps 50\n",
      "eps 0.49774999999999914\n",
      "---------------------------------------\n",
      "probe [0.13059186935424805, 0.23429667949676514, 0.11995813995599747, 0.14899007976055145, 0.3499138653278351, 0.01624934747815132]\n",
      "state [2, 0]\n",
      "action 0\n",
      "reward [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "q_val [[-0.046028923243284225, 0.026144523173570633]]\n",
      "Q_val [[[0.013294167816638947, 0.05242593213915825, -0.07801199704408646, -0.009472126141190529, -0.02727736160159111, -0.021762609481811523], [0.0780276358127594, -0.02154732123017311, -0.0417582169175148, -0.022851912304759026, -0.04398445785045624, 0.01626916415989399]]]\n",
      "tot_reward 0.0\n",
      "cnt 3\n",
      "num_eps 50\n",
      "eps 0.49774999999999914\n",
      "---------------------------------------\n",
      "probe [0.12525299191474915, 0.231574147939682, 0.11994356662034988, 0.15276706218719482, 0.35713228583335876, 0.013329945504665375]\n",
      "state [3, 0]\n",
      "action 0\n",
      "reward [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "q_val [[-0.046499453485012054, 0.02624184638261795]]\n",
      "Q_val [[[0.012258637696504593, 0.05169191211462021, -0.08170224726200104, -0.007126983720809221, -0.03009561263024807, -0.0243836697191], [0.08204749971628189, -0.015841685235500336, -0.03684506565332413, -0.018468528985977173, -0.042449548840522766, 0.00576595775783062]]]\n",
      "tot_reward 0.0\n",
      "cnt 4\n",
      "num_eps 50\n",
      "eps 0.49774999999999914\n",
      "---------------------------------------\n",
      "probe [0.11824853718280792, 0.22581997513771057, 0.1229061558842659, 0.14423468708992004, 0.37209391593933105, 0.016696708276867867]\n",
      "state [4, 0]\n",
      "action 0\n",
      "reward [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "q_val [[-0.046979960054159164, 0.02520696073770523]]\n",
      "Q_val [[[0.015022272244095802, 0.04995633661746979, -0.08359447121620178, -0.005934396293014288, -0.03708159551024437, -0.025641566142439842], [0.08595980703830719, -0.011508477851748466, -0.03392021358013153, -0.012900002300739288, -0.03732533007860184, -0.004052523523569107]]]\n",
      "tot_reward 0.0\n",
      "cnt 5\n",
      "num_eps 50\n",
      "eps 0.49774999999999914\n",
      "---------------------------------------\n",
      "probe [0.12006426602602005, 0.22153443098068237, 0.11882349848747253, 0.14687298238277435, 0.3780565559864044, 0.014648249372839928]\n",
      "state [5, 0]\n",
      "action 1\n",
      "reward [0.46075946, 5.29084735, 7.92804145, 2.28448495, 1.01115855, 1.64300963]\n",
      "q_val [[-0.04707945138216019, 0.02497088350355625]]\n",
      "Q_val [[[0.01714811846613884, 0.04810578376054764, -0.08776570856571198, -0.005686670076102018, -0.04252764582633972, -0.02747238799929619], [0.09026215970516205, -0.008613821119070053, -0.032539963722229004, -0.005185827612876892, -0.03407088667154312, -0.01297791674733162]]]\n",
      "tot_reward 1.356850786782446\n",
      "cnt 6\n",
      "num_eps 50\n",
      "eps 0.4977049999999991\n",
      "---------------------------------------\n",
      "eps 50 reward (1) 1.36, the Q is 0.04 | -0.05; the probe is 0.12 | 0.22; dirichet: 0.100; q_loss: 1.5038; exploration_loss: 0.7492\n",
      "eps 51 reward (1) 2.48, the Q is 0.04 | -0.05; the probe is 0.25 | 0.18; dirichet: 0.100; q_loss: 1.1191; exploration_loss: 0.7033\n",
      "eps 52 reward (1) 4.05, the Q is 0.04 | -0.05; the probe is 0.30 | 0.05; dirichet: 0.100; q_loss: 1.2704; exploration_loss: 0.7015\n",
      "eps 53 reward (1) 1.79, the Q is 0.04 | -0.05; the probe is 0.30 | 0.12; dirichet: 0.100; q_loss: 1.6109; exploration_loss: 0.7016\n",
      "eps 54 reward (1) 3.12, the Q is 0.04 | -0.05; the probe is 0.11 | 0.15; dirichet: 0.100; q_loss: 1.1143; exploration_loss: 0.7012\n",
      "eps 55 reward (1) 1.79, the Q is 0.04 | -0.05; the probe is 0.18 | 0.19; dirichet: 0.100; q_loss: 1.1048; exploration_loss: 0.7059\n",
      "eps 56 reward (1) 3.02, the Q is 0.04 | -0.05; the probe is 0.28 | 0.27; dirichet: 0.100; q_loss: 1.0986; exploration_loss: 0.7057\n",
      "eps 57 reward (1) 4.05, the Q is 0.04 | -0.05; the probe is 0.21 | 0.29; dirichet: 0.100; q_loss: 1.0840; exploration_loss: 0.6962\n",
      "eps 58 reward (1) 2.19, the Q is 0.04 | -0.05; the probe is 0.01 | 0.61; dirichet: 0.100; q_loss: 1.0851; exploration_loss: 0.6974\n",
      "eps 59 reward (1) 4.58, the Q is 0.13 | 0.27; the probe is 0.03 | 0.21; dirichet: 0.100; q_loss: 2.2062; exploration_loss: 0.6953\n"
     ]
    }
   ],
   "source": [
    "model = EnvelopeLinearCQN(state_size, action_size, reward_size)\n",
    "exemplar_model = Exemplar(reward_size, reward_size, 1e-3, 0, device, 3)\n",
    "agent = MetaAgent(model, exemplar_model, args, is_train=True)  \n",
    "\n",
    "agent = train(env, agent, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b1a655c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(0.6931633, dtype=float32),\n",
       " array([0.09749418, 0.09749418, 0.09749418, 0.09749418, 0.09749418,\n",
       "        0.09749418, 0.09749418, 0.09749418, 0.09749418, 0.09749418],\n",
       "       dtype=float32),\n",
       " array([0.72320235, 0.72320235, 0.72320235, 0.72320235, 0.72320235,\n",
       "        0.72320235, 0.72320235, 0.72320235, 0.72320235, 0.72320235],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "minibatch = agent.sample(agent.trans_mem, agent.priority_mem, agent.batch_size)\n",
    "batchify = lambda x: list(x) * agent.weight_num\n",
    "state_batch = batchify(map(lambda x: x.s.unsqueeze(0), minibatch))\n",
    "action_batch = batchify(map(lambda x: LongTensor([x.a]), minibatch))\n",
    "reward_batch = batchify(map(lambda x: x.r.unsqueeze(0), minibatch))\n",
    "next_state_batch = batchify(map(lambda x: x.s_.unsqueeze(0), minibatch))\n",
    "terminal_batch = batchify(map(lambda x: x.d, minibatch))\n",
    "\n",
    "# w_batch = batchify(map(lambda x: x.w, minibatch))\n",
    "# w_batch = Variable(torch.stack(w_batch), requires_grad=False).type(FloatTensor)\n",
    "\n",
    "w_batch = list(map(lambda x: x.w, minibatch))\n",
    "w_batch = Variable(torch.stack(w_batch), requires_grad=False).type(FloatTensor)\n",
    "next_w_batch = list(map(lambda x: x.w_, minibatch))\n",
    "next_w_batch = Variable(torch.stack(next_w_batch), requires_grad=False).type(FloatTensor)\n",
    "w_batch, next_w_batch = agent.generate_neighbours(w_batch, next_w_batch, agent.weight_num)\n",
    "\n",
    "exemplar_batch_size = 10\n",
    "index_list = np.random.randint(0, w_batch.shape[0], size=exemplar_batch_size)\n",
    "\n",
    "# sample1 = torch.cat((torch.cat(state_batch, dim=0)[index_list], w_batch[index_list]), dim=1)\n",
    "sample1 = w_batch[index_list]\n",
    "positive = sample1[0:int(sample1.shape[0]/2)]\n",
    "negative = sample1[int(sample1.shape[0]/2):]\n",
    "\n",
    "sample1 = torch.cat((positive, positive), axis=0)\n",
    "sample2 = torch.cat((positive, negative), axis=0)\n",
    "\n",
    "target = torch.cat((torch.ones((positive.shape[0], 1)), torch.zeros((negative.shape[0],1))))\n",
    "\n",
    "exploration_loss = agent.exemplar_exploration.update(sample1, sample2, target)\n",
    "exploration_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "057d2955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0297, 0.2281, 0.1234, 0.0945, 0.2127, 0.3115],\n",
       "        [0.1338, 0.5137, 0.1799, 0.0336, 0.0319, 0.1071],\n",
       "        [0.2994, 0.2420, 0.1056, 0.0280, 0.0977, 0.2273],\n",
       "        [0.0331, 0.2781, 0.0769, 0.2023, 0.3133, 0.0962],\n",
       "        [0.0329, 0.1789, 0.1710, 0.3797, 0.0533, 0.1842],\n",
       "        [0.0297, 0.2281, 0.1234, 0.0945, 0.2127, 0.3115],\n",
       "        [0.1338, 0.5137, 0.1799, 0.0336, 0.0319, 0.1071],\n",
       "        [0.2994, 0.2420, 0.1056, 0.0280, 0.0977, 0.2273],\n",
       "        [0.0331, 0.2781, 0.0769, 0.2023, 0.3133, 0.0962],\n",
       "        [0.0329, 0.1789, 0.1710, 0.3797, 0.0533, 0.1842]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "cb69b8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.9691e-02, 2.2813e-01, 1.2344e-01, 9.4515e-02, 2.1273e-01, 3.1149e-01],\n",
       "        [1.3377e-01, 5.1370e-01, 1.7990e-01, 3.3646e-02, 3.1887e-02, 1.0710e-01],\n",
       "        [2.9945e-01, 2.4204e-01, 1.0557e-01, 2.7976e-02, 9.7674e-02, 2.2730e-01],\n",
       "        [3.3106e-02, 2.7814e-01, 7.6940e-02, 2.0231e-01, 3.1331e-01, 9.6196e-02],\n",
       "        [3.2943e-02, 1.7889e-01, 1.7100e-01, 3.7972e-01, 5.3261e-02, 1.8418e-01],\n",
       "        [5.8789e-02, 1.1202e-02, 2.8961e-01, 2.2498e-01, 3.1856e-01, 9.6868e-02],\n",
       "        [2.8426e-01, 3.0438e-01, 2.4056e-02, 2.6303e-01, 4.1852e-02, 8.2426e-02],\n",
       "        [2.2463e-02, 2.2332e-01, 1.9728e-01, 1.6033e-01, 1.6904e-01, 2.2756e-01],\n",
       "        [8.4553e-05, 4.4013e-01, 9.1140e-02, 3.5121e-01, 9.4354e-02, 2.3085e-02],\n",
       "        [1.1008e-01, 3.6935e-03, 2.0084e-02, 2.0717e-01, 4.8589e-01, 1.7308e-01]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3086dbcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5028, 0.5028, 0.5028, 0.5028, 0.5028, 0.5028, 0.5028, 0.5028, 0.5028,\n",
       "        0.5028])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.exemplar_exploration.get_prob(torch.Tensor(sample2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "93d58dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.3842, -0.2174, -0.0095],\n",
       "         [-0.3842, -0.2174, -0.0095],\n",
       "         [-0.3842, -0.2174, -0.0095],\n",
       "         [-0.3842, -0.2174, -0.0095],\n",
       "         [-0.3842, -0.2174, -0.0095],\n",
       "         [-0.3842, -0.2174, -0.0095],\n",
       "         [-0.3842, -0.2174, -0.0095],\n",
       "         [-0.3842, -0.2174, -0.0095],\n",
       "         [-0.3842, -0.2174, -0.0095],\n",
       "         [-0.3842, -0.2174, -0.0095]], grad_fn=<AddmmBackward0>),\n",
       " tensor([1.0000, 1.0000, 1.0000], grad_fn=<ExpBackward0>))"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.exemplar_exploration.encoder1(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a1c81937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.7905, -0.8554, -0.2997],\n",
       "         [ 0.7905, -0.8554, -0.2997],\n",
       "         [ 0.7905, -0.8554, -0.2997],\n",
       "         ...,\n",
       "         [ 0.7905, -0.8554, -0.2997],\n",
       "         [ 0.7905, -0.8554, -0.2997],\n",
       "         [ 0.7905, -0.8554, -0.2997]], grad_fn=<AddmmBackward0>),\n",
       " tensor([1.0000, 1.0000, 1.0000], grad_fn=<ExpBackward0>))"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.exemplar_exploration.encoder2(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d0e79a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4491, 0.0546, 0.2129, 0.0268, 0.1296, 0.1270],\n",
       "        [0.1129, 0.4247, 0.1316, 0.0431, 0.0427, 0.2449],\n",
       "        [0.4848, 0.0818, 0.0580, 0.0354, 0.0662, 0.2738],\n",
       "        [0.2462, 0.1596, 0.1394, 0.1993, 0.0489, 0.2066],\n",
       "        [0.1032, 0.4228, 0.0668, 0.0909, 0.2036, 0.1128],\n",
       "        [0.0998, 0.1017, 0.2919, 0.1661, 0.0968, 0.2438],\n",
       "        [0.0471, 0.1219, 0.2037, 0.0927, 0.3346, 0.2000],\n",
       "        [0.2262, 0.1354, 0.2125, 0.2736, 0.0224, 0.1299],\n",
       "        [0.1114, 0.1518, 0.1869, 0.1177, 0.3797, 0.0525],\n",
       "        [0.2703, 0.0382, 0.0501, 0.3743, 0.1896, 0.0775]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7123506d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1517,  0.0255,  0.2918, -0.6622,  0.2170,  0.5774],\n",
       "        [-0.2126,  0.0381,  0.7031,  0.0107, -0.4459,  0.6690],\n",
       "        [-0.5074, -0.7704, -0.5007,  0.3616,  0.0365,  0.7566],\n",
       "        [-0.6614,  0.5753,  0.4212, -0.7270, -0.6620, -0.1003],\n",
       "        [-0.0516, -0.2310, -0.4327,  0.3027,  0.0174,  0.2454],\n",
       "        [-0.0792,  0.5761,  0.3265,  0.4939,  0.6704, -0.3653]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.exemplar_exploration.encoder1.input_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d3c1aa81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4052, -0.2013,  0.1476,  0.0046, -0.1058, -0.1568],\n",
       "        [ 0.6673,  0.0222, -0.4492,  0.0455, -0.2971,  0.2218],\n",
       "        [-0.0672, -0.0889, -0.3498, -0.5678, -0.6732,  0.3505],\n",
       "        [-0.3164,  0.2907, -0.4615, -0.3610,  0.0238,  0.6773],\n",
       "        [ 0.5307,  0.3437, -0.3207, -0.5510,  0.5744,  0.1655],\n",
       "        [-0.1069,  0.1500,  0.8121,  0.5394,  0.0563, -0.1241]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.exemplar_exploration.encoder1.middle_layers[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "440b5855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0508,  0.1165, -0.4575, -0.5614, -0.5016, -0.3066],\n",
       "        [-0.6025,  0.4505,  0.3224,  0.1770, -0.4793,  0.5280],\n",
       "        [ 0.4596,  0.3312, -0.0333,  0.6614,  0.4940,  0.1429],\n",
       "        [ 0.5125,  0.3081, -0.7372, -0.3431, -0.1594,  0.3183],\n",
       "        [-0.0357, -0.5719, -0.4612,  0.1695,  0.5028,  0.4007],\n",
       "        [-0.6890,  0.4767, -0.2983, -0.1980, -0.4688,  0.4817]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.exemplar_exploration.encoder1.middle_layers[1].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b63d0bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2280, -0.0511,  0.0087, -0.1226,  0.1295, -0.4337],\n",
       "        [ 0.7213, -0.6539, -0.5365,  0.8129, -0.0790,  0.1673],\n",
       "        [ 0.7710,  0.1478, -0.3960,  0.1658, -0.3018, -0.0182]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.exemplar_exploration.encoder1.output_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0eb5ceed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4219,  0.0388,  0.6717,  0.5690,  0.0200,  0.3857],\n",
       "        [-0.2483, -0.8850, -0.5738, -0.2108, -0.6203,  0.4135],\n",
       "        [ 0.3906, -0.6704, -0.3467,  0.4073,  0.5902, -0.4055],\n",
       "        [ 0.0849,  0.4339, -0.4858, -0.1623, -0.0183, -0.3314],\n",
       "        [ 0.2226,  0.7325, -0.3647,  0.0252,  0.2816, -0.7248],\n",
       "        [ 0.3811,  0.2219, -0.3775, -0.5197, -0.0470,  0.2148]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.exemplar_exploration.encoder2.input_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e609d04d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0442, -0.6948,  0.0905, -0.5359,  0.3059, -0.6966],\n",
       "        [ 0.4607, -0.4929, -0.6293,  0.3777, -0.0472, -0.6598],\n",
       "        [ 0.5017, -0.2959,  0.6547, -0.4876,  0.4253, -0.1499],\n",
       "        [-0.7312,  0.4837, -0.5116, -0.2307, -0.5350, -0.3990],\n",
       "        [ 0.0530, -0.3585,  0.5933, -0.4743, -0.4582,  0.5733],\n",
       "        [-0.1381,  0.5811,  0.1330,  0.5490,  0.2900, -0.3450]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.exemplar_exploration.encoder2.middle_layers[1].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "141fde09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2358,  0.7143, -0.0748,  0.3639,  0.4208, -0.1441],\n",
       "        [ 0.6619, -0.5357, -0.3894,  0.5004,  0.4789,  0.0025],\n",
       "        [-0.2524,  0.2116,  0.2926, -0.1054, -0.0636,  0.4401],\n",
       "        [-0.7706,  0.2716, -0.7749, -0.4349, -0.9102,  0.6747],\n",
       "        [-0.4367, -0.2666, -0.1968, -0.6260, -0.5411, -0.2329],\n",
       "        [ 0.2322,  0.3201,  0.5454, -0.6749, -0.2609, -0.1297]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.exemplar_exploration.encoder2.middle_layers[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f2da07ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4517, -0.5651,  0.3831, -0.7905, -0.5621,  0.5282],\n",
       "        [ 0.5757,  0.6838, -0.4832,  0.7202,  0.5067, -0.0118],\n",
       "        [ 0.1186, -0.5849,  0.3356,  0.5019, -0.4892, -0.7697]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.exemplar_exploration.encoder2.output_layer.weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
